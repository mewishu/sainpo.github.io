<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[分布式系统CAP定理]]></title>
    <url>%2F2019%2F08%2F23%2Fdistributed-systems-cap%2F</url>
    <content type="text"><![CDATA[2000年7月，加州大学伯克利分校的Eric Brewer教授在ACM PODC会议上提出CAP猜想。2年后，麻省理工学院的Seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认定理。 CAP理论概述分布式领域中存在CAP理论： ① C：Consistency，一致性，数据一致更新，所有数据变动都是同步的。 ② A：Availability，可用性，系统具有好的响应性能。 ③ P：Partition tolerance，分区容错性。以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择，也就是说无论任何消息丢失，系统都可用。 这三个基本需求，最多只能同时满足其中的两项，在分布式系统中，因为P是必须的,因此往往选择就在CP或者AP中。理解CAP理论的最简单方式是想象两个节点分处分区两侧。允许至少一个节点更新状态会导致数据不一致，即丧失了C性质。如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了A性质。除非两个节点可以互相通信，才能既保证C又保证A，这又会导致丧失P性质。 一致性（C:Consistency）在分布式环境中，一致性指“all nodes see the same data at the same time”。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。例如一个将数据副本分布在不同分布式节点上的系统来说，如果对第一个节点的数据进行了更新操作并且更新成功后，其他节点上的数据也应该得到更新，并且所有用户都可以读取到其最新的值，那么这样的系统就被认为具有强一致性（或严格的一致性，最终一致性）。 可用性（A:Available）可用性是指系统提供的服务必须一直处于可用的状态(Reads and writes always succeed)，对于用户的每一个操作请求总是能够在有限的时间内返回结果。“有效的时间内”是指，对于用户的一个操作请求，系统必须能够在指定的时间（即响应时间）内返回对应的处理结果，如果超过了这个时间范围，那么系统就被认为是不可用的。 “返回结果”是可用性的另一个非常重要的指标，它要求系统在完成对用户请求的处理后，返回一个正常的响应结果。正常的响应结果通常能够明确的反映出对请求的处理结果，即成功或失败，而不是一个让用户感到困惑的返回结果。 分区容错性（P:Partition Tolerance）分区容错性约束了一个分布式系统需要具有如下特性：分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障(the system continues to operate despite arbitrary message loss or failure of part of the system)。 网络分区是指在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络等）中，由于一些特殊的原因导致这些子网络之间出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域。需要注意的是，组成一个分布式系统的每个节点的加入与退出都可以看作是一个特殊的网络分区。 由于一个分布式系统无法同时满足上面的三个需求，而只能满足其中的两项，因此在进行对CAP定理的应用的时候，需要根据业务的要求抛弃其中的一项，下表所示是抛弃CAP定理中任意一项特性的场景说明。 一致性的分类一致性是指从系统外部读取系统内部的数据时，在一定约束条件下相同，即数据变动在系统内部各节点应该是同步的。根据一致性的强弱程度不同，可以将一致性的分类为如下几种： ① 强一致性（strong consistency）。任何时刻，任何用户都能读取到最近一次成功更新的数据。 ② 单调一致性（monotonic consistency）。任何时刻，任何用户一旦读到某个数据在某次更新后的值，那么就不会再读到比这个值更旧的值。也就是说，可获取的数据顺序必是单调递增的。 ③ 会话一致性（session consistency）。任何用户在某次会话中，一旦读到某个数据在某次更新后的值，那么在本次会话中就不会再读到比这个值更旧的值。会话一致性是在单调一致性的基础上进一步放松约束，只保证单个用户单个会话内的单调性，在不同用户或同一用户不同会话间则没有保障。 ④ 最终一致性（eventual consistency）。用户只能读到某次更新后的值，但系统保证数据将最终达到完全一致的状态，只是所需时间不能保障。 ⑤ 弱一致性（weak consistency）。用户无法在确定时间内读到最新更新的值。 CAP权衡通过CAP理论及前面的证明，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？ 我们分三种情况来阐述一下。 CA without P这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。这也是为什么在前面的CAP证明中，我们以系统满足P为前提论述了无法同时满足C和A。 比如我们熟知的关系型数据库，如MySql和Oracle就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把P也考虑进来。 其实，在CAP理论中。C，A，P三者并不是平等的，CAP之父在《Spanner, TrueTime and the CAP Theorem》一文中写到： 如果说Spanner真有什么特别之处，那就是谷歌的广域网。Google通过建立私有网络以及强大的网络工程能力来保证P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。 从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。 所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。 CP without A如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。 一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。 设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。 无论是像Redis、HBase这种分布式存储系统，还是像Zookeeper这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？ ZooKeeper是个CP（一致性+分区容错性）的，即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。 AP wihtout C要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。 这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。 你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。 但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。 对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。 参考资料 分布式系统的CAP理论]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>CAP理论</category>
      </categories>
      <tags>
        <tag>CAP理论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper一致性保证]]></title>
    <url>%2F2019%2F08%2F23%2Fzookeeper-consistency%2F</url>
    <content type="text"><![CDATA[zookeeper（简称zk），顾名思义，为动物园管理员的意思，动物对应服务节点，zk是这些节点的管理者。在分布式场景中，zk的应用非常广泛，如：数据发布/订阅、命名服务、配置中心、分布式锁、集群管理、选主与服务发现等等。这不仅得益于zk类文件系统的数据模型和基于Watcher机制的分布式事件通知，也得益于zk特殊的高容错数据一致性协议。 这里的一致性，是指数据在多个副本之间保持一致的特性。分布式环境里，多个副本处于不同的节点上，如果对副本A的更新操作，未同步到副本B上，外界获取数据时，A与B的返回结果会不一样，这是典型的分布式数据不一致情况。而强一致性，是指分布式系统中，如果某个数据更新成功，则所有用户都能读取到最新的值。CAP定理告诉我们，在分布式系统设计中，P（分区容错性）是不可缺少的，因此只能在A（可用性）与C（一致性）间做取舍。本文主要探究zk在数据一致性方面的处理逻辑。 基本概念 数据节点（dataNode）：zk数据模型中的最小数据单元，数据模型是一棵树，由斜杠（/）分割的路径名唯一标识，数据节点可以存储数据内容及一系列属性信息，同时还可以挂载子节点，构成一个层次化的命名空间。 会话（Session）：指zk客户端与zk服务器之间的会话，在zk中，会话是通过客户端和服务器之间的一个TCP长连接来实现的。通过这个长连接，客户端能够使用心跳检测与服务器保持有效的会话，也能向服务器发送请求并接收响应，还可接收服务器的Watcher事件通知。Session的sessionTimeout，是会话超时时间，如果这段时间内，客户端未与服务器发生任何沟通（心跳或请求），服务器端会清除该session数据，客户端的TCP长连接将不可用，这种情况下，客户端需要重新实例化一个Zookeeper对象。 事务及ZXID：事务是指能够改变Zookeeper服务器状态的操作，一般包括数据节点的创建与删除、数据节点内容更新和客户端会话创建与失效等操作。对于每个事务请求，zk都会为其分配一个全局唯一的事务ID，即ZXID，是一个64位的数字，高32位表示该事务发生的集群选举周期（集群每发生一次leader选举，值加1），低32位表示该事务在当前选择周期内的递增次序（leader每处理一个事务请求，值加1，发生一次leader选择，低32位要清0）。 事务日志：所有事务操作都是需要记录到日志文件中的，可通过 dataLogDir配置文件目录，文件是以写入的第一条事务zxid为后缀，方便后续的定位查找。zk会采取“磁盘空间预分配”的策略，来避免磁盘Seek频率，提升zk服务器对事务请求的影响能力。默认设置下，每次事务日志写入操作都会实时刷入磁盘，也可以设置成非实时（写到内存文件流，定时批量写入磁盘），但那样断电时会带来丢失数据的风险。 数据快照：数据快照是zk数据存储中另一个非常核心的运行机制。数据快照用来记录zk服务器上某一时刻的全量内存数据内容，并将其写入到指定的磁盘文件中，可通过dataDir配置文件目录。可配置参数snapCount，设置两次快照之间的事务操作个数，zk节点记录完事务日志时，会统计判断是否需要做数据快照（距离上次快照，事务操作次数等于snapCount/2~snapCount 中的某个值时，会触发快照生成操作，随机值是为了避免所有节点同时生成快照，导致集群影响缓慢）。 过半：所谓“过半”是指大于集群机器数量的一半，即大于或等于（n/2+1），此处的“集群机器数量”不包括observer角色节点。leader广播一个事务消息后，当收到半数以上的ack信息时，就认为集群中所有节点都收到了消息，然后leader就不需要再等待剩余节点的ack，直接广播commit消息，提交事务。选举中的投票提议及数据同步时，也是如此，leader不需要等到所有learner节点的反馈，只要收到过半的反馈就可进行下一步操作。 数据模型zk维护的数据主要有：客户端的会话（session）状态及数据节点（dataNode）信息。zk在内存中构造了个DataTree的数据结构，维护着path到dataNode的映射以及dataNode间的树状层级关系。为了提高读取性能，集群中每个服务节点都是将数据全量存储在内存中。可见，zk最适于读多写少且轻量级数据（默认设置下单个dataNode限制为1MB大小）的应用场景。数据仅存储在内存是很不安全的，zk采用事务日志文件及快照文件的方案来落盘数据，保障数据在不丢失的情况下能快速恢复。 集群架构zk集群由多个节点组成，其中有且仅有一个leader，处理所有事务请求；follower及observer统称learner。learner需要同步leader的数据。follower还参与选举及事务决策过程。zk客户端会打散配置文件中的serverAddress 顺序并随机组成新的list，然后循环按序取一个服务器地址进行连接，直到成功。follower及observer会将事务请求转交给leader处理。 要搭建一个高可用的zk集群，我们首先需要确定好集群规模。一般我们将节点（指leader及follower节点，不包括observer节点）个数设置为 2*n+1 ，n为可容忍宕机的个数。 zk使用“过半”设计原则，很好地解决了单点问题，提升了集群容灾能力。但是zk的集群伸缩不是很灵活，集群中所有机器ip及port都是事先配置在每个服务的zoo.cfg 文件里的。如果要往集群增加一个follower节点，首先需要更改所有机器的zoo.cfg，然后逐个重启。集群模式下，单个zk服务节点启动时的工作流程大体如下： 统一由QuorumPeerMain作为启动类，加载解析zoo.cfg配置文件； 初始化核心类：ServerCnxnFactory（IO操作）、FileTxnSnapLog（事务日志及快照文件操作）、QuorumPeer实例（代表zk集群中的一台机器）、ZKDatabase（内存数据库）等； 加载本地快照文件及事务日志，恢复内存数据； 完成leader选举，节点间通过一系列投票，选举产生最合适的机器成为leader，同时其余机器成为follower或是observer。关于选举算法，就是集群中哪个机器处理的数据越新（通过ZXID来比较，ZXID越大，数据越新），其越有可能被选中； 完成leader与learner间的数据同步：集群中节点角色确定后，leader会重新加载本地快照及日志文件，以此作为基准数据，再结合各个learner的本地提交数据，leader再确定需要给具体learner回滚哪些数据及同步哪些数据； 当leader收到过半的learner完成数据同步的ACK，集群开始正常工作，可以接收并处理客户端请求，在此之前集群不可用。 zookeeper一致性协议zookeeper实现数据一致性的核心是ZAB协议（Zookeeper原子消息广播协议）。该协议需要做到以下几点：（1）集群在半数以下节点宕机的情况下，能正常对外提供服务；（2）客户端的写请求全部转交给leader来处理，leader需确保写变更能实时同步给所有follower及observer；（3）leader宕机或整个集群重启时，需要确保那些已经在leader服务器上提交的事务最终被所有服务器都提交，确保丢弃那些只在leader服务器上被提出的事务，并保证集群能快速恢复到故障前的状态。Zab协议有两种模式， 崩溃恢复（选主+数据同步）和消息广播（事务操作）。任何时候都需要保证只有一个主进程负责进行事务操作，而如果主进程崩溃了，就需要迅速选举出一个新的主进程。主进程的选举机制与事务操作机制是紧密相关的。下面详细讲解这三个场景的协议规则，从细节去探索ZAB协议的数据一致性原理。 选主leader选举是zk中最重要的技术之一，也是保证分布式数据一致性的关键所在。当集群中的一台服务器处于如下两种情况之一时，就会进入leader选举阶段——服务器初始化启动、服务器运行期间无法与leader保持连接。 选举阶段，集群间互传的消息称为投票，投票Vote主要包括二个维度的信息：ID、ZXIDID 被推举的leader的服务器ID，集群中的每个zk节点启动前就要配置好这个全局唯一的ID。ZXID 被推举的leader的事务ID ，该值是从机器DataTree内存中取的，即事务已经在机器上被commit过了。 节点进入选举阶段后的大体执行逻辑如下：设置状态为LOOKING，初始化内部投票Vote (id,zxid) 数据至内存，并将其广播到集群其它节点。节点首次投票都是选举自己作为leader，将自身的服务ID、处理的最近一个事务请求的ZXID（ZXID是从内存数据库里取的，即该节点最近一个完成commit的事务id）及当前状态广播出去。然后进入循环等待及处理其它节点的投票信息的流程中。 循环等待流程中，节点每收到一个外部的Vote信息，都需要将其与自己内存Vote数据进行PK，规则为取ZXID大的，若ZXID相等，则取ID大的那个投票。若外部投票胜选，节点需要将该选票覆盖之前的内存Vote数据，并再次广播出去；同时还要统计是否有过半的赞同者与新的内存投票数据一致，无则继续循环等待新的投票，有则需要判断leader是否在赞同者之中，在则退出循环，选举结束，根据选举结果及各自角色切换状态，leader切换成LEADING、follower切换到FOLLOWING、observer切换到OBSERVING状态。 算法细节可参照FastLeaderElection.lookForLeader()，主要有三个线程在工作：选举线程（主动调用lookForLeader方法的线程，通过阻塞队列sendqueue及recvqueue与其它两个线程协作）、WorkerReceiver线程（选票接收器，不断获取其它服务器发来的选举消息，筛选后会保存到recvqueue队列中。zk服务器启动时，开始正常工作，不停止）以及WorkerSender线程（选票发送器，会不断地从sendqueue队列中获取待发送的选票，并广播至集群）。WorkerReceiver线程一直在工作，即使当前节点处于LEADING或者FOLLOWING状态，它起到了一个过滤的作用，当前节点为LOOKING时，才会将外部投票信息转交给选举线程处理；如果当前节点处于非LOOKING状态，收到了处于LOOKING状态的节点投票数据（外部节点重启或网络抖动情况下），说明发起投票的节点数据跟集群不一致，这时，当前节点需要向集群广播出最新的内存Vote(id，zxid)，落后节点收到该Vote后，会及时注册到leader上，并完成数据同步，跟上集群节奏，提供正常服务。 选主后的数据同步选主算法中的zxid是从内存数据库中取的最新事务id，事务操作是分两阶段的（提出阶段和提交阶段），leader生成提议并广播给followers，收到半数以上的ACK后，再广播commit消息，同时将事务操作应用到内存中。follower收到提议后先将事务写到本地事务日志，然后反馈ACK，等接到leader的commit消息时，才会将事务操作应用到内存中。可见，选主只是选出了内存数据是最新的节点，仅仅靠这个是无法保证已经在leader服务器上提交的事务最终被所有服务器都提交。比如leader发起提议P1,并收到半数以上follower关于P1的ACK后，在广播commit消息之前宕机了，选举产生的新leader之前是follower，未收到关于P1的commit消息，内存中是没有P1的数据。而ZAB协议的设计是需要保证选主后，P1是需要应用到集群中的。这块的逻辑是通过选主后的数据同步来弥补。 选主后，节点需要切换状态，leader切换成LEADING状态后的流程如下： 重新加载本地磁盘上的数据快照至内存，并从日志文件中取出快照之后的所有事务操作，逐条应用至内存，并添加到已提交事务缓存commitedProposals。这样能保证日志文件中的事务操作，必定会应用到leader的内存数据库中。 获取learner发送的FOLLOWERINFO/OBSERVERINFO信息，并与自身commitedProposals比对，确定采用哪种同步方式，不同的learner可能采用不同同步方式（DIFF同步、TRUNC+DIFF同步、SNAP同步）。这里是拿learner内存中的zxid与leader内存中的commitedProposals（min、max）比对，如果zxid介于min与max之间，但又不存在于commitedProposals中时，说明该zxid对应的事务需要TRUNC回滚；如果 zxid 介于min与max之间且存在于commitedProposals中，则leader需要将zxid+1~max 间所有事务同步给learner，这些内存缺失数据，很可能是因为leader切换过程中造成commit消息丢失，learner只完成了事务日志写入，未完成提交事务，未应用到内存。 leader主动向所有learner发送同步数据消息，每个learner有自己的发送队列，互不干扰。同步结束时，leader会向learner发送NEWLEADER指令，同时learner会反馈一个ACK。当leader接收到来自learner的ACK消息后，就认为当前learner已经完成了数据同步，同时进入“过半策略”等待阶段。当leader统计到收到了一半已上的ACK时，会向所有已经完成数据同步的learner发送一个UPTODATE指令，用来通知learner集群已经完成了数据同步，可以对外服务了。细节可参照Leader.lead() 、Follower.followLeader()及LearnerHandler类。 事务操作ZAB协议对于事务操作的处理是一个类似于二阶段提交过程。针对客户端的事务请求，leader服务器会为其生成对应的事务proposal，并将其发送给集群中所有follower机器，然后收集各自的选票，最后进行事务提交。流程如下图。 ZAB协议的二阶段提交过程中，移除了中断逻辑（事务回滚），所有follower服务器要么正常反馈leader提出的事务proposal，要么就抛弃leader服务器。follower收到proposal后的处理很简单，将该proposal写入到事务日志，然后立马反馈ACK给leader，也就是说如果不是网络、内存或磁盘等问题，follower肯定会写入成功，并正常反馈ACK。leader收到过半follower的ACK后，会广播commit消息给所有learner，并将事务应用到内存；learner收到commit消息后会将事务应用到内存。 ZAB协议中多次用到“过半”设计策略 ，该策略是zk在A（可用性）与C（一致性）间做的取舍，也是zk具有高容错特性的本质。相较分布式事务中的2PC（二阶段提交协议）的“全量通过”，ZAB协议可用性更高（牺牲了部分一致性），能在集群半数以下服务宕机时正常对外提供服务。 ZooKeeper提供的一致性服务ZooKeeper从以下几点保证了数据的一致性 顺序一致性：来自任意特定客户端的更新都会按其发送顺序被提交保持一致。也就是说，如果一个客户端将Znode z的值更新为a，在之后的操作中，它又将z的值更新为b，则没有客户端能够在看到z的值是b之后再看到值a（如果没有其他对z的更新）。 原子性：每个更新要么成功，要么失败。这意味着如果一个更新失败，则不会有客户端会看到这个更新的结果。 单一系统映像：一个客户端无论连接到哪一台服务器，它看到的都是同样的系统视图。这意味着，如果一个客户端在同一个会话中连接到一台新的服务器，它所看到的系统状态不会比 在之前服务器上所看到的更老。当一台服务器出现故障，导致它的一个客户端需要尝试连接集合体中其他的服务器时，所有滞后于故障服务器的服务器都不会接受该 连接请求，除非这些服务器赶上故障服务器。 持久性：一个更新一旦成功，其结果就会持久存在并且不会被撤销。这表明更新不会受到服务器故障的影响。 实时性：在特定的一段时间内，客户端看到的系统需要被保证是实时的（在十几秒的时间里）。在此时间段内，任何系统的改变将被客户端看到，或者被客户端侦测到。 用CAP理论来分析ZooKeeperCAP理论告诉我们，一个分布式系统不可能同时满足以下三种 一致性（C:Consistency） 可用性（A:Available） 分区容错性（P:Partition Tolerance） 这三个基本需求，最多只能同时满足其中的两项，因为P是必须的,因此往往选择就在CP或者AP中。 在此ZooKeeper保证的是CP 分析：可用性（A:Available） 不能保证每次服务请求的可用性。任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性；但是它不能保证每次服务请求的可用性（注：也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果）。所以说，ZooKeeper不能保证服务可用性。 进行leader选举时集群都是不可用。在使用ZooKeeper获取服务列表时，当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30 ~ 120s, 且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。所以说，ZooKeeper不能保证服务可用性。 参考资料 浅析Zookeeper的一致性原理]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper请求处理]]></title>
    <url>%2F2019%2F08%2F19%2Fzookeeper-request-processor%2F</url>
    <content type="text"><![CDATA[请求处理链对于请求处理链而言，所有请求处理器的父接口为RequestProcessor: 1234567891011public interface RequestProcessor &#123; public static class RequestProcessorException extends Exception &#123; public RequestProcessorException(String msg, Throwable t) &#123; super(msg, t); &#125; &#125; void processRequest(Request request) throws RequestProcessorException; void shutdown();&#125; 处理的主要逻辑在processRequest中，通过processRequest方法可以将请求传递到下个处理器，通常是单线程的。而shutdown表示关闭处理器，其意味着该处理器要关闭和其他处理器的连接。 PrepRequestProcessor。请求预处理器。在Zookeeper中，那些会改变服务器状态的请求称为事务请求（创建节点、更新数据、删除节点、创建会话等），PrepRequestProcessor能够识别出当前客户端请求是否是事务请求。对于事务请求，PrepRequestProcessor处理器会对其进行一系列预处理，如创建请求事务头、事务体、会话检查、ACL检查和版本检查等。 SyncRequestProcessor。事务日志记录处理器。用来将事务请求记录到事务日志文件中，同时会触发Zookeeper进行数据快照。 FinalRequestProcessor。用来进行客户端请求返回之前的操作，包括创建客户端请求的响应，针对事务请求，该处理还会负责将事务应用到内存数据库中去。 会话创建请求Zookeeper服务端对于会话创建的处理，大体可以分为请求接收、会话创建、预处理、事务处理、事务应用和会话响应六大环节，其大体流程如下: 请求接收 I/O层接收来自客户端的请求。NIOServerCnxn维护每一个客户端连接，客户端与服务器端的所有通信都是由NIOServerCnxn负责，其负责统一接收来自客户端的所有请求，并将请求内容从底层网络I/O中完整地读取出来。 判断是否是客户端会话创建请求。每个会话对应一个NIOServerCnxn实体，对于每个请求，Zookeeper都会检查当前NIOServerCnxn实体是否已经被初始化，如果尚未被初始化，那么就可以确定该客户端一定是会话创建请求。 反序列化ConnectRequest请求。一旦确定客户端请求是否是会话创建请求，那么服务端就可以对其进行反序列化，并生成一个ConnectRequest载体。 判断是否是ReadOnly客户端。如果当前Zookeeper服务器是以ReadOnly模式启动，那么所有来自非ReadOnly型客户端的请求将无法被处理。因此，服务端需要先检查是否是ReadOnly客户端，并以此来决定是否接受该会话创建请求。 检查客户端ZXID。正常情况下，在一个Zookeeper集群中，服务端的ZXID必定大于客户端的ZXID，因此若发现客户端的ZXID大于服务端ZXID，那么服务端不接受该客户端的会话创建请求。 协商sessionTimeout。在客户端向服务器发送超时时间后，服务器会根据自己的超时时间限制最终确定该会话超时时间，这个过程就是sessionTimeout协商过程。 判断是否需要重新激活创建会话。服务端根据客户端请求中是否包含sessionID来判断该客户端是否需要重新创建会话，若客户单请求中包含sessionID，那么就认为该客户端正在进行会话重连，这种情况下，服务端只需要重新打开这个会话，否则需要重新创建。 会话创建 为客户端生成sessionID。在为客户端创建会话之前，服务端首先会为每个客户端分配一个sessionID，服务端为客户端分配的sessionID是全局唯一的。 注册会话。向SessionTracker中注册会话，SessionTracker中维护了sessionsWithTimeout和sessionsById，在会话创建初期，会将客户端会话的相关信息保存到这两个数据结构中。 激活会话。激活会话涉及Zookeeper会话管理的分桶策略，其核心是为会话安排一个区块，以便会话清理程序能够快速高效地进行会话清理。 生成会话密码。服务端在创建一个客户端会话时，会同时为客户端生成一个会话密码，连同sessionID一同发给客户端，作为会话在集群中不同机器间转移的凭证。 预处理 将请求交给PrepRequestProcessor处理器处理。在提交给第一个请求处理器之前，Zookeeper会根据该请求所属的会话，进行一次激活会话操作，以确保当前会话处于激活状态，完成会话激活后，则提交请求至处理器。 创建请求事务头。对于事务请求，Zookeeper会为其创建请求事务头，服务端后续的请求处理器都是基于该请求头来识别当前请求是否是事务请求，请求事务头包含了一个事务请求最基本的一些信息，包括sessionID、ZXID（事务请求对应的事务ZXID）、CXID（客户端的操作序列）和请求类型（如create、delete、setData、createSession等）等。 创建请求事务体。由于此时是会话创建请求，其事务体是CreateSessionTxn。 注册于激活会话。处理由非Leader服务器转发过来的会话创建请求。 事务处理 将请求交给ProposalRequestProcessor处理器。与提议相关的处理器，从ProposalRequestProcessor开始，请求的处理将会进入三个子处理流程，分别是Sync流程、Proposal流程、Commit流程。 Sync流程 使用SyncRequestProcessor处理器记录事务日志，针对每个事务请求，都会通过事务日志的形式将其记录，完成日志记录后，每个Follower都会向Leader发送ACK消息，表明自身完成了事务日志的记录，以便Leader统计每个事务请求的投票情况。 Proposal流程 每个事务请求都需要集群中过半机器投票认可才能被真正应用到内存数据库中，这个投票与统计过程就是Proposal流程。 发起投票。若当前请求是事务请求，Leader会发起一轮事务投票，在发起事务投票之前，会检查当前服务端的ZXID是否可用。 生成提议Proposal。若ZXID可用，Zookeeper会将已创建的请求头和事务体以及ZXID和请求本身序列化到Proposal对象中，此Proposal对象就是一个提议。 广播提议。Leader以ZXID作为标识，将该提议放入投票箱outstandingProposals中，同时将该提议广播给所有Follower。 收集投票。Follower接收到Leader提议后，进入Sync流程进行日志记录，记录完成后，发送ACK消息至Leader服务器，Leader根据这些ACK消息来统计每个提议的投票情况，当一个提议获得半数以上投票时，就认为该提议通过，进入Commit阶段。 将请求放入toBeApplied队列中。 广播Commit消息。Leader向Follower和Observer发送COMMIT消息。向Observer发送INFORM消息，向Leader发送ZXID。 Commit流程 将请求交付CommitProcessor。CommitProcessor收到请求后，将其放入queuedRequests队列中。 处理queuedRequest队列请求。CommitProcessor中单独的线程处理queuedRequests队列中的请求。 标记nextPending。若从queuedRequests中取出的是事务请求，则需要在集群中进行投票处理，同时将nextPending标记位当前请求。 等待Proposal投票。在进行Commit流程的同时，Leader会生成Proposal并广播给所有Follower服务器，此时，Commit流程等待，直到投票结束。 投票通过。若提议获得过半机器认可，则进入请求提交阶段，该请求会被放入commitedRequests队列中，同时唤醒Commit流程。 提交请求。若commitedRequests队列中存在可以提交的请求，那么Commit流程则开始提交请求，将请求放入toProcess队列中，然后交付下一个请求处理器: FinalRequestProcessor。 事务应用 交付给FinalRequestProcessor处理器。FinalRequestProcessor处理器检查outstandingChanges队列中请求的有效性，若发现这些请求已经落后于当前正在处理的请求，那么直接从outstandingChanges队列中移除。 事务应用。之前的请求处理仅仅将事务请求记录到了事务日志中，而内存数据库中的状态尚未改变，因此，需要将事务变更应用到内存数据库。 将事务请求放入队列commitProposal。完成事务应用后，则将该请求放入commitProposal队列中，commitProposal用来保存最近被提交的事务请求，以便集群间机器进行数据的快速同步。 会话响应 统计处理。Zookeeper计算请求在服务端处理所花费的时间，统计客户端连接的基本信息，如lastZxid(最新的ZXID)、lastOp(最后一次和服务端的操作)、lastLatency(最后一次请求处理所花费的时间)等。 创建响应ConnectResponse。会话创建成功后的响应，包含了当前客户端和服务端之间的通信协议版本号、会话超时时间、sessionID和会话密码。 序列化ConnectResponse。 I/O层发送响应给客户端。 SetData请求服务端对于SetData请求大致可以分为四步，预处理、事务处理、事务应用、请求响应。 预处理 I/O层接收来自客户端的请求。 判断是否是客户端”会话创建”请求。对于SetData请求，按照正常事务请求进行处理。 将请求交给PrepRequestProcessor处理器进行处理。 创建请求事务头。 会话检查。检查该会话是否有效。 反序列化请求，并创建ChangeRecord记录。反序列化并生成特定的SetDataRequest请求，请求中包含了数据节点路径path、更新的内容data和期望的数据节点版本version。同时根据请求对应的path，Zookeeper生成一个ChangeRecord记录，并放入outstandingChanges队列中。 ACL检查。检查客户端是否具有数据更新的权限。 数据版本检查。通过version属性来实现乐观锁机制的写入校验。 创建请求事务体SetDataTxn。 保存事务操作到outstandingChanges队列中。 事务处理对于事务请求，服务端都会发起事务处理流程。所有事务请求都是由ProposalRequestProcessor处理器处理，通过Sync、Proposal、Commit三个子流程相互协作完成。 事务应用 交付给FinalRequestProcessor处理器。 事务应用。将请求事务头和事务体直接交给内存数据库ZKDatabase进行事务应用，同时返回ProcessTxnResult对象，包含了数据节点内容更新后的stat。 将事务请求放入commitProposal队列。 请求响应 创建响应体SetDataResponse。其包含了当前数据节点的最新状态stat。 创建响应头。包含当前响应对应的事务ZXID和请求处理是否成功的标识。 序列化响应。 I/O层发送响应给客户端。 GetData请求服务端对于GetData请求的处理，大致分为三步，预处理、非事务处理、请求响应。 预处理 I/O层接收来自客户端的请求。 判断是否是客户端”会话创建”请求。 将请求交给PrepRequestProcessor处理器进行处理。 会话检查。 非事务处理 反序列化GetDataRequest请求。 获取数据节点。 ACL检查。 获取数据内容和stat，注册Watcher。 请求响应 创建响应体GetDataResponse。响应体包含当前数据节点的内容和状态stat。 创建响应头。 统计处理。 序列化响应。 I/O层发送响应给客户端。 参考资料 【分布式】Zookeeper请求处理]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper会话]]></title>
    <url>%2F2019%2F08%2F19%2Fzookeeper-session%2F</url>
    <content type="text"><![CDATA[用途客户端与服务端之间任何交互操作都与会话息息相关，如临时节点的生命周期、客户端请求的顺序执行、Watcher通知机制等。Zookeeper的连接与会话就是客户端通过实例化Zookeeper对象来实现客户端与服务端创建并保持TCP连接的过程。本质上，就是：Session 映射到一个 TCP 长连接，并且标识这条长连接 通过 TCP 长连接，发送请求、接受响应 接收来自 Server 的 Watcher 事件通知 会话状态在Zookeeper客户端与服务端成功完成连接创建后，就创建了一个会话，Zookeeper会话在整个运行期间的生命周期中，会在不同的会话状态中之间进行切换，这些状态可以分为CONNECTING、CONNECTED、RECONNECTING、RECONNECTED、CLOSE等。 一旦客户端开始创建Zookeeper对象，那么客户端状态就会变成CONNECTING状态，同时客户端开始尝试连接服务端，连接成功后，客户端状态变为CONNECTED，通常情况下，由于断网或其他原因，客户端与服务端之间会出现断开情况，一旦碰到这种情况，Zookeeper客户端会自动进行重连服务，同时客户端状态再次变成CONNCTING，直到重新连上服务端后，状态又变为CONNECTED，在通常情况下，客户端的状态总是介于CONNECTING和CONNECTED之间。但是，如果出现诸如会话超时、权限检查或是客户端主动退出程序等情况，客户端的状态就会直接变更为CLOSE状态。 会话创建Session是Zookeeper中的会话实体，代表了一个客户端会话，其包含了如下四个属性: sessionID：服务节点启动时，时间戳 + sid（myid中的数字），经过一定算法计算得到 基准 sessionID 之后的 SessionID，都为基准 SessionID自增得到。 TimeOut：超时时间，时间段 TickTime：下次超时时间点，TickTime 约为 currentTime + TimeOut，方便 ZK Server 对会话分桶策略管理，高效的进行会话检查和清理。 isClosing：ZK Server 判定 Session 超时后，将会话标记为已关闭，确保不再处理其新请求； Zookeeper为了保证请求会话的全局唯一性，在SessionTracker初始化时，调用initializeNextSession方法生成一个sessionID，之后在Zookeeper运行过程中，会在该sessionID的基础上为每个会话进行分配，初始化算法如下 12345678910111213141516171819public class SessionTrackerImpl extends ZooKeeperCriticalThread implements SessionTracker &#123;... /** * Generates an initial sessionId. High order byte is serverId, next 5 * 5 bytes are from timestamp, and low order 2 bytes are 0s. */ public static long initializeNextSession(long id) &#123; long nextSid; nextSid = (Time.currentElapsedTime() &lt;&lt; 24) &gt;&gt;&gt; 8; nextSid = nextSid | (id &lt;&lt;56); if (nextSid == EphemeralType.CONTAINER_EPHEMERAL_OWNER) &#123; ++nextSid; // this is an unlikely edge case, but check it just in case &#125; return nextSid; &#125;&#125; 其中的id表示配置在myid文件中的值，通常是一个整数，如1、2、3。该算法的高8位确定了所在机器，后56位使用当前时间的毫秒表示进行随机。SessionTracker是Zookeeper服务端的会话管理器，负责会话的创建、管理和清理等工作。 会话管理分桶策略Zookeeper的会话管理主要是通过SessionTracker来负责，其采用了分桶策略（将类似的会话放在同一区块中进行管理）进行管理，以便Zookeeper对会话进行不同区块的隔离处理以及同一区块的统一处理。 Zookeeper将所有的会话都分配在不同的区块之中，分配的原则是每个会话的下次超时时间点（ExpirationTime）。ExpirationTime指该会话最近一次可能超时的时间点。同时，Zookeeper Leader服务器在运行过程中会定时地进行会话超时检查，时间间隔是ExpirationInterval，默认为tickTime的值，ExpirationTime的计算时间如下 ExpirationTime = ((CurrentTime + SessionTimeOut) / ExpirationInterval + 1) * ExpirationInterval 会话激活为了保持客户端会话的有效性，客户端会在会话超时时间过期范围内向服务端发送PING请求来保持会话的有效性（心跳检测）。同时，服务端需要不断地接收来自客户端的心跳检测，并且需要重新激活对应的客户端会话，这个重新激活过程称为TouchSession。会话激活不仅能够使服务端检测到对应客户端的存货性，同时也能让客户端自己保持连接状态，其流程如下: 如上图所示，整个流程分为四步 检查该会话是否已经被关闭。若已经被关闭，则直接返回即可。 计算该会话新的超时时间ExpirationTime_New。使用上面提到的公式计算下一次超时时间点。 获取该会话上次超时时间ExpirationTime_Old。计算该值是为了定位其所在的区块。 迁移会话。将该会话从老的区块中取出，放入ExpirationTime_New对应的新区块中。 在上面会话激活过程中，只要客户端发送心跳检测，服务端就会进行一次会话激活，心跳检测由客户端主动发起，以PING请求形式向服务端发送，在Zookeeper的实际设计中，只要客户端有请求发送到服务端，那么就会触发一次会话激活，以下两种情况都会触发会话激活。 客户端向服务端发送请求，包括读写请求，就会触发会话激活。 客户端发现在sessionTimeout/3时间内尚未和服务端进行任何通信，那么就会主动发起PING请求，服务端收到该请求后，就会触发会话激活。 会话超时检查对于会话的超时检查而言，Zookeeper使用SessionTracker来负责，SessionTracker使用单独的线程（超时检查线程）专门进行会话超时检查，即逐个一次地对会话桶中剩下的会话进行清理。如果一个会话被激活，那么Zookeeper就会将其从上一个会话桶迁移到下一个会话桶中，如ExpirationTime 1 的session n 迁移到ExpirationTime n 中，此时ExpirationTime 1中留下的所有会话都是尚未被激活的，超时检查线程就定时检查这个会话桶中所有剩下的未被迁移的会话，超时检查线程只需要在这些指定时间点（ExpirationTime 1、ExpirationTime 2…）上进行检查即可，这样提高了检查的效率，性能也非常好。 会话清理当SessionTracker的会话超时线程检查出已经过期的会话后，就开始进行会话清理工作，大致可以分为如下七步。 标记会话状态为已关闭。由于会话清理过程需要一段时间，为了保证在此期间不再处理来自该客户端的请求，SessionTracker会首先将该会话的isClosing标记为true，这样在会话清理期间接收到该客户端的心情求也无法继续处理了。 发起会话关闭请求。为了使对该会话的关闭操作在整个服务端集群都生效，Zookeeper使用了提交会话关闭请求的方式，并立即交付给PreRequestProcessor进行处理。 收集需要清理的临时节点。一旦某个会话失效后，那么和该会话相关的临时节点都需要被清理，因此，在清理之前，首先需要将服务器上所有和该会话相关的临时节点都整理出来。Zookeeper在内存数据库中会为每个会话都单独保存了一份由该会话维护的所有临时节点集合，在Zookeeper处理会话关闭请求之前，若正好有以下两类请求到达了服务端并正在处理中。 节点删除请求，删除的目标节点正好是上述临时节点中的一个。 临时节点创建请求，创建的目标节点正好是上述临时节点中的一个。 对于第一类请求，需要将所有请求对应的数据节点路径从当前临时节点列表中移出，以避免重复删除，对于第二类请求，需要将所有这些请求对应的数据节点路径添加到当前临时节点列表中，以删除这些即将被创建但是尚未保存到内存数据库中的临时节点。 添加节点删除事务变更。完成该会话相关的临时节点收集后，Zookeeper会逐个将这些临时节点转换成”节点删除”请求，并放入事务变更队列outstandingChanges中。 删除临时节点。FinalRequestProcessor会触发内存数据库，删除该会话对应的所有临时节点。 移除会话。完成节点删除后，需要将会话从SessionTracker中删除。 关闭NIOServerCnxn。最后，从NIOServerCnxnFactory找到该会话对应的NIOServerCnxn，将其关闭。 重连当客户端与服务端之间的网络连接断开时，Zookeeper客户端会自动进行反复的重连，直到最终成功连接上Zookeeper集群中的一台机器。此时，再次连接上服务端的客户端有可能处于以下两种状态之一 CONNECTED。如果在会话超时时间内重新连接上集群中一台服务器 。 EXPIRED。如果在会话超时时间以外重新连接上，那么服务端其实已经对该会话进行了会话清理操作，此时会话被视为非法会话。 在客户端与服务端之间维持的是一个长连接，在sessionTimeout时间内，服务端会不断地检测该客户端是否还处于正常连接，服务端会将客户端的每次操作视为一次有效的心跳检测来反复地进行会话激活。因此，在正常情况下，客户端会话时一直有效的。然而，当客户端与服务端之间的连接断开后，用户在客户端可能主要看到两类异常：CONNECTION_LOSS（连接断开）和SESSION_EXPIRED（会话过期）。 连接断开: CONNECTION_LOSS此时，客户端会自动从地址列表中重新逐个选取新的地址并尝试进行重新连接，直到最终成功连接上服务器。若客户端在setData时出现了CONNECTION_LOSS现象，此时客户端会收到None-Disconnected通知，同时会抛出异常。应用程序需要捕捉异常并且等待Zookeeper客户端自动完成重连，一旦重连成功，那么客户端会收到None-SyncConnected通知，之后就可以重试setData操作。 会话失效: SESSION_EXPIRED客户端与服务端断开连接后，重连时间耗时太长，超过了会话超时时间限制后没有成功连上服务器，服务器会进行会话清理，此时，客户端不知道会话已经失效，状态还是DISCONNECTED，如果客户端重新连上了服务器，此时状态为SESSION_EXPIRED，用于需要重新实例化Zookeeper对象，并且看应用的复杂情况，重新恢复临时数据。 会话转移: SESSION_MOVED客户端会话从一台服务器转移到另一台服务器，即客户端与服务端S1断开连接后，重连上了服务端S2，此时会话就从S1转移到了S2。当多个客户端使用相同的sessionId/sessionPasswd创建会话时，会收到SessionMovedException异常。因为一旦有第二个客户端连接上了服务端，就被认为是会话转移了。 参考资料 ZooKeeper 技术内幕：会话]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper数据与存储]]></title>
    <url>%2F2019%2F08%2F16%2Fzookeeper-data-and-storage%2F</url>
    <content type="text"><![CDATA[数据分类整体分为 3 类： 内存数据 磁盘数据 快照 事务日志 内存数据Zookeeper的数据模型是树结构，在内存数据库中，存储了整棵树的内容，包括所有的节点路径、节点数据、ACL信息，Zookeeper会定时将这个数据存储到磁盘上。 DataTree DataTree是内存数据存储的核心，是一个树结构，代表了内存中一份完整的数据。DataTree不包含任何与网络、客户端连接及请求处理相关的业务逻辑，是一个独立的组件。 DataNode DataNode是数据存储的最小单元，其内部保存了: 节点数据 节点 ACL 信息 节点的路径 除此之外还记录了父节点的引用和子节点列表两个属性，其也提供了对子节点列表进行操作的接口。 ZKDatabase Zookeeper的内存数据库，管理Zookeeper的所有会话、DataTree存储和事务日志。ZKDatabase会定时向磁盘dump快照数据，同时在Zookeeper启动时，会通过磁盘的事务日志和快照文件恢复成一个完整的内存数据库。 具体实现：DataTree 和 DataNode，见下图 事务日志文件存储在配置Zookeeper集群时需要配置dataDir目录，其用来存储事务日志文件。也可以为事务日志单独分配一个文件存储目录:dataLogDir。若配置dataLogDir为/home/admin/zkData/zk_log，那么Zookeeper在运行过程中会在该目录下建立一个名字为version-2的子目录，该目录确定了当前Zookeeper使用的事务日志格式版本号，当下次某个Zookeeper版本对事务日志格式进行变更时，此目录也会变更，即在version-2子目录下会生成一系列文件大小一致(64MB)的文件。 日志格式在配置好日志文件目录，启动Zookeeper后，完成如下操作 (1) 创建/test_log节点，初始值为v1。 (2) 更新/test_log节点的数据为v2。 (3) 创建/test_log/c节点，初始值为v1。 (4) 删除/test_log/c节点。 经过四步操作后，会在/log/version-2/目录下生成一个日志文件，笔者下是log.cec。 将Zookeeper下的zookeeper-3.4.6.jar和slf4j-api-1.6.1.jar复制到/log/version-2目录下，使用如下命令打开log.cec文件。 1java -classpath ./zookeeper-3.4.6.jar:./slf4j-api-1.6.1.jar org.apache.zookeeper.server.LogFormatter log.cec ZooKeeper Transactional Log File with dbid 0 txnlog format version 2 。是文件头信息，主要是事务日志的DBID和日志格式版本号。 …session 0x159…0xcec createSession 30000。表示客户端会话创建操作。 …session 0x159…0xced create ‘/test_log,… 。表示创建/test_log节点，数据内容为#7631(v1)。 …session 0x159…0xcee setData ‘/test_log,…。表示设置了/test_log节点数据，内容为#7632(v2)。 …session 0x159…0xcef create ’/test_log/c,…。表示创建节点/test_log/c。 …session 0x159…0xcf0 delete ‘/test_log/c。表示删除节点/test_log/c。 其他几行事务日志类似，具体可以对照Zookeeper源码类org.apache.zookeeper.server.LogFormatter自行分析。通过可视化这个文件，我们还注意到一点，由于这是一个记录事务操作的日志文件，因此里面没有任何读操作的日志记录。 事务日志格式如下所示: (参见$FileTxnLog$类): 12345678910111213141516171819202122232425262728293031323334353637* The format of a Transactional log is as follows:* &lt;blockquote&gt;&lt;pre&gt;* LogFile:* FileHeader TxnList ZeroPad** FileHeader: &#123;* magic 4bytes (ZKLG)* version 4bytes* dbid 8bytes* &#125;** TxnList:* Txn || Txn TxnList** Txn:* checksum Txnlen TxnHeader Record 0x42** checksum: 8bytes Adler32 is currently used* calculated across payload -- Txnlen, TxnHeader, Record and 0x42** Txnlen:* len 4bytes** TxnHeader: &#123;* sessionid 8bytes* cxid 4bytes* zxid 8bytes* time 8bytes* type 4bytes* &#125;** Record:* See Jute definition file for details on the various record types** ZeroPad:* 0 padded to EOF (filled during preallocation stage)* &lt;/pre&gt;&lt;/blockquote&gt; 日志写入FileTxnLog负责维护事务日志对外的接口，包括事务日志的写入和读取等。事务操作写入事务日志的工作主要由append方法来负责: 12public synchronized boolean append(TxnHeader hdr, Record txn) throws IOException Zookeeper的事务日志写入过程大体可以分为如下6个步骤。 确定是否有事务日志可写。当Zookeeper服务器启动完成需要进行第一次事务日志的写入，或是上一次事务日志写满时，都会处于与事务日志文件断开的状态，即Zookeeper服务器没有和任意一个日志文件相关联。因此在进行事务日志写入前，Zookeeper首先会判断FileTxnLog组件是否已经关联上一个可写的事务日志文件。若没有，则会使用该事务操作关联的ZXID作为后缀创建一个事务日志文件，同时构建事务日志的文件头信息，并立即写入这个事务日志文件中去，同时将该文件的文件流放入streamToFlush集合，该集合用来记录当前需要强制进行数据落盘的文件流。 确定事务日志文件是否需要扩容(预分配)。Zookeeper会采用磁盘空间预分配策略。当检测到当前事务日志文件剩余空间不足4096字节时，就会开始进行文件空间扩容，即在现有文件大小上，将文件增加65536KB(64MB)，然后使用”0”填充被扩容的文件空间。 事务序列化。对事务头和事务体的序列化，其中事务体又可分为会话创建事务、节点创建事务、节点删除事务、节点数据更新事务等。 生成Checksum。为保证日志文件的完整性和数据的准确性，Zookeeper在将事务日志写入文件前，会计算生成Checksum。 写入事务日志文件流。将序列化后的事务头、事务体和Checksum写入文件流中，此时并为写入到磁盘上。 事务日志刷入磁盘。由于步骤5中的缓存原因，无法实时地写入磁盘文件中，因此需要将缓存数据强制刷入磁盘。 日志截断在Zookeeper运行过程中，可能出现非Leader记录的事务ID比Leader上大，这是非法运行状态。此时，需要保证所有机器必须与该Leader的数据保持同步，即Leader会发送TRUNC命令给该机器，要求进行日志截断，Learner收到该命令后，就会删除所有包含或大于该事务ID的事务日志文件。 snapshot-数据快照数据快照是Zookeeper数据存储中非常核心的运行机制，数据快照用来记录Zookeeper服务器上某一时刻的全量内存数据内容，并将其写入指定的磁盘文件中。 文件存储与事务文件类似，Zookeeper快照文件也可以指定特定磁盘目录，通过dataDir属性来配置。若指定dataDir为/home/admin/zkData/zk_data，则在运行过程中会在该目录下创建version-2的目录，该目录确定了当前Zookeeper使用的快照数据格式版本号。在Zookeeper运行时，会生成一系列文件。 数据快照FileSnap负责维护快照数据对外的接口，包括快照数据的写入和读取等，将内存数据库写入快照数据文件其实是一个序列化过程。针对客户端的每一次事务操作，Zookeeper都会将他们记录到事务日志中，同时也会将数据变更应用到内存数据库中，Zookeeper在进行若干次事务日志记录后，将内存数据库的全量数据Dump到本地文件中，这就是数据快照。其步骤如下: 确定是否需要进行数据快照。每进行一次事务日志记录之后，Zookeeper都会检测当前是否需要进行数据快照，考虑到数据快照对于Zookeeper机器的影响，需要尽量避免Zookeeper集群中的所有机器在同一时刻进行数据快照。采用过半随机策略进行数据快照操作。 切换事务日志文件。表示当前的事务日志已经写满，需要重新创建一个新的事务日志。 创建数据快照异步线程。创建单独的异步线程来进行数据快照以避免影响Zookeeper主流程。 获取全量数据和会话信息。从ZKDatabase中获取到DataTree和会话信息。 生成快照数据文件名。Zookeeper根据当前已经提交的最大ZXID来生成数据快照文件名。 数据序列化。首先序列化文件头信息，然后再对会话信息和DataTree分别进行序列化，同时生成一个Checksum，一并写入快照数据文件中去。 关键点： 异步：异步线程生成快照文件 Fuzzy 快照： 快照文件生成过程中，仍然有新的事务提交， 因此，快照文件不是精确到某一时刻的快照文件，而是模糊的， 这就要求事务操作是幂等的，否则产生不一致。 初始化在Zookeeper服务器启动期间，首先会进行数据初始化工作，用于将存储在磁盘上的数据文件加载到Zookeeper服务器内存中。 初始化流程 数据的初始化工作是从磁盘上加载数据的过程，主要包括了从快照文件中加载快照数据和根据事务日志进行数据修正两个过程。 初始化FileTxnSnapLog。FileTxnSnapLog是Zookeeper事务日志和快照数据访问层，用于衔接上层业务和底层数据存储，底层数据包含了事务日志和快照数据两部分。FileTxnSnapLog中对应FileTxnLog和FileSnap。 初始化ZKDatabase。首先构建DataTree，同时将FileTxnSnapLog交付ZKDatabase，以便内存数据库能够对事务日志和快照数据进行访问。在ZKDatabase初始化时，DataTree也会进行相应的初始化工作，如创建一些默认结点，如/、/zookeeper、/zookeeper/quota三个节点。 创建PlayBackListener。其主要用来接收事务应用过程中的回调，在Zookeeper数据恢复后期，会有事务修正过程，此过程会回调PlayBackListener来进行对应的数据修正。 处理快照文件。此时可以从磁盘中恢复数据了，首先从快照文件开始加载。 获取最新的100个快照文件。更新时间最晚的快照文件包含了最新的全量数据。 解析快照文件。逐个解析快照文件，此时需要进行反序列化，生成DataTree和sessionsWithTimeouts，同时还会校验Checksum及快照文件的正确性。对于100个快找文件，如果正确性校验通过时，通常只会解析最新的那个快照文件。只有最新快照文件不可用时，才会逐个进行解析，直至100个快照文件全部解析完。若将100个快照文件解析完后还是无法成功恢复一个完整的DataTree和sessionWithTimeouts，此时服务器启动失败。 获取最新的ZXID。此时根据快照文件的文件名即可解析出最新的ZXID：zxid_for_snap。该ZXID代表了Zookeeper开始进行数据快照的时刻。 处理事务日志。此时服务器内存中已经有了一份近似全量的数据，现在开始通过事务日志来更新增量数据。 获取所有zxid_for_snap之后提交的事务。此时，已经可以获取快照数据的最新ZXID。只需要从事务日志中获取所有ZXID比步骤7得到的ZXID大的事务操作。 事务应用。获取大于zxid_for_snap的事务后，将其逐个应用到之前基于快照数据文件恢复出来的DataTree和sessionsWithTimeouts。每当有一个事务被应用到内存数据库中后，Zookeeper同时会回调PlayBackListener，将这事务操作记录转换成Proposal，并保存到ZKDatabase的committedLog中，以便Follower进行快速同步。 获取最新的ZXID。待所有的事务都被完整地应用到内存数据库中后，也就基本上完成了数据的初始化过程，此时再次获取ZXID，用来标识上次服务器正常运行时提交的最大事务ID。 校验epoch。epoch标识了当前Leader周期，集群机器相互通信时，会带上这个epoch以确保彼此在同一个Leader周期中。完成数据加载后，Zookeeper会从步骤11中确定ZXID中解析出事务处理的Leader周期：epochOfZxid。同时也会从磁盘的currentEpoch和acceptedEpoch文件中读取上次记录的最新的epoch值，进行校验。 数据同步ZK 集群服务器启动之后，会进行 2 个动作： 选举 Leader：分配角色 Learner 向 Leader 服务器注册，然后进行数据同步 数据同步的本质：Leader将那些没有在Learner服务器上提交过的事务请求同步给Learner。大体过程如下: 获取Learner状态。在注册Learner的最后阶段，Learner服务器会发送给Leader服务器一个ACKEPOCH数据包，Leader会从这个数据包中解析出该Learner的currentEpoch和lastZxid。 数据同步初始化。首先从Zookeeper内存数据库中提取出事务请求对应的提议缓存队列proposals，同时完成peerLastZxid(该Learner最后处理的ZXID)、minCommittedLog(Leader提议缓存队列commitedLog中最小的ZXID)、maxCommittedLog(Leader提议缓存队列commitedLog中的最大ZXID)三个ZXID值的初始化。 对于集群数据同步而言，通常分为四类，直接差异化同步(DIFF同步)、先回滚再差异化同步(TRUNC+DIFF同步)、仅回滚同步(TRUNC同步)、全量同步(SNAP同步)，在初始化阶段，Leader会优先以全量同步方式来同步数据。同时，会根据Leader和Learner之间的数据差异情况来决定最终的数据同步方式。 下面一张图，能够清晰描述发生上述同步的时机： 关键点：Learner 上的 zxid 与 Leader Proposals 中 min 和 max 的关系 直接差异化同步(DIFF同步)场景: peerLastZxid介于minCommittedLog和maxCommittedLog之间 Leader首先向这个Learner发送一个DIFF指令，用于通知Learner进入差异化数据同步阶段，Leader即将把一些Proposal同步给自己，针对每个Proposal，Leader都会通过发送PROPOSAL内容数据包和COMMIT指令数据包来完成， 先回滚再差异化同步(TRUNC+DIFF同步)场景: Leader已经将事务记录到本地事务日志中，但是没有成功发起Proposal流程 当Leader发现某个Learner包含了一条自己没有的事务记录，那么就需要该Learner进行事务回滚，回滚到Leader服务器上存在的，同时也是最接近于peerLastZxid的ZXID。 仅回滚同步(TRUNC同步)场景: peerLastZxid大于maxCommittedLog Leader要求Learner回滚到ZXID值为maxCommittedLog对应的事务操作。 全量同步(SNAP同步)场景: peerLastZxid小于minCommittedLog或peerLastZxid不等于lastProcessedZxid Leader无法直接使用提议缓存队列和Learner进行同步，因此只能进行全量同步。Leader将本机的全量内存数据同步给Learner。Leader首先向Learner发送一个SNAP指令，通知Learner即将进行全量同步，随后，Leader会从内存数据库中获取到全量的数据节点和会话超时时间记录器，将他们序列化后传输给Learner。Learner接收到该全量数据后，会对其反序列化后载入到内存数据库中。 参考资料 ZooKeeper 技术内幕：数据的存储 【Zookeeper】源码分析之持久化（一）之FileTxnLog 【Zookeeper】源码分析之持久化（二）之FileSnap 【Zookeeper】源码分析之持久化（三）之FileTxnSnapLog]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper系统模型]]></title>
    <url>%2F2019%2F08%2F16%2Fzookeeper-system-model%2F</url>
    <content type="text"><![CDATA[数据模型Zookeeper的数据节点称为ZNode，ZNode是Zookeeper中数据的最小单元，每个ZNode都可以保存数据，同时还可以挂载子节点，因此构成了一个层次化的命名空间，称为树。 在Zookeeper中，事务是指能够改变Zookeeper服务器状态的操作，一般包括节点创建与删除，数据节点内容更新和客户端会话创建与失效，对于每个事务请求，Zookeeper都会为其分配一个全局唯一的事务ID，用 zxid表示，通常是64位的数字，每个 zxid对应一次更新操作，从这些ZXID中可以间接地识别出Zookeeper处理这些更新操作请求的全局顺序。 节点特性在Zookeeper中，每个数据节点都是由生命周期的，类型不同则会不同的生命周期，节点类型可以分为持久节点（PERSISTENT）、临时节点（EPHEMERAL）、顺序节点（SEQUENTIAL）三大类，可以通过组合生成如下四种类型节点 持久节点（PERSISTENT）: 节点创建后便一直存在于Zookeeper服务器上，直到有删除操作来主动清除该节点。 持久顺序节点（PERSISTENT_SEQUENTIAL）: 相比持久节点，其新增了顺序特性，每个父节点都会为它的第一级子节点维护一份顺序，用于记录每个子节点创建的先后顺序。在创建节点时，会自动添加一个数字后缀，作为新的节点名，该数字后缀的上限是整形的最大值。 临时节点（EPEMERAL）: 临时节点的生命周期与客户端会话绑定，客户端失效，节点会被自动清理。同时，Zookeeper规定不能基于临时节点来创建子节点，即临时节点只能作为叶子节点。 临时顺序节点（EPEMERAL_SEQUENTIAL）: 在临时节点的基础添加了顺序特性。 每个节点除了存储数据外，还存储了节点本身的一些状态信息，可通过get命令获取。 版本–保证分布式数据原子性操作每个数据节点都具有三种类型的版本信息，对数据节点的任何更新操作都会引起版本号的变化。 每个 ZNode 都有 3 类版本信息： version：当前数据节点数据内容的版本号 cversion：子节点列表，当前数据子节点的版本号，只会感知子节点列表变更信息，新增子节点、删除子节点，而不会感知子节点数据内容的变更 aversion：当前数据节点ACL变更版本号 上述各版本号都是表示修改次数，如version为1表示对数据节点的内容变更了一次。即使前后两次变更并没有改变数据内容，version的值仍然会改变。version可以用于写入验证，类似于CAS。 版本号，什么作用？目标：解决 ZNode 的并发更新问题，实现 CAS（Compare And Switch）乐观锁。 补充：乐观锁事务，分为 3 个典型阶段： 数据读取 写入校验 数据写入 Watcher–数据变更通知Zookeeper使用Watcher机制实现分布式数据的发布/订阅功能。 实现原理ZooKeeper 允许客户端向服务端注册一个 Watcher 监听，当服务端的一些指定事件触发了这个 Watcher，那么就会向指定客户端发送一个事件通知来实现分布式的通知功能。 ZooKeeper 的 Watcher 机制主要包括客户端线程、客户端 WatchManager 和 ZooKeeper 服务器三部分。在具体工作流程上，简单地讲，客户端在向 ZooKeeper 服务器注册 Watcher 的同时，会将 Watcher 对象存储在客户端的 WatchManager 中。当 ZooKeeper 服务器端触发 Watcher 事件后，会向客户端发送通知，客户端线程从 WatchManager 中取出对应的 Watcher 对象来执行回调逻辑。如下面WatchManager所示，WatchManager 创建了一个 HashMap，这个 HashMap 被用来存放 Watcher 对象。 WatchManager 类 12345678910111213141516171819202122232425private final HashMap&lt;String, HashSet&lt;Watcher&gt;&gt; watchTable = new HashMap&lt;String, HashSet&lt;Watcher&gt;&gt;();private final HashMap&lt;Watcher, HashSet&lt;String&gt;&gt; watch2Paths = new HashMap&lt;Watcher, HashSet&lt;String&gt;&gt;();synchronized void addWatch(String path, Watcher watcher) &#123; HashSet&lt;Watcher&gt; list = watchTable.get(path); if (list == null) &#123; // don't waste memory if there are few watches on a node // rehash when the 4th entry is added, doubling size thereafter // seems like a good compromise list = new HashSet&lt;Watcher&gt;(4); watchTable.put(path, list); &#125; list.add(watcher); HashSet&lt;String&gt; paths = watch2Paths.get(watcher); if (paths == null) &#123; // cnxns typically have many watches, so use default cap here paths = new HashSet&lt;String&gt;(); watch2Paths.put(watcher, paths); &#125; paths.add(path);&#125; 整个 Watcher 注册和通知流程如下图所示: 总体框图 说明： Watcher，接口类型，其定义了process方法，需子类实现。 Event，接口类型，Watcher的内部类，无任何方法。 KeeperState，枚举类型，Event的内部类，表示Zookeeper所处的状态。 EventType，枚举类型，Event的内部类，表示Zookeeper中发生的事件类型。 WatchedEvent，表示对ZooKeeper上发生变化后的反馈，包含了KeeperState和EventType。 ClientWatchManager，接口类型，表示客户端的Watcher管理者，其定义了materialized方法，需子类实现。 ZKWatchManager，Zookeeper的内部类，继承ClientWatchManager。 MyWatcher，ZooKeeperMain的内部类，继承Watcher。 ServerCnxn，接口类型，继承Watcher，表示客户端与服务端的一个连接。 WatchManager，管理Watcher。 Watcher 接口Watcher 的理念是启动一个客户端去接收从 ZooKeeper 服务端发过来的消息并且同步地处理这些信息。ZooKeeper 的 Java API 提供了公共接口 Watcher，具体操作类通过实现这个接口相关的方法来实现从所连接的 ZooKeeper 服务端接收数据。如果要处理这个消息，需要为客户端注册一个 CallBack（回调）对象。Watcher 接口定义在 org.apache.zookeeper 包里面，如下所示: 123456789101112131415161718192021222324252627282930313233343536373839404142public interface Watcher &#123; // Event、WatchType定义 public interface Event &#123; /** * Enumeration of states the ZooKeeper may be at the event */ @InterfaceAudience.Public public enum KeeperState &#123; /** Unused, this state is never generated by the server */ @Deprecated Unknown (-1), /** The client is in the disconnected state - it is not connected * to any server in the ensemble. */ Disconnected (0), /** Unused, this state is never generated by the server */ @Deprecated NoSyncConnected (1), ... /** * Enumeration of types of events that may occur on the ZooKeeper */ @InterfaceAudience.Public public enum EventType &#123; None (-1), NodeCreated (1), NodeDeleted (2), NodeDataChanged (3), NodeChildrenChanged (4), DataWatchRemoved (5), ChildWatchRemoved (6); private final int intValue; // Integer representation of value // for sending over wire abstract public void process(WatchedEvent event);&#125; 在 Watcher 接口里面，除了回调函数 process 以外，还包含 KeeperState 和 EventType 两个枚举类，分别代表了通知状态和事件类型，如下图所示。 process 方法是 Watcher 接口中的一个回调方法，当 ZooKeeper 向客户端发送一个 Watcher 事件通知时，客户端就会对相应的 process 方法进行回调，从而实现对事件的处理。 process 方法包含 WatcherEvent 类型的参数，WatchedEvent 包含了每一个事件的三个基本属性：通知状态（KeeperState）、事件类型（EventType）和节点路径（Path），ZooKeeper 使用 WatchedEvent 对象来封装服务端事件并传递给 Watcher，从而方便回调方法 process 对服务端事件进行处理。 WatchedEvent 和 WatcherEvent 都表示的是同一个事物，都是对一个服务端事件的封装。不同的是，WatchedEvent 是一个逻辑事件，用于服务端和客户端程序执行过程中所需的逻辑对象，而 WatcherEvent 因为实现了序列化接口，因此可以用于网络传输。 服务端在线程 WatchedEvent 事件之后，会调用 getWrapper 方法将自己包装成一个可序列化的 WatcherEvent 事件，如清单 7 所示，以便通过网络传输到客户端。客户端在接收到服务端的这个事件对象后，首先会将 WatcherEvent 事件还原成一个 WatchedEvent 事件，并传递给 process 方法处理，回调方法 process 根据入参就能够解析出完整的服务端事件了。 1234567891011121314151617181920public class WatchedEvent &#123; final private KeeperState keeperState; final private EventType eventType; private String path; /** * Create a WatchedEvent with specified type, state and path */ public WatchedEvent(EventType eventType, KeeperState keeperState, String path) &#123; this.keeperState = keeperState; this.eventType = eventType; this.path = path; &#125; // 可序列化的事件 public WatcherEvent getWrapper() &#123; return new WatcherEvent(eventType.getIntValue(), keeperState.getIntValue(), path); &#125; 客户端注册 Watcher 流程创建一个Zookeeper客户端对象实例时，可以向构造方法中传入一个默认的Watcher(如: zk = new ZooKeeper(HostPort,2000,connectionWatcher)): 12345678910public class ZooKeeper implements AutoCloseable &#123; // 构造函数 public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher) throws IOException &#123; this(connectString, sessionTimeout, watcher, false); &#125; ... &#125; 第三个参数是连接到 ZooKeeper 服务端的 connectionWatcher 事件监听，这个 Watcher 将作为整个 ZooKeeper 会话期间的默认 Watcher，会一直被保存在客户端 ZKWatchManager 的 defaultWatcher 里面。 这里我们以getData这个接口为例来说明。getData接口用于获取指定节点的数据内容: 12345678910111213141516171819202122232425262728293031public byte[] getData(final String path, Watcher watcher, Stat stat) throws KeeperException, InterruptedException &#123; final String clientPath = path; PathUtils.validatePath(clientPath); // the watch contains the un-chroot path WatchRegistration wcb = null; if (watcher != null) &#123; wcb = new DataWatchRegistration(watcher, clientPath); &#125; final String serverPath = prependChroot(clientPath); RequestHeader h = new RequestHeader(); h.setType(ZooDefs.OpCode.getData); GetDataRequest request = new GetDataRequest(); request.setPath(serverPath); // 对当前客户端请求进行标记，将其设置为使用 Watcher 监听 request.setWatch(watcher != null); GetDataResponse response = new GetDataResponse(); ReplyHeader r = cnxn.submitRequest(h, request, response, wcb); if (r.getErr() != 0) &#123; throw KeeperException.create(KeeperException.Code.get(r.getErr()), clientPath); &#125; if (stat != null) &#123; DataTree.copyStat(response.getStat(), stat); &#125; return response.getData(); &#125; 客户端的请求基本都是在 ClientCnxn 里面进行操作，当收到请求后，客户端会对当前客户端请求进行标记，将其设置为使用 Watcher 监听，同时会封装一个 Watcher 的注册信息 WatchRegistration 对象，用于暂时保存数据节点的路径和 Watcher 的对应关系。 在 ZooKeeper 中，Packet 是一个最小的通信协议单元，即数据包。Pakcet 用于进行客户端与服务端之间的网络传输，任何需要传输的对象都需要包装成一个 Packet 对象。在 ClientCnxn 中 WatchRegistration 也会被封装到 Pakcet 中，然后由 SendThread 线程调用 queuePacke 方法把 Packet 放入发送队列中等待客户端发送，这又是一个异步过程，分布式系统采用异步通信是一个普遍认同的观念。随后，SendThread 线程会通过 readResponse 方法接收来自服务端的响应，异步地调用 finishPacket 方法从 Packet 中取出对应的 Watcher 并注册到 ZKWatchManager 中去: 1234567891011121314151617protected void finishPacket(Packet p) &#123; int err = p.replyHeader.getErr(); if (p.watchRegistration != null) &#123; p.watchRegistration.register(err); &#125; ... if (p.cb == null) &#123; synchronized (p) &#123; p.finished = true; p.notifyAll(); &#125; &#125; else &#123; p.finished = true; eventThread.queuePacket(p); &#125; &#125; 除了上面介绍的方式以外，ZooKeeper 客户端也可以通过 getData、getChildren 和 exist 三个接口来向 ZooKeeper 服务器注册 Watcher，无论使用哪种方式，注册 Watcher 的工作原理是一致的。如下所示，getChildren 方法调用了 WatchManager 类的 addWatch 方法添加了 watcher 事件。 12345678910111213141516171819202122232425262728293031public List&lt;String&gt; getChildren(final String path, Watcher watcher, Stat stat) throws KeeperException, InterruptedException &#123; final String clientPath = path; PathUtils.validatePath(clientPath); // the watch contains the un-chroot path WatchRegistration wcb = null; if (watcher != null) &#123; wcb = new ChildWatchRegistration(watcher, clientPath); &#125; final String serverPath = prependChroot(clientPath); RequestHeader h = new RequestHeader(); h.setType(ZooDefs.OpCode.getChildren2); GetChildren2Request request = new GetChildren2Request(); request.setPath(serverPath); request.setWatch(watcher != null); GetChildren2Response response = new GetChildren2Response(); ReplyHeader r = cnxn.submitRequest(h, request, response, wcb); if (r.getErr() != 0) &#123; throw KeeperException.create(KeeperException.Code.get(r.getErr()), clientPath); &#125; if (stat != null) &#123; DataTree.copyStat(response.getStat(), stat); &#125; return response.getChildren(); &#125; 如下代码所示，现在需要从这个封装对象中再次提取出 Watcher 对象来，在 register 方法里面，客户端将 Watcher 对象转交给 ZKWatchManager，并最终保存在一个 Map 类型的数据结构 dataWatches 里面，用于将数据节点的路径和 Watcher 对象进行一一映射后管理起来。 12345678910111213public void register(int rc) &#123; if (shouldAddWatch(rc)) &#123; Map&lt;String, Set&lt;Watcher&gt;&gt; watches = getWatches(rc); synchronized(watches) &#123; Set&lt;Watcher&gt; watchers = watches.get(clientPath); if (watchers == null) &#123; watchers = new HashSet&lt;Watcher&gt;(); watches.put(clientPath, watchers); &#125; watchers.add(watcher); &#125; &#125; &#125; 注意，WatcherRegistation 除了 Header 和 request 两个属性被传递到了服务端，其他都没有到服务端，否则服务端就容易出现内存紧张甚至溢出的危险，因为数据量太大了。这就是 ZooKeeper 为什么适用于分布式环境的原因，它在网络中传输的是消息，而不是数据包实体。 服务端处理 Watcher 流程 注意，以下所有代码均为精简版，去除了日志、判断分支，只在源码上保留了主线代码。 FinalRequestProcessor 类接收到客户端请求后，会调用 processRequest 方法进行处理，会进一步转向 ZooKeeperServer 的 processRequest 进行进一步处理，处理结由 ZKDatabase 类返回: 12345678910111213public class FinalRequestProcessor implements RequestProcessor &#123; private static final Logger LOG = LoggerFactory.getLogger(FinalRequestProcessor.class); ZooKeeperServer zks; public void processRequest(Request request) &#123; ... ProcessTxnResult rc = null; synchronized (zks.outstandingChanges) &#123; // Need to process local session requests rc = zks.processTxn(request); ...&#125; 1234567891011121314151617181920212223242526272829303132public class ZooKeeperServer implements SessionExpirer, ServerStats.Provider &#123; ... private ProcessTxnResult processTxn(Request request, TxnHeader hdr, Record txn) &#123; ProcessTxnResult rc; int opCode = request != null ? request.type : hdr.getType(); long sessionId = request != null ? request.sessionId : hdr.getClientId(); if (hdr != null) &#123; rc = getZKDatabase().processTxn(hdr, txn); &#125; else &#123; rc = new ProcessTxnResult(); &#125; if (opCode == OpCode.createSession) &#123; if (hdr != null &amp;&amp; txn instanceof CreateSessionTxn) &#123; CreateSessionTxn cst = (CreateSessionTxn) txn; sessionTracker.addGlobalSession(sessionId, cst.getTimeOut()); &#125; else if (request != null &amp;&amp; request.isLocalSession()) &#123; request.request.rewind(); int timeout = request.request.getInt(); request.request.rewind(); sessionTracker.addSession(request.sessionId, timeout); &#125; else &#123; LOG.warn("*****&gt;&gt;&gt;&gt;&gt; Got " + txn.getClass() + " " + txn.toString()); &#125; &#125; else if (opCode == OpCode.closeSession) &#123; sessionTracker.removeSession(sessionId); &#125; return rc; &#125;&#125; 123456789101112131415public class ZKDatabase &#123; private static final Logger LOG = LoggerFactory.getLogger(ZKDatabase.class); /** * the process txn on the data * @param hdr the txnheader for the txn * @param txn the transaction that needs to be processed * @return the result of processing the transaction on this * datatree/zkdatabase */ public ProcessTxnResult processTxn(TxnHeader hdr, Record txn) &#123; return dataTree.processTxn(hdr, txn); &#125;&#125; ServerCnxn存储对于注册 Watcher 请求，FinalRequestProcessor 的 ProcessRequest 方法会判断当前请求是否需要注册 Watcher，如果为 true，就会将当前的 ServerCnxn 对象和数据节点路径传入 getData 方法中去: 123456789101112131415161718case OpCode.getData: &#123; lastOp = "GETD"; GetDataRequest getDataRequest = new GetDataRequest(); ByteBufferInputStream.byteBuffer2Record(request.request, getDataRequest); DataNode n = zks.getZKDatabase().getNode(getDataRequest.getPath()); if (n == null) &#123; throw new KeeperException.NoNodeException(); &#125; PrepRequestProcessor.checkACL(zks, zks.getZKDatabase().aclForNode(n), ZooDefs.Perms.READ, request.authInfo); Stat stat = new Stat(); byte b[] = zks.getZKDatabase().getData(getDataRequest.getPath(), stat, getDataRequest.getWatch() ? cnxn : null); rsp = new GetDataResponse(b, stat); break; &#125; ServerCnxn 是一个 ZooKeeper 客户端和服务器之间的连接接口，代表了一个客户端和服务器的连接，我们后面讲到的 process 回调方法，实际上也是从这里回调的，所以可以把 ServerCnxn 看作是一个 Watcher 对象。数据节点的节点路径和 ServerCnxn 最终会被存储在 WatchManager 的 watchTable 和 watch2Paths 中。 如前所述，WatchManager 负责 Watcher 事件的触发，它是一个统称，在服务端 DataTree 会托管两个 WatchManager，分别是 dataWatches 和 childWatches，分别对应数据变更 Watcher 和子节点变更 Watcher。 123456789class WatchManager &#123; private static final Logger LOG = LoggerFactory.getLogger(WatchManager.class); private final HashMap&lt;String, HashSet&lt;Watcher&gt;&gt; watchTable = new HashMap&lt;String, HashSet&lt;Watcher&gt;&gt;(); private final HashMap&lt;Watcher, HashSet&lt;String&gt;&gt; watch2Paths = new HashMap&lt;Watcher, HashSet&lt;String&gt;&gt;();&#125; Watcher触发当发生 Create、Delete、NodeChange（数据变更）这样的事件后，DataTree 会调用相应方法去触发 WatchManager 的 triggerWatch 方法，该方法返回 ZNODE 的信息，自此进入到回调本地 process 的序列： 123456789101112131415161718192021222324public class DataTree &#123; private static final Logger LOG = LoggerFactory.getLogger(DataTree.class); public ProcessTxnResult processTxn(TxnHeader header, Record txn, boolean isSubTxn) &#123; ProcessTxnResult rc = new ProcessTxnResult(); try &#123; rc.clientId = header.getClientId(); rc.cxid = header.getCxid(); rc.zxid = header.getZxid(); rc.type = header.getType(); rc.err = 0; rc.multiResult = null; switch (header.getType()) &#123; case OpCode.setData: SetDataTxn setDataTxn = (SetDataTxn) txn; rc.path = setDataTxn.getPath(); rc.stat = setData(setDataTxn.getPath(), setDataTxn .getData(), setDataTxn.getVersion(), header .getZxid(), header.getTime()); break;&#125; 12345678910111213141516171819202122232425public Stat setData(String path, byte data[], int version, long zxid, long time) throws KeeperException.NoNodeException &#123; Stat s = new Stat(); DataNode n = nodes.get(path); if (n == null) &#123; throw new KeeperException.NoNodeException(); &#125; byte lastdata[] = null; synchronized (n) &#123; lastdata = n.data; n.data = data; n.stat.setMtime(time); n.stat.setMzxid(zxid); n.stat.setVersion(version); n.copyStat(s); &#125; // now update if the path is in a quota subtree. String lastPrefix = getMaxPrefixWithQuota(path); if(lastPrefix != null) &#123; this.updateBytes(lastPrefix, (data == null ? 0 : data.length) - (lastdata == null ? 0 : lastdata.length)); &#125; dataWatches.triggerWatch(path, EventType.NodeDataChanged); return s;&#125; 1234567891011121314151617181920212223242526272829Set&lt;Watcher&gt; triggerWatch(String path, EventType type, Set&lt;Watcher&gt; supress) &#123; WatchedEvent e = new WatchedEvent(type, KeeperState.SyncConnected, path); HashSet&lt;Watcher&gt; watchers; synchronized (this) &#123; watchers = watchTable.remove(path); if (watchers == null || watchers.isEmpty()) &#123; if (LOG.isTraceEnabled()) &#123; ZooTrace.logTraceMessage(LOG, ZooTrace.EVENT_DELIVERY_TRACE_MASK, "No watchers for " + path); &#125; return null; &#125; for (Watcher w : watchers) &#123; HashSet&lt;String&gt; paths = watch2Paths.get(w); if (paths != null) &#123; paths.remove(path); &#125; &#125; &#125; for (Watcher w : watchers) &#123; if (supress != null &amp;&amp; supress.contains(w)) &#123; continue; &#125; w.process(e); &#125; return watchers; &#125; 从上面的代码我们可以总结出，如果想要处理一个 Watcher，需要执行的步骤如下所示： 将事件类型（EventType）、通知状态（WatchedEvent）、节点路径封装成一个 WatchedEvent 对象。 根据数据节点的节点路径从 watchTable 里面取出对应的 Watcher。如果没有找到 Watcher 对象，说明没有任何客户端在该数据节点上注册过 Watcher，直接退出。如果找到了 Watcher 就将其提取出来，同时会直接从 watchTable 和 watch2Paths 里删除 Watcher，即 Watcher 是一次性的，触发一次就失效了。 对于需要注册 Watcher 的请求，ZooKeeper 会把请求对应的 ServerCnxn 作为一个 Watcher 存储，所以这里调用的 process 方法实质上是 ServerCnxn 的对应方法，如下所示: NIOServerCnxn 类代码 1234567891011121314public void process(WatchedEvent event) &#123; ReplyHeader h = new ReplyHeader(-1, -1L, 0); if (LOG.isTraceEnabled()) &#123; ZooTrace.logTraceMessage(LOG, ZooTrace.EVENT_DELIVERY_TRACE_MASK, &quot;Deliver event &quot; + event + &quot; to 0x&quot; + Long.toHexString(this.sessionId) + &quot; through &quot; + this); &#125; // Convert WatchedEvent to a type that can be sent over the wire WatcherEvent e = event.getWrapper(); sendResponse(h, e, &quot;notification&quot;); &#125; 从上面的代码片段中，我们可以看出在请求头标记“-1”表示当前是一个通知，将 WatchedEvent 包装成 WatcherEvent 用于网络传输序列化，向客户端发送通知，真正的回调方法在客户端。 客户端回调Watcher客户端收到消息后，会调用 ClientCnxn 的 SendThread.readResponse 方法来进行统一处理，如清单所示。如果响应头 replyHdr 中标识的 Xid 为 02，表示是 ping，如果为-4，表示是验证包，如果是-1，表示这是一个通知类型的响应，然后进行反序列化、处理 chrootPath、还原 WatchedEvent、回调 Watcher 等步骤，其中回调 Watcher 步骤将 WacthedEvent 对象交给 EventThread 线程，在下一个轮询周期中进行 Watcher 回调。 123456789101112131415161718192021222324class SendThread extends ZooKeeperThread &#123; private long lastPingSentNs; private final ClientCnxnSocket clientCnxnSocket; private Random r = new Random(); private boolean isFirstConnect = true; void readResponse(ByteBuffer incomingBuffer) throws IOException &#123; ByteBufferInputStream bbis = new ByteBufferInputStream( incomingBuffer); BinaryInputArchive bbia = BinaryInputArchive.getArchive(bbis); ReplyHeader replyHdr = new ReplyHeader(); replyHdr.deserialize(bbia, "header"); if (replyHdr.getXid() == -2) &#123; // -2 is the xid for pings if (LOG.isDebugEnabled()) &#123; LOG.debug("Got ping response for sessionid: 0x" + Long.toHexString(sessionId) + " after " + ((System.nanoTime() - lastPingSentNs) / 1000000) + "ms"); &#125; return; &#125; SendThread 接收到服务端的通知事件后，会通过调用 EventThread 类的 queueEvent 方法将事件传给 EventThread 线程，queueEvent 方法根据该通知事件，从 ZKWatchManager 中取出所有相关的 Watcher: 12345678910111213141516171819202122232425class EventThread extends ZooKeeperThread &#123; private final LinkedBlockingQueue&lt;Object&gt; waitingEvents = new LinkedBlockingQueue&lt;Object&gt;(); ... private void queueEvent(WatchedEvent event, Set&lt;Watcher&gt; materializedWatchers) &#123; if (event.getType() == EventType.None &amp;&amp; sessionState == event.getState()) &#123; return; &#125; sessionState = event.getState(); final Set&lt;Watcher&gt; watchers; if (materializedWatchers == null) &#123; // materialize the watchers based on the event watchers = watcher.materialize(event.getState(), event.getType(), event.getPath()); &#125; else &#123; watchers = new HashSet&lt;Watcher&gt;(); watchers.addAll(materializedWatchers); &#125; WatcherSetEventPair pair = new WatcherSetEventPair(watchers, event); // queue the pair (watch set &amp; event) for later processing waitingEvents.add(pair); &#125; 客户端在识别出事件类型 EventType 之后，会从相应的 Watcher 存储中删除对应的 Watcher，获取到相关的 Watcher 之后，会将其放入 waitingEvents 队列，该队列从字面上就能理解是一个待处理队列，线程的 run 方法会不断对该该队列进行处理，这就是一种异步处理思维的实现。 ZKWatchManager 取出 Watcher 1234567891011121314151617public Set&lt;Watcher&gt; materialize(Watcher.Event.KeeperState state, Watcher.Event.EventType type, String clientPath) &#123; Set&lt;Watcher&gt; result = new HashSet&lt;Watcher&gt;(); switch (type) &#123; case NodeDataChanged: case NodeCreated: synchronized (dataWatches) &#123; addTo(dataWatches.remove(clientPath), result); &#125; synchronized (existWatches) &#123; addTo(existWatches.remove(clientPath), result); &#125; break;&#125; EventThread 线程的 run 方法 12345678910111213141516171819202122public void run() &#123; try &#123; isRunning = true; while (true) &#123; Object event = waitingEvents.take(); if (event == eventOfDeath) &#123; wasKilled = true; &#125; else &#123; processEvent(event); &#125; if (wasKilled) synchronized (waitingEvents) &#123; if (waitingEvents.isEmpty()) &#123; isRunning = false; break; &#125; &#125; &#125; &#125; catch (InterruptedException e) &#123; LOG.error("Event thread exiting due to interruption", e); &#125; &#125; ZooKeeper Watcher 特性总结1. 注册只能确保一次消费 无论是服务端还是客户端，一旦一个 Watcher 被触发，ZooKeeper 都会将其从相应的存储中移除。因此，开发人员在 Watcher 的使用上要记住的一点是需要反复注册。这样的设计有效地减轻了服务端的压力。如果注册一个 Watcher 之后一直有效，那么针对那些更新非常频繁的节点，服务端会不断地向客户端发送事件通知，这无论对于网络还是服务端性能的影响都非常大。 2. 客户端串行执行 客户端 Watcher 回调的过程是一个串行同步的过程，这为我们保证了顺序，同时，需要开发人员注意的一点是，千万不要因为一个 Watcher 的处理逻辑影响了整个客户端的 Watcher 回调。 3. 轻量级设计 WatchedEvent 是 ZooKeeper 整个 Watcher 通知机制的最小通知单元，这个数据结构中只包含三部分的内容：通知状态、事件类型和节点路径。也就是说，Watcher 通知非常简单，只会告诉客户端发生了事件，而不会说明事件的具体内容。例如针对 NodeDataChanged 事件，ZooKeeper 的 Watcher 只会通知客户指定数据节点的数据内容发生了变更，而对于原始数据以及变更后的新数据都无法从这个事件中直接获取到，而是需要客户端主动重新去获取数据，这也是 ZooKeeper 的 Watcher 机制的一个非常重要的特性。另外，客户端向服务端注册 Watcher 的时候，并不会把客户端真实的 Watcher 对象传递到服务端，仅仅只是在客户端请求中使用 boolean 类型属性进行了标记，同时服务端也仅仅只是保存了当前连接的 ServerCnxn 对象。这样轻量级的 Watcher 机制设计，在网络开销和服务端内存开销上都是非常廉价的。 ACL–保障数据的安全Zookeeper内部存储了分布式系统运行时状态的元数据，这些元数据会直接影响基于Zookeeper进行构造的分布式系统的运行状态，如何保障系统中数据的安全，从而避免因误操作而带来的数据随意变更而导致的数据库异常十分重要，Zookeeper提供了一套完善的ACL权限控制机制来保障数据的安全。 我们可以从三个方面来理解ACL机制：权限模式（Scheme）、授权对象（ID）、权限（Permission），通常使用：schema:id:permission 来标识一个有效的ACL信息。 权限模式 Schema常用的权限模式如下： IPIP： 使用 IP 识别用户，可以精确匹配 IP，也可以匹配到 IP 段 ip:168.192.1.23 ：精确匹配到 IP ip:168.192.0.1/24：模糊匹配 IP 段，168.192.0.* Note： IP 地址是 32 位，十进制表示 4 个十进制；IP 段，表示前面多少位相同。 DigestDigest，类似 username:password，用户名和密码。便于区分不同应用来进行权限控制。Zookeeper会对其进行SHA-1加密和BASE64编码。 World没有密码，对所有用户都开放权限。可以看作特殊的 Digest 模式。 Super只有超级用户，才有权限，也可看作特殊的 Digest 模式。 授权对象 ID授权对象是指，权限赋予的用户或者一个实体，例如：IP 地址或者机器。 授权模式 schema 与 授权对象 ID 之间关系： 权限 PermissionZooKeeper 中数据节点的权限分为 5 类： Create（C）：创建子节点 Delete（D）：删除子节点 Read（R）：读取当前节点，以及子节点列表 Write（W）：更新当前节点 Admin（A）：当前节点的 ACL 管理 附录UGO 权限控制UGO，（User，Group，Others），是（用户，组，权限）的简称。 Linux\Unix 文件系统，采用 UGO 的权限控制，针对文件/文件夹的创建者，创建者所在组以及其他用户，分别分配不同的权限。 UGO 的权限控制，是基于用户的，跟系统的用户体系严格绑定。 ACL 权限控制ACL，Access Control List，访问权限列表。 从 data 角度出发，赋予权限。 Note： 目前绝大部分 Unix 系统都支持了 ACL，Linux 内核从 2.6+ 也开始支持 ACL。 RBAC 权限控制RBAC，(Role-Based Access Control )基于角色的访问控制。 一个用户拥有若干角色，每一个角色拥有若干权限。 构成 用户-角色-权限 的授权模型。 在这种模型中，用户与角色之间，角色与权限之间，一般者是多对多的关系。 参考资料 从Paxos到Zookeeper分布式一致性原理与实践 Apache ZooKeeper Watcher 机制源码解释 ZooKeeper 技术内幕：ACL 权限访问控制 源码分析之Watcher机制（三）之ZooKeeper]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper选举机制及源码解析]]></title>
    <url>%2F2019%2F08%2F15%2Fzookeeper-leader-election%2F</url>
    <content type="text"><![CDATA[Zookeeper选举概述时机Leader选举是保证分布式数据一致性的关键所在。当Zookeeper集群中的一台服务器出现以下两种情况之一时，需要进入Leader选举: (1) 服务器初始化启动。 (2) 服务器运行期间无法和Leader保持连接。 在Zookeeper运行期间，Leader与非Leader服务器各司其职，即便当有非Leader服务器宕机或新加入，此时也不会影响Leader，但是一旦Leader服务器挂了，那么整个集群将暂停对外服务，进入新一轮Leader选举，其过程和启动时期的Leader选举过程基本一致。假设正在运行的有Server1、Server2、Server3三台服务器，当前Leader是Server2，若某一时刻Leader挂了，此时便开始Leader选举。 过程若进行Leader选举，则至少需要两台机器，这里选取3台机器组成的服务器集群为例。在集群初始化阶段，当有一台服务器Server1启动时，其单独无法进行和完成Leader选举，当第二台服务器Server2启动时，此时两台机器可以相互通信，每台机器都试图找到Leader，于是进入Leader选举过程。Leader 选举过程，本质就是广播优先级消息的过程，选出数据最新的服务节点，选出优先级最高的服务节点，基本步骤： 每个Server发出一个投票。Server1和Server2都会将自己作为Leader服务器来进行投票，广播自己的优先级标识 (sid，zxid)。例如服务器启动时，此时Server1的投票为(1, 0)，Server2的投票为(2, 0)，然后各自将这个投票发给集群中其他机器。 接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。 处理投票。服务器节点收到其他广播消息后，跟自己的优先级对比，自己优先级低，则变更当前节点投票的优先级(sid，zxid) ，并广播变更后的结果，优先级PK规则: 优先比较 zxid （事务 ID），其次比较sid（服务器ID） sid (服务器 ID) 是节点配置文件中设定的 对于Server1而言，它的投票是(1, 0)，接收Server2的投票为(2, 0)，首先会比较两者的ZXID，均为0，再比较myid，此时Server2的myid最大，于是更新自己的投票为(2, 0)，然后重新投票，对于Server2而言，其无须更新自己的投票，只是再次向集群中所有机器发出上一次投票信息即可。 统计投票。当任意一个服务器节点收到的投票数，超过了法定数量(quorum)亦即集群规模大小的半数以上，则升级为 Leader，并广播结果。对于Server1、Server2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出了Leader。 改变服务器状态。一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。 Leader选举算法分析在3.4.0后的Zookeeper的版本只保留了TCP版本的FastLeaderElection选举算法。当一台机器进入Leader选举时，当前集群可能会处于以下两种状态 集群中已经存在Leader。 集群中不存在Leader。 对于集群中已经存在Leader而言，此种情况一般都是某台机器启动得较晚，在其启动之前，集群已经在正常工作，对这种情况，该机器试图去选举Leader时，会被告知当前服务器的Leader信息，对于该机器而言，仅仅需要和Leader机器建立起连接，并进行状态同步即可。而在集群中不存在Leader情况下则会相对复杂，其步骤如下。 第一次投票无论哪种导致进行Leader选举，集群的所有机器都处于试图选举出一个Leader的状态，即LOOKING状态，LOOKING机器会向所有其他机器发送消息，该消息称为投票。投票中包含了(sid，zxid) 形式来标识一次投票信息。假定Zookeeper由5台机器组成，sid分别为1、2、3、4、5，zxid分别为9、9、9、8、8，并且此时sid为2的机器是Leader机器，某一时刻，1、2所在机器出现故障，因此集群开始进行Leader选举。在第一次投票时，每台机器都会将自己作为投票对象，于是sid为3、4、5的机器投票情况分别为(3, 9)，(4, 8)， (5, 8)。 变更投票每台机器发出投票后，也会收到其他机器的投票，每台机器会根据一定规则来处理收到的其他机器的投票，并以此来决定是否需要变更自己的投票，这个规则也是整个Leader选举算法的核心所在，其中术语描述如下: vote_sid：接收到的投票中所推举Leader服务器的sid。 vote_zxid：接收到的投票中所推举Leader服务器的zxid。 self_sid：当前服务器自己的sid。 self_zxid：当前服务器自己的zxid。 每次对收到的投票的处理，都是对(vote_sid, vote_zxid)和(self_sid, self_zxid)对比的过程。 规则一：如果vote_zxid大于self_zxid，就认可当前收到的投票，并再次将该投票发送出去。 规则二：如果vote_zxid小于self_zxid，那么坚持自己的投票，不做任何变更。 规则三：如果vote_zxid等于self_zxid，那么就对比两者的SID，如果vote_sid大于self_sid，那么就认可当前收到的投票，并再次将该投票发送出去。 规则四：如果vote_zxid等于self_zxid，并且vote_sid小于self_sid，那么坚持自己的投票，不做任何变更。 结合上面规则，给出下面的集群变更过程。 确定Leader经过第二轮投票后，集群中的每台机器都会再次接收到其他机器的投票，然后开始统计投票，如果一台机器收到了超过半数的相同投票，那么这个投票对应的SID机器即为Leader。此时Server3将成为Leader。 由上面规则可知，通常那台服务器上的数据越新（zxid会越大），其成为Leader的可能性越大，也就越能够保证数据的恢复。如果zxid相同，则sid越大机会越大。 Leader选举实现细节服务器状态服务器具有四种状态，分别是LOOKING、FOLLOWING、LEADING、OBSERVING。 LOOKING：寻找Leader状态。当服务器处于该状态时，它会认为当前集群中没有Leader，因此需要进入Leader选举状态。 FOLLOWING：跟随者状态。表明当前服务器角色是Follower。 LEADING：领导者状态。表明当前服务器角色是Leader。 OBSERVING：观察者状态。表明当前服务器角色是Observer。 投票数据结构每个投票中包含了两个最基本的信息，所推举服务器的sid和zxid，投票（Vote）在Zookeeper中包含字段如下 属性 说明 id 被推举 Leader 的 sid zxid 被推举 Leader 的zxid electionEpoch 投票的轮次，逻辑时钟，用来判断多个投票是否在同一轮选举周期中，该值在服务端是一个自增序列，每次进入新一轮的投票后，都会对该值进行加1操作 peerEpoch 被推举 Leader 的 epoch state 当前服务器的状态 一次 Leader 选举过程，属于同一个 electionEpoch，结束时，会选出新的 Leader；服务器节点，在比较 (sid，zxid) 之前，会先比较选举轮次 electionEpoch，只有同一轮次的 Leader 投票信息才是有效的： 外部投票轮次 &gt; 内部投票轮次，更新内部投票，并且触发当前节点投票信息的重新广播 外部投票轮次 &lt; 内部投票轮次，直接忽略当前的外部投票 外部投票轮次 = 内部投票轮次，进一步比较 (sid，zxid) 疑问：Leader 负责执行所有的事务操作，一次事务操作， Leader 如何将事务操作同步到 Follower 和 Observer ？同步、异步？ 如何保证同步过程中，事务一定执行成功？事务失败的影响？ Leader 上执行的事务状态，通过 Zab 状态更新的广播协议，更新到 Follower 和 Observer。 QuorumCnxManager：网络I/O每台服务器在启动的过程中，会启动一个QuorumPeerManager，负责各台服务器之间的底层Leader选举过程中的网络通信。 消息队列QuorumCnxManager内部维护了一系列的队列，用来保存接收到的、待发送的消息以及消息的发送器，除接收队列以外，其他队列都按照SID分组形成队列集合，如一个集群中除了自身还有3台机器，那么就会为这3台机器分别创建一个发送队列，互不干扰。 recvQueue：消息接收队列，用于存放那些从其他服务器接收到的消息。 queueSendMap：消息发送队列，用于保存那些待发送的消息，按照sid进行分组。 senderWorkerMap：发送器集合，每个SenderWorker消息发送器，都对应一台远程Zookeeper服务器，负责消息的发送，也按照SID进行分组。 lastMessageSent：最近发送过的消息，为每个sid保留最近发送过的一个消息。 建立连接为了能够相互投票，Zookeeper集群中的所有机器都需要两两建立起网络连接。QuorumCnxManager在启动时会创建一个ServerSocket来监听Leader选举的通信端口(默认为3888)。开启监听后，Zookeeper能够不断地接收到来自其他服务器的创建连接请求，在接收到其他服务器的TCP连接请求时，会进行处理。为了避免两台机器之间重复地创建TCP连接，Zookeeper只允许sid大的服务器主动和其他机器建立连接，否则断开连接。在接收到创建连接请求后，服务器通过对比自己和远程服务器的sid值来判断是否接收连接请求，如果当前服务器发现自己的sid更大，那么会断开当前连接，然后自己主动和远程服务器建立连接。一旦连接建立，就会根据远程服务器的SID来创建相应的消息发送器SendWorker和消息接收器RecvWorker，并启动。 消息接收与发送消息接收：由消息接收器RecvWorker负责，由于Zookeeper为每个远程服务器都分配一个单独的RecvWorker，因此，每个RecvWorker只需要不断地从这个TCP连接中读取消息，并将其保存到recvQueue队列中。 消息发送：由于Zookeeper为每个远程服务器都分配一个单独的SendWorker，因此，每个SendWorker只需要不断地从对应的消息发送队列中获取出一个消息发送即可，同时将这个消息放入lastMessageSent中。在SendWorker中，一旦Zookeeper发现针对当前服务器的消息发送队列为空，那么此时需要从lastMessageSent中取出一个最近发送过的消息来进行再次发送，这是为了解决接收方在消息接收前或者接收到消息后服务器挂了，导致消息尚未被正确处理。同时，Zookeeper能够保证接收方在处理消息时，会对重复消息进行正确的处理。 FastLeaderElection：选举算法核心选票管理 sendqueue：选票发送队列，用于保存待发送的选票。 recvqueue：选票接收队列，用于保存接收到的外部投票。 WorkerReceiver：选票接收器。其会不断地从QuorumCnxManager中获取其他服务器发来的选举消息，并将其转换成一个选票，然后保存到recvqueue中，在选票接收过程中，如果发现该外部选票的选举轮次小于当前服务器的，那么忽略该外部投票，同时立即发送自己的内部投票。 WorkerSender：选票发送器，不断地从sendqueue中获取待发送的选票，并将其传递到底层QuorumCnxManager中。 算法核心 上图展示了FastLeaderElection模块是如何与底层网络I/O进行交互的。Leader选举的基本流程如下: 自增选举轮次。Zookeeper规定所有有效的投票都必须在同一轮次中，在开始新一轮投票时，会首先对logicalclock进行自增操作。 初始化选票。在开始进行新一轮投票之前，每个服务器都会初始化自身的选票，并且在初始化阶段，每台服务器都会将自己推举为Leader。 发送初始化选票。完成选票的初始化后，服务器就会发起第一次投票。Zookeeper会将刚刚初始化好的选票放入sendqueue中，由发送器WorkerSender负责发送出去。 接收外部投票。每台服务器会不断地从recvqueue队列中获取外部选票。如果服务器发现无法获取到任何外部投票，那么就会立即确认自己是否和集群中其他服务器保持着有效的连接，如果没有连接，则马上建立连接，如果已经建立了连接，则再次发送自己当前的内部投票。 判断选举轮次。在发送完初始化选票之后，接着开始处理外部投票。在处理外部投票时，会根据选举轮次来进行不同的处理。 外部投票的选举轮次大于内部投票。若服务器自身的选举轮次落后于该外部投票对应服务器的选举轮次，那么就会立即更新自己的选举轮次(logicalclock)，并且清空所有已经收到的投票，然后使用初始化的投票来进行PK以确定是否变更内部投票。最终再将内部投票发送出去。 外部投票的选举轮次小于内部投票。若服务器接收的外选票的选举轮次落后于自身的选举轮次，那么Zookeeper就会直接忽略该外部投票，不做任何处理，并返回步骤4。 外部投票的选举轮次等于内部投票。此时可以开始进行选票PK。 选票PK。在进行选票PK时，符合任意一个条件就需要变更投票。 若外部投票中推举的Leader服务器的选举轮次大于内部投票，那么需要变更投票。 若选举轮次一致，那么就对比两者的zxid，若外部投票的zxid大，那么需要变更投票。 若两者的ZXID一致，那么就对比两者的sid，若外部投票的sid大，那么就需要变更投票。 变更投票。经过PK后，若确定了外部投票优于内部投票，那么就变更投票，即使用外部投票的选票信息来覆盖内部投票，变更完成后，再次将这个变更后的内部投票发送出去。 选票归档。无论是否变更了投票，都会将刚刚收到的那份外部投票放入选票集合recvset中进行归档。recvset用于记录当前服务器在本轮次的Leader选举中收到的所有外部投票（按照服务队的SID区别，如{(1, vote1), (2, vote2)…}）。 统计投票。完成选票归档后，就可以开始统计投票，统计投票是为了统计集群中是否已经有过半的服务器认可了当前的内部投票，如果确定已经有过半服务器认可了该投票，则终止投票。否则返回步骤4。 更新服务器状态。若已经确定可以终止投票，那么就开始更新服务器状态，服务器首选判断当前被过半服务器认可的投票所对应的Leader服务器是否是自己，若是自己，则将自己的服务器状态更新为LEADING，若不是，则根据具体情况来确定自己是FOLLOWING或是OBSERVING。 以上10个步骤就是FastLeaderElection的核心，其中步骤4-9会经过几轮循环，直到有Leader选举产生。 FastLeaderElection源码分析类的继承关系 1public class FastLeaderElection implements Election &#123;&#125; 说明：FastLeaderElection实现了Election接口，其需要实现接口中定义的lookForLeader方法和shutdown方法，其是标准的Fast Paxos算法的实现，各服务器之间基于TCP协议进行选举。 类的内部类FastLeaderElection有三个较为重要的内部类，分别为Notification、ToSend、Messenger。 Notification类Notification表示收到的选举投票信息（其他服务器发来的选举投票信息），其包含了被选举者的id、zxid、选举周期等信息，其buildMsg方法将选举信息封装至ByteBuffer中再进行发送。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Notifications are messages that let other peers know that * a given peer has changed its vote, either because it has * joined leader election or because it learned of another * peer with higher zxid or same zxid and higher server id */static public class Notification &#123; /* * Format version, introduced in 3.4.6 */ public final static int CURRENTVERSION = 0x2; int version; /* * Proposed leader */ long leader; /* * zxid of the proposed leader */ long zxid; /* * Epoch */ long electionEpoch; /* * current state of sender */ QuorumPeer.ServerState state; /* * Address of sender */ long sid; QuorumVerifier qv; /*Notification * epoch of the proposed leader */ long peerEpoch;&#125; ToSend类ToSend表示发送给其他服务器的选举投票信息，也包含了被选举者的id、zxid、选举周期等信息。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Messages that a peer wants to send to other peers. * These messages can be both Notifications and Acks * of reception of notification. */static public class ToSend &#123; static enum mType &#123;crequest, challenge, notification, ack&#125; ToSend(mType type, long leader, long zxid, long electionEpoch, ServerState state, long sid, long peerEpoch, byte[] configData) &#123; this.leader = leader; this.zxid = zxid; this.electionEpoch = electionEpoch; this.state = state; this.sid = sid; this.peerEpoch = peerEpoch; this.configData = configData; &#125; /* * Proposed leader in the case of notification */ long leader; /* * id contains the tag for acks, and zxid for notifications */ long zxid; /* * Epoch */ long electionEpoch; /* * Current state; */ QuorumPeer.ServerState state; /* * Address of recipient */ long sid; /* * Used to send a QuorumVerifier (configuration info) */ byte[] configData = dummyData; /* * Leader epoch */ long peerEpoch;&#125; Messenger类Messenger包含了WorkerReceiver和WorkerSender两个内部类: 123456protected class Messenger &#123; // 选票发送器 WorkerSender ws; // 选票接收器 WorkerReceiver wr; &#125; 构造函数: 123456789101112131415Messenger(QuorumCnxManager manager) &#123; // 创建WorkerSender this.ws = new WorkerSender(manager); this.wsThread = new Thread(this.ws, "WorkerSender[myid=" + self.getId() + "]"); this.wsThread.setDaemon(true); // 创建WorkerReceiver this.wr = new WorkerReceiver(manager); this.wrThread = new Thread(this.wr, "WorkerReceiver[myid=" + self.getId() + "]"); this.wrThread.setDaemon(true);&#125; WorkerReceiver123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226/** * Receives messages from instance of QuorumCnxManager on * method run(), and processes such messages. */class WorkerReceiver extends ZooKeeperThread &#123; volatile boolean stop; // 服务器之间的连接 QuorumCnxManager manager; WorkerReceiver(QuorumCnxManager manager) &#123; super("WorkerReceiver"); this.stop = false; this.manager = manager; &#125; public void run() &#123; Message response; while (!stop) &#123; // Sleeps on receive try &#123; // 从recvQueue中取出一个选举投票消息（从其他服务器发送过来） response = manager.pollRecvQueue(3000, TimeUnit.MILLISECONDS); // 无投票，跳过 if(response == null) continue; // 大小校验 ... response.buffer.clear(); // Instantiate Notification and set its attributes // 创建接收通知 Notification n = new Notification(); int rstate = response.buffer.getInt(); long rleader = response.buffer.getLong(); long rzxid = response.buffer.getLong(); long relectionEpoch = response.buffer.getLong(); long rpeerepoch; int version = 0x0; if (!backCompatibility28) &#123; rpeerepoch = response.buffer.getLong(); if (!backCompatibility40) &#123; /* * Version added in 3.4.6 */ version = response.buffer.getInt(); &#125; else &#123; LOG.info("Backward compatibility mode (36 bits), server id: &#123;&#125;", response.sid); &#125; &#125; else &#123; LOG.info("Backward compatibility mode (28 bits), server id: &#123;&#125;", response.sid); rpeerepoch = ZxidUtils.getEpochFromZxid(rzxid); &#125; QuorumVerifier rqv = null; // check if we have a version that includes config. If so extract config info from message. if (version &gt; 0x1) &#123; int configLength = response.buffer.getInt(); byte b[] = new byte[configLength]; response.buffer.get(b); synchronized(self) &#123; try &#123; rqv = self.configFromString(new String(b)); QuorumVerifier curQV = self.getQuorumVerifier(); if (rqv.getVersion() &gt; curQV.getVersion()) &#123; LOG.info("&#123;&#125; Received version: &#123;&#125; my version: &#123;&#125;", self.getId(), Long.toHexString(rqv.getVersion()), Long.toHexString(self.getQuorumVerifier().getVersion())); if (self.getPeerState() == ServerState.LOOKING) &#123; LOG.debug("Invoking processReconfig(), state: &#123;&#125;", self.getServerState()); self.processReconfig(rqv, null, null, false); if (!rqv.equals(curQV)) &#123; LOG.info("restarting leader election"); self.shuttingDownLE = true; self.getElectionAlg().shutdown(); break; &#125; &#125; else &#123; LOG.debug("Skip processReconfig(), state: &#123;&#125;", self.getServerState()); &#125; &#125; &#125; catch (IOException e) &#123; LOG.error("Something went wrong while processing config received from &#123;&#125;", response.sid); &#125; catch (ConfigException e) &#123; LOG.error("Something went wrong while processing config received from &#123;&#125;", response.sid); &#125; &#125; &#125; else &#123; LOG.info("Backward compatibility mode (before reconfig), server id: &#123;&#125;", response.sid); &#125; /* * If it is from a non-voting server (such as an observer or * a non-voting follower), respond right away. */ if(!validVoter(response.sid)) &#123; Vote current = self.getCurrentVote(); QuorumVerifier qv = self.getQuorumVerifier(); ToSend notmsg = new ToSend(ToSend.mType.notification, current.getId(), current.getZxid(), logicalclock.get(), self.getPeerState(), response.sid, current.getPeerEpoch(), qv.toString().getBytes()); sendqueue.offer(notmsg); &#125; else &#123; // Receive new message if (LOG.isDebugEnabled()) &#123; LOG.debug("Receive new notification message. My id = " + self.getId()); &#125; // State of peer that sent this message // 推选者的状态 QuorumPeer.ServerState ackstate = QuorumPeer.ServerState.LOOKING; switch (rstate) &#123; case 0: ackstate = QuorumPeer.ServerState.LOOKING; break; case 1: ackstate = QuorumPeer.ServerState.FOLLOWING; break; case 2: ackstate = QuorumPeer.ServerState.LEADING; break; case 3: ackstate = QuorumPeer.ServerState.OBSERVING; break; default: continue; &#125; n.leader = rleader; n.zxid = rzxid; n.electionEpoch = relectionEpoch; n.state = ackstate; n.sid = response.sid; n.peerEpoch = rpeerepoch; n.version = version; n.qv = rqv; /* * Print notification info */ if(LOG.isInfoEnabled())&#123; printNotification(n); &#125; /* * If this server is looking, then send proposed leader */ if(self.getPeerState() == QuorumPeer.ServerState.LOOKING)&#123; recvqueue.offer(n); /* * Send a notification back if the peer that sent this * message is also looking and its logical clock is * lagging behind. */ if((ackstate == QuorumPeer.ServerState.LOOKING) &amp;&amp; (n.electionEpoch &lt; logicalclock.get()))&#123; // 获取自己的投票 Vote v = getVote(); QuorumVerifier qv = self.getQuorumVerifier(); // 构造ToSend消息 ToSend notmsg = new ToSend(ToSend.mType.notification, v.getId(), v.getZxid(), logicalclock.get(), self.getPeerState(), response.sid, v.getPeerEpoch(), qv.toString().getBytes()); // 放入sendqueue队列，等待发送 sendqueue.offer(notmsg); &#125; &#125; else &#123; /* * If this server is not looking, but the one that sent the ack * is looking, then send back what it believes to be the leader. */ Vote current = self.getCurrentVote(); if(ackstate == QuorumPeer.ServerState.LOOKING)&#123; if(LOG.isDebugEnabled())&#123; LOG.debug("Sending new notification. My id =&#123;&#125; recipient=&#123;&#125; zxid=0x&#123;&#125; leader=&#123;&#125; config version = &#123;&#125;", self.getId(), response.sid, Long.toHexString(current.getZxid()), current.getId(), Long.toHexString(self.getQuorumVerifier().getVersion())); &#125; QuorumVerifier qv = self.getQuorumVerifier(); ToSend notmsg = new ToSend( ToSend.mType.notification, current.getId(), current.getZxid(), current.getElectionEpoch(), self.getPeerState(), response.sid, current.getPeerEpoch(), qv.toString().getBytes()); sendqueue.offer(notmsg); &#125; &#125; &#125; &#125; catch (InterruptedException e) &#123; LOG.warn("Interrupted Exception while waiting for new message" + e.toString()); &#125; &#125; LOG.info("WorkerReceiver is down"); &#125;&#125; WorkerReceiver实现了Runnable接口，是选票接收器。其会不断地从QuorumCnxManager中获取其他服务器发来的选举消息，并将其转换成一个选票，然后保存到recvqueue中，在选票接收过程中，如果发现该外部选票的选举轮次小于当前服务器的，那么忽略该外部投票，同时立即发送自己的内部投票。其是将QuorumCnxManager的Message转化为FastLeaderElection的Notification。 其中，WorkerReceiver的主要逻辑在run方法中，其首先会从QuorumCnxManager中的recvQueue队列中取出其他服务器发来的选举消息，消息封装在Message数据结构中。然后判断消息中的服务器id是否包含在可以投票的服务器集合中，若不是，则会将本服务器的内部投票发送给该服务器，其流程如下: 123456789101112131415161718192021/* * If it is from a non-voting server (such as an observer or * a non-voting follower), respond right away. */if(!validVoter(response.sid)) &#123; // 获取自己的投票 Vote current = self.getCurrentVote(); QuorumVerifier qv = self.getQuorumVerifier(); // 构造ToSend消息 ToSend notmsg = new ToSend(ToSend.mType.notification, current.getId(), current.getZxid(), logicalclock.get(), self.getPeerState(), response.sid, current.getPeerEpoch(), qv.toString().getBytes()); // 放入sendqueue队列，等待发送 sendqueue.offer(notmsg);&#125; 若包含该服务器，则根据消息（Message）解析出投票服务器的投票信息并将其封装为Notification，然后判断当前服务器是否为LOOKING，若为LOOKING，则直接将Notification放入FastLeaderElection的recvqueue（区别于recvQueue）中。然后判断投票服务器是否为LOOKING状态，并且其选举周期小于当前服务器的逻辑时钟，则将本（当前）服务器的内部投票发送给该服务器，否则，直接忽略掉该投票。其流程如下: 12345678910111213141516171819202122232425262728293031/* * If this server is looking, then send proposed leader */if(self.getPeerState() == QuorumPeer.ServerState.LOOKING)&#123; // 本服务器为LOOKING状态 // 将消息放入recvqueue中 recvqueue.offer(n); /* * Send a notification back if the peer that sent this * message is also looking and its logical clock is * lagging behind. */ if((ackstate == QuorumPeer.ServerState.LOOKING) // 推选者服务器为LOOKING状态 &amp;&amp; (n.electionEpoch &lt; logicalclock.get()))&#123; // 选举周期小于逻辑时钟 // 创建新的投票 Vote v = getVote(); QuorumVerifier qv = self.getQuorumVerifier(); // 构造新的发送消息（本服务器自己的投票） ToSend notmsg = new ToSend(ToSend.mType.notification, v.getId(), v.getZxid(), logicalclock.get(), self.getPeerState(), response.sid, v.getPeerEpoch(), qv.toString().getBytes()); // 将发送消息放置于队列，等待发送 sendqueue.offer(notmsg); &#125;&#125; 若本服务器的状态不为LOOKING，则会根据投票服务器中解析的version信息来构造ToSend消息，放入sendqueue，等待发送，起流程如下: 123456789101112131415161718192021222324252627282930313233&#125; else &#123; // 本服务器状态不为LOOKING /* * If this server is not looking, but the one that sent the ack * is looking, then send back what it believes to be the leader. */ // 获取当前投票 Vote current = self.getCurrentVote(); if(ackstate == QuorumPeer.ServerState.LOOKING)&#123; // 为LOOKING状态 if(LOG.isDebugEnabled())&#123; LOG.debug("Sending new notification. My id =&#123;&#125; recipient=&#123;&#125; zxid=0x&#123;&#125; leader=&#123;&#125; config version = &#123;&#125;", self.getId(), response.sid, Long.toHexString(current.getZxid()), current.getId(), Long.toHexString(self.getQuorumVerifier().getVersion())); &#125; QuorumVerifier qv = self.getQuorumVerifier(); // 构造ToSend消息 ToSend notmsg = new ToSend( ToSend.mType.notification, current.getId(), current.getZxid(), current.getElectionEpoch(), self.getPeerState(), response.sid, current.getPeerEpoch(), qv.toString().getBytes()); // 将发送消息放置于队列，等待发送 sendqueue.offer(notmsg); &#125; &#125;&#125; WorkerSender12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * This worker simply dequeues a message to send and * and queues it on the manager's queue. */class WorkerSender extends ZooKeeperThread &#123; volatile boolean stop; QuorumCnxManager manager; WorkerSender(QuorumCnxManager manager)&#123; super("WorkerSender"); this.stop = false; this.manager = manager; &#125; public void run() &#123; while (!stop) &#123; try &#123; ToSend m = sendqueue.poll(3000, TimeUnit.MILLISECONDS); if(m == null) continue; process(m); &#125; catch (InterruptedException e) &#123; break; &#125; &#125; LOG.info("WorkerSender is down"); &#125; /** * Called by run() once there is a new message to send. * * @param m message to send */ void process(ToSend m) &#123; ByteBuffer requestBuffer = buildMsg(m.state.ordinal(), m.leader, m.zxid, m.electionEpoch, m.peerEpoch, m.configData); manager.toSend(m.sid, requestBuffer); &#125;&#125; WorkerSender也实现了Runnable接口，为选票发送器，其会不断地从sendqueue中获取待发送的选票，并将其传递到底层QuorumCnxManager中，其过程是将FastLeaderElection的ToSend转化为QuorumCnxManager的Message。 类的构造函数 12345public FastLeaderElection(QuorumPeer self, QuorumCnxManager manager)&#123; this.stop = false; this.manager = manager; starter(self, manager);&#125; 构造函数中初始化了stop字段和manager字段，并且调用了starter函数: 123456789private void starter(QuorumPeer self, QuorumCnxManager manager) &#123; this.self = self; proposedLeader = -1; proposedZxid = -1; sendqueue = new LinkedBlockingQueue&lt;ToSend&gt;(); recvqueue = new LinkedBlockingQueue&lt;Notification&gt;(); this.messenger = new Messenger(manager);&#125; 核心函数分析sendNotifications函数 1234567891011121314151617181920212223private void sendNotifications() &#123; for (long sid : self.getCurrentAndNextConfigVoters()) &#123; // 遍历投票参与者集合 QuorumVerifier qv = self.getQuorumVerifier(); // 构造发送消息 ToSend notmsg = new ToSend(ToSend.mType.notification, proposedLeader, proposedZxid, logicalclock.get(), QuorumPeer.ServerState.LOOKING, sid, proposedEpoch, qv.toString().getBytes()); if(LOG.isDebugEnabled())&#123; LOG.debug("Sending Notification: " + proposedLeader + " (n.leader), 0x" + Long.toHexString(proposedZxid) + " (n.zxid), 0x" + Long.toHexString(logicalclock.get()) + " (n.round), " + sid + " (recipient), " + self.getId() + " (myid), 0x" + Long.toHexString(proposedEpoch) + " (n.peerEpoch)"); &#125; // 将发送消息放置于队列 sendqueue.offer(notmsg); &#125;&#125; 其会遍历所有的参与者投票集合，然后将自己的选票信息发送至上述所有的投票者集合，其并非同步发送，而是将ToSend消息放置于sendqueue中，之后由WorkerSender进行发送。 totalOrderPredicate函数1234567891011121314151617181920212223242526/** * Check if a pair (server id, zxid) succeeds our * current vote. * * @param id Server identifier * @param zxid Last zxid observed by the issuer of this vote */protected boolean totalOrderPredicate(long newId, long newZxid, long newEpoch, long curId, long curZxid, long curEpoch) &#123; LOG.debug("id: " + newId + ", proposed id: " + curId + ", zxid: 0x" + Long.toHexString(newZxid) + ", proposed zxid: 0x" + Long.toHexString(curZxid)); if(self.getQuorumVerifier().getWeight(newId) == 0)&#123; return false; &#125; /* * We return true if one of the following three cases hold: * 1- New epoch is higher * 2- New epoch is the same as current epoch, but new zxid is higher * 3- New epoch is the same as current epoch, new zxid is the same * as current zxid, but server id is higher. */ return ((newEpoch &gt; curEpoch) || ((newEpoch == curEpoch) &amp;&amp; ((newZxid &gt; curZxid) || ((newZxid == curZxid) &amp;&amp; (newId &gt; curId)))));&#125; 该函数将接收的投票与自身投票进行PK，查看是否消息中包含的服务器id是否更优，其按照epoch、zxid、id的优先级进行PK。 termPredicate函数12345678910111213141516171819202122232425262728293031/** * Termination predicate. Given a set of votes, determines if have * sufficient to declare the end of the election round. * * @param votes * Set of votes * @param vote * Identifier of the vote received last */protected boolean termPredicate(Map&lt;Long, Vote&gt; votes, Vote vote) &#123; SyncedLearnerTracker voteSet = new SyncedLearnerTracker(); voteSet.addQuorumVerifier(self.getQuorumVerifier()); if (self.getLastSeenQuorumVerifier() != null &amp;&amp; self.getLastSeenQuorumVerifier().getVersion() &gt; self .getQuorumVerifier().getVersion()) &#123; voteSet.addQuorumVerifier(self.getLastSeenQuorumVerifier()); &#125; /* * First make the views consistent. Sometimes peers will have different * zxids for a server depending on timing. */ for (Map.Entry&lt;Long, Vote&gt; entry : votes.entrySet()) &#123; // 遍历已经接收的投票集合 if (vote.equals(entry.getValue())) &#123; // 将等于当前投票的项放入set voteSet.addAck(entry.getKey()); &#125; &#125; //统计set，查看投某个id的票数是否超过一半 return voteSet.hasAllQuorums();&#125; 该函数用于判断Leader选举是否结束，即是否有一半以上的服务器选出了相同的Leader，其过程是将收到的选票与当前选票进行对比，选票相同的放入同一个集合，之后判断选票相同的集合是否超过了半数。 checkLeader函数 12345678910111213141516171819202122232425262728293031323334/** * In the case there is a leader elected, and a quorum supporting * this leader, we have to check if the leader has voted and acked * that it is leading. We need this check to avoid that peers keep * electing over and over a peer that has crashed and it is no * longer leading. * * @param votes set of votes * @param leader leader id * @param electionEpoch epoch id */protected boolean checkLeader( Map&lt;Long, Vote&gt; votes, long leader, long electionEpoch)&#123; boolean predicate = true; /* * If everyone else thinks I'm the leader, I must be the leader. * The other two checks are just for the case in which I'm not the * leader. If I'm not the leader and I haven't received a message * from leader stating that it is leading, then predicate is false. */ if(leader != self.getId())&#123; // 自己不为leader if(votes.get(leader) == null) predicate = false; // 还未选出leader else if(votes.get(leader).getState() != ServerState.LEADING) predicate = false; // 选出的leader还未给出ack信号，其他服务器还不知道leader &#125; else if(logicalclock.get() != electionEpoch) &#123; // 逻辑时钟不等于选举周期 predicate = false; &#125; return predicate;&#125; 该函数检查是否已经完成了Leader的选举，此时Leader的状态应该是LEADING状态。 lookForLeader函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202/** * Starts a new round of leader election. Whenever our QuorumPeer * changes its state to LOOKING, this method is invoked, and it * sends notifications to all other peers. */public Vote lookForLeader() throws InterruptedException &#123; // 注册jmx ... try &#123; HashMap&lt;Long, Vote&gt; recvset = new HashMap&lt;Long, Vote&gt;(); HashMap&lt;Long, Vote&gt; outofelection = new HashMap&lt;Long, Vote&gt;(); int notTimeout = finalizeWait; synchronized(this)&#123; // 更新逻辑时钟，每进行一轮选举，都需要更新逻辑时钟 logicalclock.incrementAndGet(); // 更新选票 updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch()); &#125; LOG.info("New election. My id = " + self.getId() + ", proposed zxid=0x" + Long.toHexString(proposedZxid)); // 向其他服务器发送自己的选票 sendNotifications(); /* * Loop in which we exchange notifications until we find a leader */ while ((self.getPeerState() == ServerState.LOOKING) &amp;&amp; (!stop))&#123; // 本服务器状态为LOOKING并且还未选出leader // 从recvqueue接收队列中取出投票 Notification n = recvqueue.poll(notTimeout, TimeUnit.MILLISECONDS); /* * Sends more notifications if haven't received enough. * Otherwise processes new notification. */ if(n == null)&#123; if(manager.haveDelivered())&#123; sendNotifications(); &#125; else &#123; manager.connectAll(); &#125; /* * Exponential backoff */ int tmpTimeOut = notTimeout*2; notTimeout = (tmpTimeOut &lt; maxNotificationInterval? tmpTimeOut : maxNotificationInterval); LOG.info("Notification time out: " + notTimeout); &#125; else if (validVoter(n.sid) &amp;&amp; validVoter(n.leader)) &#123; // 投票者集合中包含接收到消息中的服务器id /* * Only proceed if the vote comes from a replica in the current or next * voting view for a replica in the current or next voting view. */ switch (n.state) &#123; case LOOKING: // If notification &gt; current, replace and send messages out if (n.electionEpoch &gt; logicalclock.get()) &#123; // 其选举周期大于逻辑时钟 // 重新赋值逻辑时钟 logicalclock.set(n.electionEpoch); recvset.clear(); if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, getInitId(), getInitLastLoggedZxid(), getPeerEpoch())) &#123; // 选出较优的服务器 // 更新选票 updateProposal(n.leader, n.zxid, n.peerEpoch); &#125; else &#123; // 无法选出较优的服务器 updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch()); &#125; sendNotifications(); &#125; else if (n.electionEpoch &lt; logicalclock.get()) &#123; // 选举周期小于逻辑时钟，不做处理 if(LOG.isDebugEnabled())&#123; LOG.debug("Notification election epoch is smaller than logicalclock. n.electionEpoch = 0x" + Long.toHexString(n.electionEpoch) + ", logicalclock=0x" + Long.toHexString(logicalclock.get())); &#125; break; &#125; else if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) &#123; // 等于，并且能选出较优的服务器 // 更新选票 updateProposal(n.leader, n.zxid, n.peerEpoch); // 发送消息 sendNotifications(); &#125; if(LOG.isDebugEnabled())&#123; LOG.debug("Adding vote: from=" + n.sid + ", proposed leader=" + n.leader + ", proposed zxid=0x" + Long.toHexString(n.zxid) + ", proposed election epoch=0x" + Long.toHexString(n.electionEpoch)); &#125; // don't care about the version if it's in LOOKING state // recvset用于记录当前服务器在本轮次的Leader选举中收到的所有外部投票 recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch)); if (termPredicate(recvset, new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch))) &#123; // 若能选出leader // Verify if there is any change in the proposed leader while((n = recvqueue.poll(finalizeWait, TimeUnit.MILLISECONDS)) != null)&#123; if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch))&#123; recvqueue.put(n); break; &#125; &#125; /* * This predicate is true once we don't read any new * relevant message from the reception queue */ if (n == null) &#123; self.setPeerState((proposedLeader == self.getId()) ? ServerState.LEADING: learningState()); Vote endVote = new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch); leaveInstance(endVote); return endVote; &#125; &#125; break; case OBSERVING: LOG.debug("Notification from observer: " + n.sid); break; case FOLLOWING: case LEADING: /* * Consider all notifications from the same epoch * together. */ if(n.electionEpoch == logicalclock.get())&#123; // 与逻辑时钟相等 // 将该服务器和选票信息放入recvset中 recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch)); if(termPredicate(recvset, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state)) &amp;&amp; checkLeader(outofelection, n.leader, n.electionEpoch)) &#123; // 判断是否完成了leader选举 // 设置本服务器的状态 self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); // 创建投票信息 Vote endVote = new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch); leaveInstance(endVote); return endVote; &#125; &#125; /* * Before joining an established ensemble, verify that * a majority are following the same leader. */ outofelection.put(n.sid, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state)); if (termPredicate(outofelection, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state)) &amp;&amp; checkLeader(outofelection, n.leader, n.electionEpoch)) &#123; synchronized(this)&#123; logicalclock.set(n.electionEpoch); self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); &#125; Vote endVote = new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch); leaveInstance(endVote); return endVote; &#125; break; default: LOG.warn("Notification state unrecoginized: " + n.state + " (n.state), " + n.sid + " (n.sid)"); break; &#125; &#125; else &#123; if (!validVoter(n.leader)) &#123; LOG.warn("Ignoring notification for non-cluster member sid &#123;&#125; from sid &#123;&#125;", n.leader, n.sid); &#125; if (!validVoter(n.sid)) &#123; LOG.warn("Ignoring notification for sid &#123;&#125; from non-quorum member sid &#123;&#125;", n.leader, n.sid); &#125; &#125; &#125; return null; &#125; finally &#123; // 卸载注册的jmx ... &#125;&#125; 该函数用于开始新一轮的Leader选举，其首先会将逻辑时钟自增，然后更新本服务器的选票信息（初始化选票），之后将选票信息放入sendqueue等待发送给其他服务器，其流程如下: 12345678synchronized(this)&#123; logicalclock.incrementAndGet(); updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch());&#125;LOG.info("New election. My id = " + self.getId() + ", proposed zxid=0x" + Long.toHexString(proposedZxid));sendNotifications(); 之后每台服务器会不断地从recvqueue队列中获取外部选票。如果服务器发现无法获取到任何外部投票，就立即确认自己是否和集群中其他服务器保持着有效的连接，如果没有连接，则马上建立连接，如果已经建立了连接，则再次发送自己当前的内部投票，其流程如下: 12345678910111213141516171819202122232425// 从recvqueue接收队列中取出投票Notification n = recvqueue.poll(notTimeout, TimeUnit.MILLISECONDS);/* * Sends more notifications if haven't received enough. * Otherwise processes new notification. */if(n == null)&#123; // 无法获取选票 if(manager.haveDelivered())&#123; // manager已经发送了所有选票消息（表示有连接） // 向所有其他服务器发送消息 sendNotifications(); &#125; else &#123; // 还未发送所有消息（表示无连接） // 连接其他每个服务器 manager.connectAll(); &#125; /* * Exponential backoff */ int tmpTimeOut = notTimeout*2; notTimeout = (tmpTimeOut &lt; maxNotificationInterval? tmpTimeOut : maxNotificationInterval); LOG.info("Notification time out: " + notTimeout);&#125; 在发送完初始化选票之后，接着开始处理外部投票。在处理外部投票时，会根据选举轮次来进行不同的处理。 外部投票的选举轮次大于内部投票。若服务器自身的选举轮次落后于该外部投票对应服务器的选举轮次，那么就会立即更新自己的选举轮次(logicalclock)，并且清空所有已经收到的投票，然后使用初始化的投票来进行PK以确定是否变更内部投票。最终再将内部投票发送出去。 外部投票的选举轮次小于内部投票。若服务器接收的外选票的选举轮次落后于自身的选举轮次，那么Zookeeper就会直接忽略该外部投票，不做任何处理。 外部投票的选举轮次等于内部投票。此时可以开始进行选票PK，如果消息中的选票更优，则需要更新本服务器内部选票，再发送给其他服务器。 之后再对选票进行归档操作，无论是否变更了投票，都会将刚刚收到的那份外部投票放入选票集合recvset中进行归档，其中recvset用于记录当前服务器在本轮次的Leader选举中收到的所有外部投票，然后开始统计投票，统计投票是为了统计集群中是否已经有过半的服务器认可了当前的内部投票，如果确定已经有过半服务器认可了该投票，然后再进行最后一次确认，判断是否又有更优的选票产生，若无，则终止投票，然后最终的选票，其流程如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677// If notification &gt; current, replace and send messages outif (n.electionEpoch &gt; logicalclock.get()) &#123; // 其选举周期大于逻辑时钟 // 重新赋值逻辑时钟 logicalclock.set(n.electionEpoch); // 清空所有接收到的所有选票 recvset.clear(); if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, getInitId(), getInitLastLoggedZxid(), getPeerEpoch())) &#123; // 进行PK，选出较优的服务 // 更新选票 updateProposal(n.leader, n.zxid, n.peerEpoch); &#125; else &#123; // 无法选出较优的服务器 // 更新选票 updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch()); &#125; // 发送本服务器的内部选票消息 sendNotifications();&#125; else if (n.electionEpoch &lt; logicalclock.get()) &#123; // 选举周期小于逻辑时钟，不做处理，直接忽略 if(LOG.isDebugEnabled())&#123; LOG.debug("Notification election epoch is smaller than logicalclock. n.electionEpoch = 0x" + Long.toHexString(n.electionEpoch) + ", logicalclock=0x" + Long.toHexString(logicalclock.get())); &#125; break;&#125; else if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) &#123; // PK，选出较优的服务器 // 更新选票 updateProposal(n.leader, n.zxid, n.peerEpoch); // 发送消息 sendNotifications();&#125;if(LOG.isDebugEnabled())&#123; LOG.debug("Adding vote: from=" + n.sid + ", proposed leader=" + n.leader + ", proposed zxid=0x" + Long.toHexString(n.zxid) + ", proposed election epoch=0x" + Long.toHexString(n.electionEpoch));&#125;// don't care about the version if it's in LOOKING state// recvset用于记录当前服务器在本轮次的Leader选举中收到的所有外部投票recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch));if (termPredicate(recvset, new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch))) &#123; // 若能选出leader // Verify if there is any change in the proposed leader while((n = recvqueue.poll(finalizeWait, TimeUnit.MILLISECONDS)) != null)&#123; // 遍历已经接收的投票集合 if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch))&#123; // 选票有变更，比之前提议的Leader有更好的选票加入 // 将更优的选票放在recvset中 recvqueue.put(n); break; &#125; &#125; /* * This predicate is true once we don't read any new * relevant message from the reception queue */ if (n == null) &#123; // 表示之前提议的Leader已经是最优的 // 设置服务器状态 self.setPeerState((proposedLeader == self.getId()) ? ServerState.LEADING: learningState()); // 最终的选票 Vote endVote = new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch); // 清空recvqueue队列的选票 leaveInstance(endVote); // 返回选票 return endVote; &#125;&#125; 若选票中的服务器状态为FOLLOWING或者LEADING时，其大致步骤会判断选举周期是否等于逻辑时钟，归档选票，是否已经完成了Leader选举，设置服务器状态，修改逻辑时钟等于选举周期，返回最终选票，其流程如下: 123456789101112131415161718192021222324252627282930313233343536373839404142if(n.electionEpoch == logicalclock.get())&#123; // 与逻辑时钟相等 // 将该服务器和选票信息放入recvset中 recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch)); if(termPredicate(recvset, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state)) &amp;&amp; checkLeader(outofelection, n.leader, n.electionEpoch)) &#123; // 已经完成了leader选举 // 设置本服务器的状态 self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); // 最终的选票 Vote endVote = new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch); // 清空recvqueue队列的选票 leaveInstance(endVote); return endVote; &#125;&#125;/* * Before joining an established ensemble, verify that * a majority are following the same leader. */outofelection.put(n.sid, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state));if (termPredicate(outofelection, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state)) &amp;&amp; checkLeader(outofelection, n.leader, n.electionEpoch)) &#123; // 已经完成了leader选举 synchronized(this)&#123; // 设置逻辑时钟 logicalclock.set(n.electionEpoch); // 设置状态 self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); &#125; // 最终选票 Vote endVote = new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch); // 清空recvqueue队列的选票 leaveInstance(endVote); // 返回选票 return endVote;&#125; 参考资料 Zookeeper目录: https://www.cnblogs.com/leesf456/p/6239578.html Zookeeper源码分析目录: https://www.cnblogs.com/leesf456/p/6518040.html]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>Leader选举</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行万里路:葛剑雄旅行]]></title>
    <url>%2F2019%2F08%2F10%2Fgejianxiong-xingwanlilu%2F</url>
    <content type="text"><![CDATA[缘起之前在微博上了解过有一个很著名的文化学者: 葛剑雄，那时其title为复旦大学图书馆馆长。后来在杭州图书馆偶尔看到他的一本书叫《行万里路: 葛剑雄旅行》，带回来读了下，没想到他的授业恩师居然是赫赫有名的谭其骧教授(最早通过《中国国家地图集历史地图集》知道他，这部作品是迄今最权威的中国历史政区地图集，谭先生也是中国历史地理学科主要奠基人和开拓者)，所以拜读的兴趣就更浓厚。书中有不少精彩的文章，下面是一些书摘。 开放的城市离不开移民移民迁移关乎城市兴衰 都市离不开移民，无论是在其形成之初，还是在其发展过程中。一旦移民断绝，甚至出现人口大量外迁，富有该都市特色的文化亦随之而停滞，而衰落，以至最终消亡，古今都市概莫能外。 如西汉长安，是在秦朝咸阳城外的废墟上新建的，故址也只是一个乡，几乎已没有原住民。但在建成之日即迁入大批功臣、贵族、关东六国后裔及豪强，以后又通过迁入陵县(依托皇帝陵墓而设的县)的办法，在长安附近形成了一个城市群，总人口超过一百万。西汉后期，长安一带已是“名都对郭，邑居相承，英俊之域，绂冕所兴，冠盖如云”的繁华都市，形成“五方杂错，风俗不纯”的文化特色。但在公元初新莽政权覆灭引发的战乱中，长安人口损失惨重。东汉建都洛阳后，不仅吸引了大批移民，连长安和关中的精英也纷纷迁往。尽管关中父老仍在梦想有朝一日首都迁回长安，但实际上再未恢复昔日的繁盛。 在北魏孝文帝拓跋宏决定将首都从平城(今山西大同一带)迁往洛阳之时，东汉以来的洛阳早已不复存在。所以在迁都之初只能以金墉城为临时驻地，与西汉迁都长安时的形势相似。但孝文帝实行了最彻底的迁都和汉化政策，所以平城的人口特别是其中的上中层，几乎全部被迁往洛阳。由于此前北魏已在其辖境范围内进行过多次强制性的移民，平城集中了中国北方的上层人士，因而这次新的移民就意味着他们都集中到了洛阳。如在中原战乱中迁往河西走廊的移民后裔，和当地长期形成的文化精英，由西域(泛指今新疆、中亚、印度等地)内迁的僧人、商人、学者等，都先被迁至平城，又被迁于洛阳。此后，洛阳作为北方政权的首都成为中外移民的集中点，也成为多元文化的交汇点。《洛阳伽蓝记》描述了当时的繁盛：“自葱岭已西，至于大秦，百国千城，莫不欢附，商胡贩客，日奔塞下，所谓尽天地之区，已乐中国风土，因而宅者，不可胜数。是以附化之民，万有余家。门巷修整，阊阖填列，青槐荫陌，绿树垂庭，天下难得之货，咸悉在焉。”如此丰富的物质文明也滋养了极其多彩的精神文明，云冈石窟、大批寺庙和辉煌的佛教艺术应运而生。永安二年(529年)，南朝梁国的陈庆之在洛阳短期停留。尽管洛阳的极盛时期已经过去，还是让陈庆之瞠目结舌：“自晋、宋以来，号洛阳为荒土，此中(南方)谓长江以北，尽是夷狄。昨至洛阳，始知衣冠士族并在中原，礼仪富盛，人物殷阜，目所不识，口不能传。所谓‘帝京翼翼，四方之则’，始知登泰山卑培嵝，涉江海者小湘、沅。北人安可不重?” 近代上海是一个典型的移民城市。1843年上海开埠时，整个上海县只有50余万人口，英租界和法租界所在地是上海县城外的乡村，大部分还是农田和坟墓，人口稀少。但到1900年，上海的人口已经突破100万，到1949年更高达500多万。上海开埠时总共才有26位外国人，但以后迅速增加，经常保持着数万人的规模，1943年外国侨民高达15万，1949年上海解放时还有28000多人。上海从一个中等水平的江南县城一跃成为中国和亚洲最大、最发达的都市，移民无疑具有决定性的作用。但自1949年至20世纪70年代末，上海的高素质人口大量迁出，如迁往成为首都的北京，参军参干，支援外地建设，求学，随国民党迁往台湾，迁往港澳和国外，60年代后大规模的大小三线建设和知识青年上山下乡，迁出人口数以百万计。迁入的人口不但数量有限，而且以干部、退伍军人、体力劳动者为主，少数大专毕业生往往学非所用，作用无法充分发挥。再无新的外国侨民迁入，原有侨民大多迁出，未迁者也陆续消失，最后一位外国侨民至80年代初死亡。由移民带入的内资、外资全部断绝。上海都市文化的长期萧条正是这些因素的必然结果。而同期的香港却因大批高素质内地移民的迁入而获益，更因其特殊地位而成为东西方、海内外文化接触和交流的场所，在相当程度上已与上海主客易位。 地图上的中国和历史上的中国疆域：谭其骧编纂历史地图集的历程 但在重大原则问题上，谭先生对中国疆域的处理是经过深思熟虑，始终坚持的。长期以来，出于政治目的，史学界对今天中国境内的疆域一直强调“自古以来”，似乎中国从夏、商、周三代以来一直是这么大，似乎不找到一点“自古以来”的证据，一个地方归属于中国就失去了合法性。其中最敏感的地方就是台湾，由于谭先生坚持实事求是原则，以史料史实为根据，因此，“文革”期间这成为一条重要的“反革命”罪状，他受到了严厉的批判斗争。在修订过程中，他对台湾的处理方案多次被主管部门否决，《图集》的正式出版因此推迟了好几年，直到中央领导亲自过问并签阅批准，才涉险过关。但是，谭先生坚持认为： 台湾在明朝以前，既没有设过羁縻府州，也没有设过羁縻卫所，岛上的部落首领没有向大陆王朝进过贡，称过臣，中原王朝更没有在台湾岛上设官置守。过去我们历史学界也受了“左”的影响，把“台湾自古以来是中国的一部分”这句话曲解了。台湾自古以来是中国的一部分，这是一点没有错的，但是你不能把这句话解释为台湾自古以来是中原王朝的一部分，这是完全违反历史事实，明以前历代中原王朝都管不到台湾。有人要把台湾纳入中国从三国时算起，理由是三国时候孙权曾经派军队到过台湾，但历史事实是“军士万人征夷州（即台湾），军行经岁，士众疾疫死者十有八九”，只俘虏了几千人回来，“得不偿失”。我们根据这条史料，就说台湾从三国时候起就是大陆王朝的领土，不是笑话吗？派了一支军队去，俘虏了几千人回来，这块土地就是孙吴的了？这跟清代沙俄殖民者派一支探险队在黑龙江某个地方劫掠一番，就宣称这个地方已归沙俄所有，有什么区别？ 有人也感到这样实在说不过去，于是又提出了所谓台澎一体论，这也是绝对讲不通的。我们知道，南宋时澎湖在福建泉州同安县辖境之内，元朝在岛上设立了巡检司，这是大陆王朝在澎湖岛上设立政权之始，这是靠得住的。有些同志主张“台澎一体”论，说是既然在澎湖设立了巡检司，可见元朝已管到了台湾，这怎么说得通？在那么小的澎湖列岛上设了巡检司，就会管到那么大的台湾？宋元明清时，一个县可以设立几个巡检司，这等于现在的公安分局或者是派出所。设在澎湖岛上的巡检司，它就能管辖整个台湾了？有什么证据呢？相反，我们有好多证据证明是管不到的。（台湾）为什么自古以来是中国的？因为历史演变的结果，到了清朝台湾是清帝国疆域的一部分。所以台湾岛上的土著民族——高山族是我们中华民族的一个组成部分，是我们中国的一个少数民族。对台湾我们应该这样理解，在明朝以前，台湾岛是由我们中华民族的成员之一高山族居住着的，他们自己管理自己，中原王朝管不到。到了明朝后期。才有大陆上的汉人跑到台湾岛的西海岸建立了汉人的政权。……一直到1683年（康熙二十二年），清朝平定台湾，台湾才开始同大陆属于一个政权。 但这种机械的、教条的观念根深蒂固，无处不在，以至在解释任何一个地方“自古以来”就属于中国时，总是采取“实用”甚至“歪曲”的态度，只讲一部分被认为是有利的事实，却完全不提相反的事实，使绝大多数人误以为自古以来都是如此。 例如新疆，只说公元前60年汉宣帝设立西域都护府，却不提及王莽时已经撤销，东汉时“三通三绝”，以后多数年代名存实亡，或者仅是部分恢复；只说唐朝打败突厥，控制整个西域地区，却不提及安史之乱后唐朝再未重返西域；只说蒙古征服西辽，却不提及元朝从未完全统治西域地区。事实上，中原王朝对西域的统治直到乾隆二十四年（1759年）才重新实现。对于清朝来说，西域的确是新纳的疆域，因此，才有“新疆”的命名。谭先生还以云南为例，虽然汉、晋时代是由中原王朝统治，但是，在南朝后期就脱离了中原王朝。隋唐时期，云南是中原王朝的羁縻地区，不是直辖地区。8世纪中叶以后，南诏依附吐蕃反唐，根本就脱离了唐朝。南诏以后成为大理。总之，从6世纪脱离中原王朝，经过了差不多700年，到13世纪才由元朝征服大理，云南地区又被中原王朝统治。 不过，谭先生特别强调：“我们认为18世纪中叶以后，1840年以前的中国范围是我们几千年来历史发展所自然形成的中国，这就是我们历史上的中国。至于现在的中国疆域，已经不是历史上自然形成的那个范围了，而是这一百多年来资本主义列强、帝国主义侵略宰割了我们的部分领土的结果，所以不能代表我们历史上的中国的疆域了”，“为什么说清朝的版图是历史发展自然形成的呢？而不是说清帝国扩张侵略的结果？因为历史事实的确是这样。……清朝以前，我们中原地区跟各个边疆地区关系长期以来就很密切了，不但经济、文化方面很密切，并且在政治上曾经几度和中原地区在一个政权统治之下。” 虽然作为谭先生的学生与助手，深切理解他无法突破政治底线的苦衷。他的上述说法在理论上存在着局限性，在实际上也存在着无法调和的矛盾：一方面，从秦朝最多300多万平方公里的疆域发展到清朝极盛时期1300多万平方公里的疆域，并不能一概称之为“自然形成”。我们不能因为中国最终形成了一个疆域辽阔的国家，就将历史上那些侵略扩张行为视为促进王朝统一、社会进步的必要手段。秦始皇征服岭南，汉武帝用兵闽粤，唐朝灭高句丽、高昌、薛延陀，蒙古人建立元朝，清朝灭明、平定准噶尔，客观上促成国家统一，为中国疆域最终形成奠定了有利条件。但是，这种侵略扩张行为本身，未必就有“自然”的正义性可言；另一方面，在尚未形成现代国际法和国际关系、不同国家民族平等观念之前，世界上能够生存和发展下来的国家，特别是葡、西、荷、英、法、德、日、美、加、澳等近代大国强国，无一不是侵略扩张的产物，中国岂能例外？1840年以前，中国疆域之所以保持稳定，一个重要的有利因素是地理环境的封闭性，以致在工业化以前的外部世界尚缺乏这种突破地理障碍的能力。尽管如此，唐朝军队在中亚受挫于阿拉伯军队，伊斯兰教在突厥语游牧民族的武力支持下侵入新疆地区取代佛教，葡萄牙人长期占据澳门，葡萄牙人、西班牙人、荷兰人相继占据台湾和澎湖，沙俄哥萨克闯进黑龙江流域烧杀抢掠，这也不能不说是国家竞争“自然”的结果。在中俄雅克萨之战并导致《尼布楚条约》签订后，清朝应该了解俄国人的真实意图，而且有足够的时间和能力移民实边，却继续实施对东北的封禁，以致俄国人进入黑龙江以北和乌苏里江以东如入无人之境，至今一些俄国学家还声称俄国人是这片”新土地的开发者”，而非中国领土的掠夺者。但在清朝对东北开禁，鼓励移民，设置府州县，建东三省后，俄国与日本尽管仍然积心积虑要占据东北，却未能得逞。这岂不也是自然的结果吗？ 顺化散记]]></content>
      <categories>
        <category>历史&amp;地理</category>
        <category>历史地理</category>
      </categories>
      <tags>
        <tag>历史地理</tag>
        <tag>葛剑雄</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bash Shell常用快捷键]]></title>
    <url>%2F2019%2F08%2F06%2Fbash-shortcuts%2F</url>
    <content type="text"><![CDATA[移动光标 ctrl+b: 前移一个字符(backward) ctrl+f: 后移一个字符(forward) alt+b: 前移一个单词 alt+f: 后移一个单词 ctrl+a: 移到行首（a是首字母） ctrl+e: 移到行尾（end） ctrl+xx: 行首到当前光标替换 编辑命令 alt+.: 粘帖最后一次命令最后的参数（通常用于mkdir long-long-dir后, cd配合着alt+.） alt+d: 删除当前光标到临近右边单词开始(delete) ctrl+w: 删除当前光标到临近左边单词结束(word) ctrl+h: 删除光标前一个字符（相当于backspace） ctrl+d: 删除光标后一个字符（相当于delete） ctrl+u: 删除光标左边所有 ctrl+k: 删除光标右边所有 ctrl+l: 清屏 ctrl+shift+c: 复制（相当于鼠标左键拖拽） ctrl+shift+v: 粘贴（相当于鼠标中键） 其它 ctrl+n: 下一条命令 ctrl+p: 上一条命令 alt+n: 下一条命令（例如输入ls, 然后按’alt+n’, 就会找到历史记录下的ls命令） alt+p: 上一条命令（跟alt+n相似） shift+PageUp: 向上翻页 shift+PageDown: 向下翻页 ctrl+r: 进入历史查找命令记录， 输入关键字。 多次按返回下一个匹配项 zsh d: 列出以前的打开的命令 j: jump到以前某个目录，模糊匹配]]></content>
      <categories>
        <category>linux命令</category>
        <category>Bash Shell快捷键</category>
      </categories>
      <tags>
        <tag>Bash Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim命令]]></title>
    <url>%2F2019%2F08%2F05%2Flinux-vim%2F</url>
    <content type="text"><![CDATA[一般模式光标移动 h 或 向左箭头键(←) 光标向左移动一个字符 j 或 向下箭头键(↓) 光标向下移动一个字符 k 或 向上箭头键(↑) 光标向上移动一个字符 l 或 向右箭头键(→) 光标向右移动一个字符 20j 向下移动20行(以上四个命令可以配合数字使用)，在Vim中，很多命令都可以配合数字使用，比如删除10个字符10x w 前移一个单词，光标停在下一个单词开头 2w 重复执行w操作2次 W 移动下一个单词开头，但忽略一些标点 e 前移一个单词，光标停在下一个单词末尾 E 移动到下一个单词末尾，如果词尾有标点，则移动到标点 b 后移一个单词，光标停在上一个单词开头 B 移动到上一个单词开头，忽略一些标点 ( 前移1句 ) 后移1句 3) 光标移动到向下3句 { 前移1段 } 后移1段 f (find）命令也可以用于移动，fx将找到光标后第一个为x的字符，3fd将找到第三个为d的字符 F 同f，反向查找 [Ctrl] + [f] 屏幕『向下』移动一页，相当于 [Page Down]按键 (常用) [Ctrl] + [b] 屏幕『向上』移动一页，相当于 [Page Up] 按键 (常用) [Ctrl] + [d] 屏幕『向下』移动半页 [Ctrl] + [u] 屏幕『向上』移动半页 + 光标移动到非空格符的下一行 - 光标移动到非空格符的上一行 n&lt;space&gt; 那个 n 表示『数字』，例如 20 。按下数字后再按空格键，光标会向右移动这一行的 n 个字符。例如 20&lt;space&gt; 则光标会向后面移动 20 个字符距离。 0 或功能键[Home] 这是数字『 0 』：移动到这一行的最前面字符处 (常用) $ 或功能键[End] 移动到这一行的最后面字符处(常用) H 光标移动到这个屏幕的最上方那一行的第一个字符 M 光标移动到这个屏幕的中央那一行的第一个字符 L 光标移动到这个屏幕的最下方那一行的第一个字符 G 移动到这个档案的最后一行(常用) nG n 为数字。移动到这个档案的第 n 行。例如 20G 则会移动到这个档案的第 20 行(可配合 :set nu) gg 移动到这个档案的第一行，相当于 1G 啊！ (常用) n&lt;Enter&gt; n 为数字。光标向下移动 n 行(常用) 复制粘贴 x, X 在一行字当中，x 为向后删除一个字符 (相当于 [del] 按键)， X 为向前删除一个字符(相当于 [backspace] 亦即是退格键) (常用) nx n 为数字，连续向后删除 n 个字符。举例来说，我要连续删除 10 个字符， 『10x』。 dd 删除游标所在的那一整行(常用) ndd n 为数字。删除光标所在的向下 n 行，例如 20dd 则是删除 20 行 (常用) d1G 删除光标所在到第一行的所有数据 dG 删除光标所在到最后一行的所有数据 d$ 删除游标所在处，到该行的最后一个字符 d0 那个是数字的 0 ，删除游标所在处，到该行的最前面一个字符 yy 复制游标所在的那一行(常用) nyy n 为数字。复制光标所在的向下 n 行，例如 20yy 则是复制 20 行(常用) y1G 复制游标所在行到第一行的所有数据 yG 复制游标所在行到最后一行的所有数据 y0 复制光标所在的那个字符到该行行首的所有数据 y$ 复制光标所在的那个字符到该行行尾的所有数据 p, P p 为将已复制的数据在光标下一行贴上，P 则为贴在游标上一行！ 举例来说，我目前光标在第 20 行，且已经复制了 10 行数据。则按下 p 后， 那 10 行数据会贴在原本的 20 行之后，亦即由 21 行开始贴。但如果是按下 P 呢？ 那么原本的第 20 行会被推到变成 30 行。 (常用) J 将光标所在行与下一行的数据结合成同一行 c 重复删除多个数据，例如向下删除 10 行，[ 10cj ] u 复原前一个动作。(常用) [Ctrl]+r 重做上一个动作。(常用) 这个 u 与 [Ctrl]+r 是很常用的指令！一个是复原，另一个则是重做一次～ . 不要怀疑！这就是小数点！意思是重复前一个动作的意思。 如果你想要重复删除、重复贴上等等动作，按下小数点『.』就好了！ (常用) 搜索替换 /word 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用) ?word 向光标之上寻找一个字符串名称为 word 的字符串。 n 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！ N 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。 使用 /word 配合 n 及 N 是非常有帮助的！可以让你重复的找到一些你搜寻的关键词！ :n1,n2s/word1/word2/g n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则： 『:100,200s/vbird/VBIRD/g』。(常用) :1,$s/word1/word2/g 或 :%s/word1/word2/g 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用) :1,$s/word1/word2/gc 或 :%s/word1/word2/gc 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用) 一般模式切换到编辑模式 进入输入或取代的编辑模式 i, I 进入输入模式(Insert mode)： i 为『从目前光标所在处输入』， I 为『在目前所在行的第一个非空格符处开始输入』。 (常用) a, A 进入输入模式(Insert mode)： a 为『从目前光标所在的下一个字符处开始输入』， A 为『从光标所在行的最后一个字符处开始输入』。(常用) o, O 进入输入模式(Insert mode)： 这是英文字母 o 的大小写。o 为『在目前光标所在的下一行处输入新的一行』； O 为在目前光标所在处的上一行输入新的一行！(常用) r, R 进入取代模式(Replace mode)： r 只会取代光标所在的那一个字符一次；R会一直取代光标所在的文字，直到按下 ESC 为止；(常用) 上面这些按键中，在 vi 画面的左下角处会出现『–INSERT–』或『–REPLACE–』的字样。 由名称就知道该动作了吧！！特别注意的是，我们上面也提过了，你想要在档案里面输入字符时， 一定要在左下角处看到 INSERT 或 REPLACE 才能输入喔！ [Esc] 退出编辑模式，回到一般模式中(常用) 一般模式切换到指令行模式 指令行的储存、离开等指令 :w 将编辑的数据写入硬盘档案中(常用) :w! 若文件属性为『只读』时，强制写入该档案。不过，到底能不能写入， 还是跟你对该档案的档案权限有关啊！ :q 离开 vi (常用) :q! 若曾修改过档案，又不想储存，使用 ! 为强制离开不储存档案。 注意一下啊，那个惊叹号 (!) 在 vi 当中，常常具有『强制』的意思～ :wq 储存后离开，若为 :wq! 则为强制储存后离开 (常用) ZZ 这是大写的 Z 喔！若档案没有更动，则不储存离开，若档案已经被更动过，则储存后离开！ :w [filename] 将编辑的数据储存成另一个档案（类似另存新档） :r [filename] 在编辑的数据中，读入另一个档案的数据。亦即将 『filename』 这个档案内容加到游标所在行后面 :n1,n2 w [filename] 将 n1 到 n2 的内容储存成 filename 这个档案。 :! command 暂时离开 vi 到指令行模式下执行 command 的显示结果！例如 『:! ls /home』即可在 vi 当中察看 /home 底下以 ls 输出的档案信息！ vim 环境的变更 :set nu 显示行号，设定之后，会在每一行的前缀显示该行的行号 :set nonu 与 set nu 相反，为取消行号！ 区域选择 &lt;action&gt;a&lt;object&gt; 或 &lt;action&gt;i&lt;object&gt;在visual 模式下，这些命令很强大，其命令格式为 1&lt;action&gt;a&lt;object&gt;` 和 `&lt;action&gt;i&lt;object&gt; action可以是任何的命令，如 d (删除), y (拷贝), v (可以视模式选择)。 object 可能是： w 一个单词， W 一个以空格为分隔的单词， s 一个句字， p 一个段落。也可以是一个特别的字符：&quot;、 &#39;、 )、 }、 ]。 假设你有一个字符串 (map (+) (&quot;foo&quot;)).而光标键在第一个 o的位置。 vi&quot; → 会选择 foo. va&quot; → 会选择 &quot;foo&quot;. vi) → 会选择 &quot;foo&quot;. va) → 会选择(&quot;foo&quot;). v2i) → 会选择 map (+) (&quot;foo&quot;) v2a) → 会选择 (map (+) (&quot;foo&quot;)) 块操作: &lt;C-v&gt;块操作，典型的操作： 0 &lt;C-v&gt; &lt;C-d&gt; I-- [ESC] ^ → 到行头 &lt;C-v&gt; → 开始块操作 &lt;C-d&gt; → 向下移动 (你也可以使用hjkl来移动光标，或是使用%，或是别的) I-- [ESC] → I是插入，插入“--”，按ESC键来为每一行生效。 自动提示： &lt;C-n&gt; 和 &lt;C-p&gt;在 Insert 模式下，你可以输入一个词的开头，然后按 &lt;C-p&gt;或是&lt;C-n&gt;，自动补齐功能就出现了…… 参考资料 简明 VIM 练级攻略: https://coolshell.cn/articles/5426.html Vim使用笔记: https://www.cnblogs.com/jiqingwu/archive/2012/06/14/vim_notes.html Vim命令合集: https://www.jianshu.com/p/117253829581 ###]]></content>
      <categories>
        <category>linux命令</category>
        <category>vim命令</category>
      </categories>
      <tags>
        <tag>vim命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grep命令]]></title>
    <url>%2F2019%2F08%2F05%2Fgrep-command%2F</url>
    <content type="text"><![CDATA[基本用法grep命令123456-c 统计包含匹配字符串的行数。-i 忽略字符大小写的差别。-v 反转查找。-o 只输出文件中匹配到的部分。-E 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。-r 递归查找。 GNU Grep用法 示例1grep "test[53]" jfedu.txt 以字符test开头，接5或者3的行]]></content>
      <categories>
        <category>linux命令</category>
        <category>grep命令</category>
      </categories>
      <tags>
        <tag>grep命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux基本命令]]></title>
    <url>%2F2019%2F08%2F05%2Flinux-command%2F</url>
    <content type="text"><![CDATA[sort: 排序sort 命令的用法很简单。最基本的用法有两种： 12cat data.txt | sortsort data.txt 选项： 12-r 倒序-f 忽略字母大小写 -k 指定用于排序的栏目范围，-t 指定栏目的分隔符。 以下实例，用:分割行，排序值从第3栏开始： 1234567$ sort -k 3 -t : sort.txtaaa:30:1.6bbb:10:2.5ccc:50:3.3ddd:20:4.2eee:60:5.1eee:40:5.4 uniq: 重复uniq 的作用可以描述为：针对相邻的重复行，进行相应的动作。 这句话中，有两个地方需要注意。首先，针对的是相邻的重复行。因此，uniq 对于不相邻的重复行是不起作用的。其次，进行相应的动作。这意味着，uniq 可以做的事情很多，不止一样。 不带任何参数的时候 uniq 的动作是：对相邻的重复行进行去除。例如： 1cat &lt;filename&gt; | sort | uniq 我们已经见过了 sort 的作用，那么上面命令的作用就很显然了：将 &lt;filename&gt; 按照 ASCII 升序排序；然后去除重复出现的行；最后将这个没有重复行的内容输出到标准输出。 给 uniq 加上参数，就能解锁更多姿势。 12345cat &lt;filename&gt; | sort | uniq -d # 只显示重复的行，每行只显示一次cat &lt;filename&gt; | sort | uniq -D # 只显示重复的行cat &lt;filename&gt; | sort | uniq -i # 忽略大小写cat &lt;filename&gt; | sort | uniq -u # 只显示只出现一次的行cat &lt;filename&gt; | sort | uniq -c # 统计每行重复的次数 xargs: 将标准输入转为命令行参数Unix有些命令可以接受”标准输入”（stdin）作为参数。 12&gt; $ cat /etc/passwd | grep root&gt; 上面的代码使用了管道命令（|）。管道命令的作用，是将左侧命令（cat /etc/passwd）的标准输出转换为标准输入，提供给右侧命令（grep root）作为参数。 但是，大多数命令都不接受标准输入作为参数，只能直接在命令行输入参数，这导致无法用管道命令传递参数。举例来说，echo命令就不接受管道传参。 12&gt; $ echo "hello world" | echo&gt; 上面的代码不会有输出。因为管道右侧的echo不接受管道传来的标准输入作为参数。 xargs命令的作用，是将标准输入转为命令行参数。 123&gt; $ echo "hello world" | xargs echo&gt; hello world&gt; 上面的代码将管道左侧的标准输入，转为命令行参数hello world，传给第二个echo命令。 示例 查找所有后辍为txt的文件列表，然后进行grep: 1$ find . -name "*.txt" | xargs grep "abc" 由于xargs默认将空格作为分隔符，所以不太适合处理文件名，因为文件名可能包含空格。 find命令有一个特别的参数-print0，指定输出的文件列表以null分隔。然后，xargs命令的-0参数表示用null当作分隔符。 1find . -type f -name "*.log" -print0 | xargs -0 rm -f 统计一个源代码目录中所有 php 文件的行数： 1find . -type f -name "*.php" -print0 | xargs -0 wc -l 查找所有的 jpg 文件，并且压缩它们： 1find . -type f -name "*.jpg" -print | xargs tar -czvf images.tar.gz]]></content>
      <categories>
        <category>linux命令</category>
        <category>linux基本命令</category>
      </categories>
      <tags>
        <tag>linux基本命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk用法]]></title>
    <url>%2F2019%2F08%2F05%2Fawk-syntax%2F</url>
    <content type="text"><![CDATA[基本用法1awk [options] 'pattern &#123;action&#125;' file... awk的工作过程是这样的：按行读取输入(标准输入或文件)，对于符合模式pattern的行，执行action。当pattern省略时表示匹配任何字符串；当action省略时表示执行&#39;{print}&#39;；它们不可以同时省略。 模式可以是以下任意一个： /正则表达式/：使用通配符的扩展集。 关系表达式：使用运算符进行操作，可以是字符串或数字的比较测试。 模式匹配表达式：用运算符~(匹配) 和 ~!(不匹配)。 BEGIN语句块、pattern语句块、END语句块 每一行输入，对awk来说都是一条记录(record)，awk使用$0来引用当前记录： 12[root@centos7 ~]# head -1 /etc/passwd | awk '&#123;print $0&#125;'root:x:0:0:root:/root:/bin/bash 例子中将命令head -1 /etc/passwd作为awk的输入，awk省略了pattern，action为print $0，意为打印当前记录。对于每条记录，awk使用分隔符将其分割成列，第一列用$1表示，第二列用$2表示…最后一列用$NF表示 选项-F表示指定分隔符如输出文件/etc/passwd第一行第一列(用户名)和最后一列(登录shell)： 12[root@centos7 ~]# head -1 /etc/passwd | awk -F: '&#123;print $1,$NF&#125;'root /bin/bash 当没有指定分隔符时，使用一到多个blank(空白字符，由空格键或TAB键产生)作为分隔符。输出的分隔符默认为空格。如输出命令ls -l *的结果中，文件大小和文件名： 1234567[root@centos7 temp]# ls -l * | awk '&#123;print $5,$NF&#125;'13 b.txt58 c.txt12 d.txt0 e.txt0 f.txt24 test.sh 还可以对任意列进行过滤： 12[root@centos7 temp]# ls -l *|awk '$5&gt;20 &amp;&amp; $NF ~ /txt$/'-rw-r--r-- 1 nobody nobody 58 11月 16 16:34 c.txt 其中$5&gt;20表示第五列的值大于20；&amp;&amp;表示逻辑与；$NF ~ /txt$/中，~表示匹配，符号//内部是正则表达式。这里省略了action，整条awk语句表示打印文件大小大于20字节并且文件名以txt结尾的行。 awk用NR表示行号 123[root@centos7 temp]# awk '/^root/ || NR==2' /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologin 例子中||表示逻辑或，语句表示：输出文件/etc/passwd中以root开头的行或者第二行。 在一些情况下，使用awk过滤甚至比使用grep更灵活如获得ifconfig的输出中网卡名及其对应的mtu值 12345[root@idc-v-71253 ~]# ifconfig|awk '/^\S/&#123;print $1"\t"$NF&#125;'ens32: 1500ens33: 1500lo: 65536#这里的正则表示不以空白字符开头的行，输出内容中使用\t进行了格式化。 以上所说的NR、NF等都是awk的内建变量，下面列出部分常用内置变量 12345678910$0 当前记录（这个变量中存放着整个行的内容）$1~$n 当前记录的第n个字段，字段间由FS分隔FS 输入字段分隔符 默认是空格或TabNF 当前记录中的字段个数，就是有多少列NR 行号，从1开始，如果有多个文件话，这个值也不断累加。FNR 输入文件行号RS 输入的记录分隔符， 默认为换行符OFS 输出字段分隔符， 默认也是空格ORS 输出的记录分隔符，默认为换行符FILENAME 当前输入文件的名字 awk中还可以使用自定义变量，如将网卡名赋值给变量a，然后输出网卡名及其对应的RX bytes的值(注意不同模式匹配及其action的写法)： 1234[root@idc-v-71253 ~]# ifconfig|awk '/^\S/&#123;a=$1&#125;/RX p/&#123;print a,$5&#125;'ens32: 999477100ens33: 1663197120lo: 0 awk中有两个特殊的pattern：BEGIN和END；它们不会对输入文本进行匹配，BEGIN对应的action部分组合成一个代码块，在任何输入开始之前执行；END对应的action部分组合成一个代码块，在所有输入处理完成之后执行。 12345678#注意类似于C语言的赋值及print函数用法[root@centos7 temp]# ls -l *|awk 'BEGIN&#123;print "size name\n---------"&#125;$5&gt;20&#123;x+=$5;print $5,$NF&#125;END&#123;print "---------\ntotal",x&#125;'size name---------58 c.txt24 test.sh---------total 82 再来看看统计每个用户的进程的占了多少内存（注：sum的RSS那一列） 123456$ ps aux | awk &apos;NR!=1&#123;a[$1]+=$6;&#125; END &#123; for(i in a) print i &quot;, &quot; a[i]&quot;KB&quot;;&#125;&apos;dbus, 540KBmysql, 99928KBwww, 3264924KBroot, 63644KBhchen, 6020KB awk还支持数组，数组的索引都被视为字符串(即关联数组)，可以使用for循环遍历数组元素如输出文件/etc/passwd中各种登录shell及其总数量 1234567#注意数组赋值及for循环遍历数组的写法[root@centos7 temp]# awk -F ':' '&#123;a[$NF]++&#125;END&#123;for(i in a) print i,a[i]&#125;' /etc/passwd/bin/sync 1/bin/bash 2/sbin/nologin 19/sbin/halt 1/sbin/shutdown 1 当然也有if分支语句 123#注意大括号是如何界定action块的[root@centos7 temp]# netstat -antp|awk '&#123;if($6=="LISTEN")&#123;x++&#125;else&#123;y++&#125;&#125;END&#123;print x,y&#125;'6 3 pattern之间可以用逗号分隔，表示从匹配第一个模式开始直到匹配第二个模式 12345[root@centos7 ~]# awk '/^root/,/^adm/' /etc/passwd root:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologin 还支持三目操作符pattern1 ? pattern2 : pattern3，表示判断pattern1是否匹配，true则匹配pattern2，false则匹配pattern3，pattern也可以是类似C语言的表达式。如判断文件/etc/passwd中UID大于500的登录shell是否为/bin/bash，是则输出整行，否则输出UID为0的行： 12345#注意为避免混淆对目录分隔符进行了转义[root@centos7 ~]# awk -F: '$3&gt;500?/\/bin\/bash$/:$3==0 &#123;print $0&#125;' /etc/passwd root:x:0:0:root:/root:/bin/bashlearner:x:1000:1000::/home/learner:/bin/bash#三目运算符也可以嵌套，例子略 选项-f file表示从file中读取awk指令 1234567891011121314#打印斐波那契数列前十项[root@centos7 temp]# cat test.awk BEGIN&#123; $1=1 $2=1 OFS="," for(i=3;i&lt;=10;i++) &#123; $i=$(i-2)+$(i-1) &#125; print&#125;[root@centos7 temp]# awk -f test.awk 1,1,2,3,5,8,13,21,34,55 选项-F指定列分隔符 1234#多个字符作为分隔符时[root@centos7 temp]# echo 1.2,3:4 5|awk -F '[., :]' '&#123;print $2,$NF&#125;'2 5#这里-F后单引号中的内容也是正则表达式 选项-v var=val设定变量 1234567#这里printf函数用法类似C语言同名函数[root@centos7 ~]# awk -v n=5 'BEGIN&#123;for(i=0;i&lt;n;i++) printf "%02d\n",i&#125;' 0001020304 print等函数还支持使用重定向符&gt;和&gt;&gt;将输出保存至文件 1234567#如按第一列(IP)分类拆分文件access.log，并保存至ip.txt文件中[root@centos7 temp]# awk '&#123;print &gt; $1".txt"&#125;' access.log [root@centos7 temp]# ls -l 172.20.71.*-rw-r--r-- 1 root root 5297 11月 22 21:33 172.20.71.38.txt-rw-r--r-- 1 root root 1236 11月 22 21:33 172.20.71.39.txt-rw-r--r-- 1 root root 4533 11月 22 21:33 172.20.71.84.txt-rw-r--r-- 1 root root 2328 11月 22 21:33 172.20.71.85.txt 内建函数length()获得字符串长度 12[root@centos7 temp]# awk -F: '&#123;if(length($1)&gt;=16)print&#125;' /etc/passwd systemd-bus-proxy:x:999:997:systemd Bus Proxy:/:/sbin/nologin split()将字符串按分隔符分隔，并保存至数组 12345678[root@centos7 temp]# head -1 /etc/passwd|awk '&#123;split($0,arr,/:/);for(i=1;i&lt;=length(arr);i++) print arr[i]&#125;'rootx00root/root/bin/bash getline从输入(可以是管道、另一个文件或当前文件的下一行)中获得记录，赋值给变量或重置某些环境变量 1234567891011121314151617181920212223#从shell命令date中通过管道获得当前的小时数[root@centos7 temp]# awk 'BEGIN&#123;"date"|getline;split($5,arr,/:/);print arr[1]&#125;'09#从文件中获取，此时会覆盖当前的$0。(注意逐行处理b.txt的同时也在逐行从c.txt中获得记录并覆盖$0，当getline先遇到eof时&lt;即c.txt文件行数较少&gt;将输出空行)[root@centos7 temp]# awk '&#123;getline &lt;"c.txt";print $4&#125;' b.txt "https://segmentfault.com/blog/learnning"[root@centos7 temp]# #赋值给变量[root@centos7 temp]# awk '&#123;getline blog &lt;"c.txt";print $0"\n"blog&#125;' b.txt aasdasdadsadBLOG ADDRESS IS "https://segmentfault.com/blog/learnning"[root@centos7 temp]# #读取下一行(也会覆盖当前$0)[root@centos7 temp]# cat fileanny100bob150cindy120[root@centos7 temp]# awk '&#123;getline;total+=$0&#125;END&#123;print total&#125;' file370#此时表示只对偶数行进行处理 next作用和getline类似，也是读取下一行并覆盖$0，区别是next执行后，其后的命令不再执行，而是读取下一行从头再执行。 123456789101112131415161718192021#跳过以a-s开头的行，统计行数，打印最终结果[root@centos7 temp]# awk '/^[a-s]/&#123;next&#125;&#123;count++&#125;END&#123;print count&#125;' /etc/passwd2[root@centos7 temp]# #又如合并相同列的两个文件[root@centos7 temp]# cat f.txt 学号 分值00001 8000002 7500003 90[root@centos7 temp]# cat e.txt 姓名 学号张三 00001李四 00002王五 00003[root@centos7 temp]# awk 'NR==FNR&#123;a[$1]=$2;next&#125;&#123;print $0,a[$2]&#125;' f.txt e.txt 姓名 学号 分值张三 00001 80李四 00002 75王五 00003 90#这里当读第一个文件时NR==FNR成立，执行a[$1]=$2，然后next忽略后面的。读取第二个文件时，NR==FNR不成立，执行后面的打印命令 sub(regex,substr,string)替换字符串string(省略时为$0)中首个出现匹配正则regex的子串substr 12[root@centos7 temp]# echo 178278 world|awk 'sub(/[0-9]+/,"hello")'hello world gsub(regex,substr,string)与sub()类似，但不止替换第一个，而是全局替换 123456[root@centos7 temp]# head -n5 /etc/passwd|awk '&#123;gsub(/[0-9]+/,"----");print $0&#125;' root:x:----:----:root:/root:/bin/bashbin:x:----:----:bin:/bin:/sbin/nologindaemon:x:----:----:daemon:/sbin:/sbin/nologinadm:x:----:----:adm:/var/adm:/sbin/nologinlp:x:----:----:lp:/var/spool/lpd:/sbin/nologin substr(str,n,m)切割字符串str，从第n个字符开始，切割m个。如果m省略，则到结尾 12[root@centos7 temp]# echo "hello,世界！"|awk '&#123;print substr($0,8,1)&#125;'界 tolower(str)和toupper(str)表示大小写转换 12[root@centos7 temp]# echo "hello,世界！"|awk '&#123;A=toupper($0);print A&#125;'HELLO,世界！ system(cmd)执行shell命令cmd，返回执行结果，执行成功为0，失败为非0 123#此处if语句判断和C语言一致，0为false，非0为true[root@centos7 temp]# awk 'BEGIN&#123;if(!system("date&gt;/dev/null"))print "success"&#125;'success match(str,regex)返回字符串str中匹配正则regex的位置 12[root@centos7 temp]# awk 'BEGIN&#123;A=match("abc.f.11.12.1.98",/[0-9]&#123;1,3&#125;\./);print A&#125;'7 示例 条件统计 1awk '$2&gt;=6 &amp;&amp; $2&lt;20 &#123; tot++ &#125; END &#123; print +tot&#125;' 花式用法 12345#按连接数查看客户端IPnetstat -ntu | awk '&#123;print $5&#125;' | cut -d: -f1 | sort | uniq -c | sort -nr #打印99乘法表seq 9 | sed 'H;g' | awk -v RS='' '&#123;for(i=1;i&lt;=NF;i++)printf("%dx%d=%d%s", i, NR, i*NR, i==NR?"\n":"\t")&#125;']]></content>
      <categories>
        <category>linux命令</category>
        <category>awk命令</category>
      </categories>
      <tags>
        <tag>awk命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2019%2F08%2F05%2Fregexp-syntax%2F</url>
    <content type="text"><![CDATA[元字符 字符 描述 \ 将下一个字符标记为一个特殊字符、或一个原义字符、或一个 向后引用、或一个八进制转义符。例如，’n’ 匹配字符 “n”。’\n’ 匹配一个换行符。序列 ‘\‘ 匹配 “&quot; 而 “(“ 则匹配 “(“。 ^ 匹配输入字符串的开始位置。如果设置了 RegExp 对象的 Multiline 属性，^ 也匹配 ‘\n’ 或 ‘\r’ 之后的位置。 $ 匹配输入字符串的结束位置。如果设置了RegExp 对象的 Multiline 属性，$ 也匹配 ‘\n’ 或 ‘\r’ 之前的位置。 * 匹配前面的子表达式零次或多次。例如，zo* 能匹配 “z” 以及 “zoo”。* 等价于{0,}。 + 匹配前面的子表达式一次或多次。例如，’zo+’ 能匹配 “zo” 以及 “zoo”，但不能匹配 “z”。+ 等价于 {1,}。 ? 匹配前面的子表达式零次或一次。例如，”do(es)?” 可以匹配 “do” 或 “does” 。? 等价于 {0,1}。 {n} n 是一个非负整数。匹配确定的 n 次。例如，’o{2}’ 不能匹配 “Bob” 中的 ‘o’，但是能匹配 “food” 中的两个 o。 {n,} n 是一个非负整数。至少匹配n 次。例如，’o{2,}’ 不能匹配 “Bob” 中的 ‘o’，但能匹配 “foooood” 中的所有 o。’o{1,}’ 等价于 ‘o+’。’o{0,}’ 则等价于 ‘o*’。 {n,m} m 和 n 均为非负整数，其中n &lt;= m。最少匹配 n 次且最多匹配 m 次。例如，”o{1,3}” 将匹配 “fooooood” 中的前三个 o。’o{0,1}’ 等价于 ‘o?’。请注意在逗号和两个数之间不能有空格。 ? 当该字符紧跟在任何一个其他限制符 (*, +, ?, {n}, {n,}, {n,m}) 后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串 “oooo”，’o+?’ 将匹配单个 “o”，而 ‘o+’ 将匹配所有 ‘o’。 . 匹配除换行符（\n、\r）之外的任何单个字符。要匹配包括 ‘\n’ 在内的任何字符，请使用像”(.|\n)“的模式。 (pattern) 匹配 pattern 并获取这一匹配。所获取的匹配可以从产生的 Matches 集合得到。要匹配圆括号字符，请使用 ‘(‘ 或 ‘)‘。 (?:pattern) 匹配 pattern 但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用 “或” 字符 (|) 来组合一个模式的各个部分是很有用。例如， ‘industr(?:y|ies) 就是一个比 ‘industry|industries’ 更简略的表达式。 (?=pattern) 正向肯定预查（look ahead positive assert），在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，”Windows(?=95|98|NT|2000)”能匹配”Windows2000”中的”Windows”，但不能匹配”Windows3.1”中的”Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。 (?!pattern) 正向否定预查(negative assert)，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如”Windows(?!95|98|NT|2000)”能匹配”Windows3.1”中的”Windows”，但不能匹配”Windows2000”中的”Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。 (?&lt;=pattern) 反向(look behind)肯定预查，与正向肯定预查类似，只是方向相反。例如，”`(?&lt;=95 (?&lt;!pattern) 反向否定预查，与正向否定预查类似，只是方向相反。例如”`(?&lt;!95 x|y 匹配 x 或 y。例如，’z|food’ 能匹配 “z” 或 “food”。’(z|f)ood’ 则匹配 “zood” 或 “food”。 [xyz] 字符集合。匹配所包含的任意一个字符。例如， ‘[abc]’ 可以匹配 “plain” 中的 ‘a’。 [^xyz] 负值字符集合。匹配未包含的任意字符。例如， ‘[^abc]’ 可以匹配 “plain” 中的’p’、’l’、’i’、’n’。 [a-z] 字符范围。匹配指定范围内的任意字符。例如，’[a-z]’ 可以匹配 ‘a’ 到 ‘z’ 范围内的任意小写字母字符。 [^a-z] 负值字符范围。匹配任何不在指定范围内的任意字符。例如，’[^a-z]’ 可以匹配任何不在 ‘a’ 到 ‘z’ 范围内的任意字符。 \b 匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’。 \B 匹配非单词边界。’er\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’。 \cx 匹配由 x 指明的控制字符。例如， \cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 ‘c’ 字符。 \d 匹配一个数字字符。等价于 [0-9]。 \D 匹配一个非数字字符。等价于 [^0-9]。 \f 匹配一个换页符。等价于 \x0c 和 \cL。 \n 匹配一个换行符。等价于 \x0a 和 \cJ。 \r 匹配一个回车符。等价于 \x0d 和 \cM。 \s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。 \S 匹配任何非空白字符。等价于 [^ \f\n\r\t\v]。 \t 匹配一个制表符。等价于 \x09 和 \cI。 \v 匹配一个垂直制表符。等价于 \x0b 和 \cK。 \w 匹配字母、数字、下划线。等价于’[A-Za-z0-9_]’。 \W 匹配非字母、数字、下划线。等价于 ‘[^A-Za-z0-9_]’。 \xn 匹配 n，其中 n 为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，’\x41’ 匹配 “A”。’\x041’ 则等价于 ‘\x04’ &amp; “1”。正则表达式中可以使用 ASCII 编码。 \num 匹配 num，其中 num 是一个正整数。对所获取的匹配的引用。例如，’(.)\1’ 匹配两个连续的相同字符。 \n 标识一个八进制转义值或一个向后引用。如果 \n 之前至少 n 个获取的子表达式，则 n 为向后引用。否则，如果 n 为八进制数字 (0-7)，则 n 为一个八进制转义值。 \nm 标识一个八进制转义值或一个向后引用。如果 \nm 之前至少有 nm 个获得子表达式，则 nm 为向后引用。如果 \nm 之前至少有 n 个获取，则 n 为一个后跟文字 m 的向后引用。如果前面的条件都不满足，若 n 和 m 均为八进制数字 (0-7)，则 \nm 将匹配八进制转义值 nm。 \nml 如果 n 为八进制数字 (0-3)，且 m 和 l 均为八进制数字 (0-7)，则匹配八进制转义值 nml。 \un 匹配 n，其中 n 是一个用四个十六进制数字表示的 Unicode 字符。例如， \u00A9 匹配版权符号 (?)。 常用正则表达式收集 最全的常用正则表达式大全——包括校验数字、字符、一些特殊的需求等 常用正则表达式 - 收集一些在平时项目开发中经常用到的正则表达式]]></content>
      <categories>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paxos精髓及示例]]></title>
    <url>%2F2019%2F07%2F31%2Fpaxos-essence-and-demo%2F</url>
    <content type="text"><![CDATA[Paxos精髓paxos协议用来解决的问题可以用一句话来简化: proposer将发起提案（value）给所有accpetor，超过半数accpetor获得批准后，proposer将提案写入accpetor内，最终所有accpetor获得一致性的确定性取值，且后续不允许再修改。 协议分为两大阶段，每个阶段又分为a/b两小步骤： 准备阶段（占坑阶段）Phase 1a: PrepareProposer选择一个提议编号n，向所有的Acceptor广播Prepare（n）请求。 Phase 1b: PromiseAcceptor接收到Prepare（n）请求，若提议编号n比之前接收的Prepare请求都要大，则承诺将不会接收提议编号比n小的提议，并且带上之前Accept的提议中编号小于n的最大的提议，否则不予理会。 接受阶段（提交阶段）Phase 2a: Accept​ 1. 如果未超过半数accpetor响应，直接转为提议失败； ​ 2. 如果超过多数Acceptor的承诺，又分为不同情况： ​ 2.1 如果所有Acceptor都未接收过值（都为null），那么向所有的Acceptor发起自己的值和提议编号n，记住，一定是所有Acceptor都没接受过值； ​ 2.2 如果有部分Acceptor接收过值，那么从所有接受过的值中选择对应的提议编号最大的作为提议的值，提议编号仍然为n。但此时Proposer就不能提议自己的值，只能信任Acceptor通过的值，维护一但获得确定性取值就不能更改原则； Phase 2b: AcceptedAcceptor接收到提议后，如果该提议版本号不等于自身保存记录的版本号（第一阶段记录的），不接受该请求，相等则写入本地。 整个paxos协议过程看似复杂难懂，但只要把握和理解这两点就基本理解了paxos的精髓： 理解第一阶段accpetor的处理流程：如果本地已经写入了，不再接受和同意后面的所有请求，并返回本地写入的值；如果本地未写入，则本地记录该请求的版本号，并不再接受其他版本号的请求，简单来说只信任最后一次提交的版本号的请求，使其他版本号写入失效； 理解第二阶段proposer的处理流程：未超过半数accpetor响应，提议失败；超过半数的accpetor值都为空才提交自身要写入的值，否则选择非空值里版本号最大的值提交，最大的区别在于是提交的值是自身的还是使用以前提交的。 Basic Paxos信息流协议过程举例看这个最简单的例子：1个processor，3个Acceptor，无learner。 目标：proposer向3个aceptort 将name变量写为v1。 第一阶段A：proposer发起prepare（name，n1）,n1是递增提议版本号，发送给3个Acceptor，说，我现在要写name这个变量，我的版本号是n1第一阶段B：Acceptor收到proposer的消息，比对自己内部保存的内容，发现之前name变量（null，null）没有被写入且未收到过提议，都返回给proposer，并在内部记录name这个变量，已经有proposer申请提议了，提议版本号是n1;第二阶段A：proposer收到3个Acceptor的响应，响应内容都是：name变量现在还没有写入，你可以来写。proposer确认获得超过半数以上Acceptor同意，发起第二阶段写入操作：accept（v1,n1），告诉Acceptor我现在要把name变量协议v1,我的版本号是刚刚获得通过的n1;第二阶段B：accpetor收到accept（v1,n1），比对自身的版本号是一致的，保存成功，并响应accepted（v1,n1）；结果阶段：proposer收到3个accepted响应都成功，超过半数响应成功，到此name变量被确定为v1。 以上是正常的paxos协议提议确定流程，是不是很简单，很容易理解呢？ 确定你理解了上面的例子再往后看。 这是最简单也最容易理解的例子，但真实情况远比这个复杂，还有以下问题： 如果其中的某个Acceptor没响应怎么处理？如果只写成功了一个accpetor又怎么处理，写成功两个呢？如果多个proposer并发写会导致accpetor写成不同值吗？learner角色是做什么用？为什么是超过半数同意？ paxos特殊情况下的处理第一种情况：Proposer提议正常，未超过accpetor失败情况 问题：还是上面的例子，如果第二阶段B，只有2个accpetor响应接收提议成功，另外1个没有响应怎么处理呢？ 处理：proposer发现只有2个成功，已经超过半数，那么还是认为提议成功，并把消息传递给learner，由learner角色将确定的提议通知给所有accpetor，最终使最后未响应的accpetor也同步更新，通过learner角色使所有Acceptor达到最终一致性。 第二种情况：Proposer提议正常，但超过accpetor失败情况 问题：假设有2个accpetor失败，又该如何处理呢？ 处理：由于未达到超过半数同意条件，proposer要么直接提示失败，要么递增版本号重新发起提议，如果重新发起提议对于第一次写入成功的accpetor不会修改，另外两个accpetor会重新接受提议，达到最终成功。 情况再复杂一点：还是一样有3个accpetor，但有两个proposer。 情况一：proposer1和proposer2串行执行 proposer1和最开始情况一样，把name设置为v1，并接受提议。 proposer1提议结束后，proposer2发起提议流程： 第一阶段A：proposer1发起prepare（name，n2） 第一阶段B：Acceptor收到proposer的消息，发现内部name已经写入确定了，返回（name,v1,n1） 第二阶段A：proposer收到3个Acceptor的响应，发现超过半数都是v1，说明name已经确定为v1，接受这个值，不再发起提议操作。 情况二：proposer1和proposer2交错执行 proposer1提议accpetor1成功，但写入accpetor2和accpetor3时，发现版本号已经小于accpetor内部记录的版本号（保存了proposer2的版本号），直接返回失败。 proposer2写入accpetor2和accpetor3成功，写入accpetor1失败，但最终还是超过半数写入v2成功，name变量最终确定为v2； proposer1递增版本号再重试发现超过半数为v2，接受name变量为v2，也不再写入v1。name最终确定还是为v2 情况三：proposer1和proposer2第一次都只写成功1个Acceptor怎么办 都只写成功一个，未超过半数，那么Proposer会递增版本号重新发起提议，这里需要分多种情况： 3个Acceptor都响应提议，发现Acceptor1{v1,n1} ,Acceptor2{v2,n2},Acceptor{null,null}，Processor选择最大的{v2,n2}发起第二阶段，成功后name值为v2; 2个Acceptor都响应提议， 如果是Acceptor1{v1,n1} ,Acceptor2{v2,n2}，那么选择最大的{v2,n2}发起第二阶段，成功后name值为v2; 如果是Acceptor1{v1,n1} ,Acceptor3{null,null}，那么选择最大的{v1,n1}发起第二阶段，成功后name值为v1; 如果是Acceptor2{v2,n2} ,Acceptor3{null,null}，那么选择最大的{v2,n2}发起第二阶段，成功后name值为v2; 只有1个Acceptor响应提议，未达到半数，放弃或者递增版本号重新发起提议可以看到，都未达到半数时，最终值是不确定的！ 参考资料 paxos算法wiki: https://en.wikipedia.org/wiki/Paxos_(computer_science)#Basic_Paxos 首先，推荐的是知行学社的《分布式系统与Paxos算法视频课程》： 视频讲的非常好，很适合入门，循序渐进慢慢推导，我自己看了不下5遍，视频讲解理解更深，推荐大家都看看。 推荐刘杰的《分布式系统原理介绍》 ，里面有关于paxos的详细介绍，例子非常多，也有包括paxos协议的证明过程，大而全，质量相当高的一份学习资料！ 推荐的一份高质量ppt《可靠分布式系统基础 Paxos 的直观解释》: https://drmingdrmer.github.io/tech/distributed/2015/11/11/paxos-slide.html； 技术类的东西怎么能只停留在看上面，肯定要看代码啊，推荐微信开源的phxpaxos：https://github.com/tencent-wechat/phxpaxos，结合代码对协议理解更深。 《Paxos Made live》。这篇论文是 Google 发表的，讨论了 paxos 的工程实践，也就是 chubby 这个众所周知的分布式服务的实现，可以结合《The Chubby lock service for loosely-coupled distributed systems》 一起看。实际应用中的难点，比如 master 租约实现、group membership 变化、Snapshot 加快复制和恢复以及实际应用中遇到的故障、测试等问题，特别是最后的测试部分。非常值得一读。《The Chubby lock service for loosely-coupled distributed systems》 更多介绍了 Chubby 服务本身的设计决策，为什么是分布式锁服务，为什么是粗粒度的锁，为什么是目录文件模式，事件通知、多机房部署以及应用碰到的使用问题等等。 Paxos 我还着重推荐阅读微信后端团队写的系列博客，包括他们开源的 phxpaxos 实现，基本上将所有问题都讨论到了，并且通俗易懂。 一致性方面另一块就是 Raft 算法，按照 Google Chubby 论文里的说法， 1`Indeed, all working protocols for asynchronous consensus we have so far encountered have Paxos at their core.` 但是 Raft 真的好理解多了，我读的是《In Search of an Understandable Consensus Algorithm》，论文写到这么详细的步骤，你不想理解都难。毕竟 Raft 号称就是一个 Understandable Consensus Algorithm。无论从任何角度，都推荐阅读这一篇论文。 首先能理解 paxos 的一些难点，其次是了解 Raft 的实现，加深对 Etcd 等系统的理解。这篇论文还有一个 250 多页的加强版《CONSENSUS: BRIDGING THEORY AND PRACTICE》，教你一行一行写出一个 Raft 实现，我还没有学习，有兴趣可以自行了解。Raft 通过明确引入 leader（其实 multi paxos 引申出来也有，但是没有这么明确的表述）来负责 client 交互和日志复制，将整个算法过程非常清晰地表达出来。Raft 的算法正确性的核心在于保证 Leader Completeness ，选举算法选出来的 leader 一定是包含了所有 committed entries 的，这是因为所有 committed entries 一定会在多数派至少一个成员里存在，所以设计的选举算法一定能选出来这么一个成员作为 leader。多数派 accept 应该说是一致性算法正确性的最重要的保证。 最后，我还读了《Building Consistent Transactions with Inconsistent Replication》，包括作者的演讲，作者也开放了源码。Google Spanner 基本是将 paxos 算法应用到了极致，但是毕竟不是所有公司都是这么财大气粗搞得起 TrueTime API，架得起全球机房，控制或者承受得了事务延时。这篇论文提出了另一个思路，论文介绍的算法分为两个层次： IR 和基于其他上的 TAPIR。前者就是 Inconsistent Replication，它将操作分为两类： inconsistent： 可以任意顺序执行，成功执行的操作将持久化，支持 failover。 consensus：同样可以任意顺序执行并持久化 failover，但是会返回一个唯一的一致(consensus)结果。 Java简单实现Basic Paxos算法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249/** * Alipay.com Inc. * Copyright (c) 2004-2019 All Rights Reserved. */package paxos;import com.google.common.base.Charsets;import org.apache.commons.lang3.StringUtils;import com.google.common.hash.HashFunction;import com.google.common.hash.Hashing;import java.util.ArrayList;import java.util.Arrays;import java.util.Collection;import java.util.Collections;import java.util.List;import java.util.Random;/** * @author xbyan * @version $Id: PaxosStudy.java, v 0.1 2019-07-01 5:47 PM xbyan Exp $$ */public class PaxosStudy &#123; private static final HashFunction HASH_FUNCTION = Hashing.murmur3_32(); private static final Random RAMDOM = new Random(); private static final String[] PROPOSALS = &#123; "ProjectA", "ProjectB", "ProjectC" &#125;; public static void main(String[] args) throws InterruptedException &#123; //新建server组 List&lt;Acceptor&gt; acceptors = new ArrayList&lt;&gt;(); Arrays.asList("A", "B", "C", "D", "E").forEach(name -&gt; acceptors.add(new Acceptor(name))); //开始投票 Proposer.vote(new Proposal(1L, null), acceptors); &#125; private static void printInfo(String subject, String operation, String result) throws InterruptedException &#123; System.out.println(subject + ":" + operation + "&lt;" + result + "&gt;"); Thread.sleep(1000); &#125; /** * 对于提案的约束，第三方约束要求 * 如果maxVote不存在，那么没有限制，下一次表决可以使用任意提案 * 否则，下一次表决要沿用maxVote提案 * * @param currentVoteNumber * @param proposals * @return */ private static Proposal nextProposal(long currentVoteNumber, List&lt;Proposal&gt; proposals) &#123; long voteNumber = currentVoteNumber + 1; //刚开始投票 if (proposals.isEmpty()) &#123; Proposal proposal = new Proposal(voteNumber, PROPOSALS[RAMDOM.nextInt(PROPOSALS.length)]); System.out.println("NEXT PROPOSER(刚开始投票), currentVoteNumber" + currentVoteNumber + ",proposal=" + proposal); return proposal; &#125; //为之前的票排序 Collections.sort(proposals); Proposal maxVote = proposals.get(proposals.size() - 1); //列表里最大值 long maxVoteNumber = maxVote.getVoteNumber(); String content = maxVote.getContent(); if (maxVoteNumber &gt;= currentVoteNumber) &#123; throw new IllegalStateException("illegal state maxVoteNumber"); &#125; if (content != null) &#123; Proposal proposal = new Proposal(voteNumber, content); System.out.println("NEXT PROPOSER(content不为空), voteNumber=" + voteNumber + ",proposal=" + proposal + ",maxVote=" + maxVote); return proposal; &#125; else &#123; Proposal proposal = new Proposal(voteNumber, PROPOSALS[RAMDOM.nextInt(PROPOSALS.length)]); System.out.println("NEXT PROPOSER(content为空), voteNumber=" + voteNumber + ",proposal=" + proposal + ",maxVote=" + maxVote); return proposal; &#125; &#125; private static class Proposer &#123; /** * @param proposal * @param acceptors */ public static void vote(Proposal proposal, Collection&lt;Acceptor&gt; acceptors) throws InterruptedException &#123; //法定人数 n/2+1 int quorum = Math.floorDiv(acceptors.size(), 2) + 1; System.out.println("quorum " + quorum); int count = 0; while (true) &#123; //开始投票 printInfo("VOTE_ROUND", "START", ++count + "：开始向acceptor发送信息"); List&lt;Proposal&gt; proposals = new ArrayList&lt;&gt;(); for (Acceptor acceptor : acceptors) &#123; Promise promise = acceptor.onPrepare(proposal); if (promise != null &amp;&amp; promise.isAck()) &#123; //Acceptor批准后就可以将发起者加入发起者列表里 System.out .println("--------------------------------------------------ACCEPTOR批准,将发起者加入队列里,proposal=" + promise.getProposal()); proposals.add(promise.getProposal()); &#125; &#125; if (proposals.size() &lt; quorum) &#123; //小于法定人数 printInfo("PROPOSER[" + proposal + "]", "VOTE", "NOT PREPARED：接收到消息的acceptor小于半数"); System.out.println("onPrepare阶段编号增加 重新发起投票"); proposal = nextProposal(proposal.getVoteNumber(), proposals); continue; &#125; System.out.println("接收到消息的acceptor大于半数，开始提案内容"); int acceptCount = 0; for (Acceptor acceptor1 : acceptors) &#123; if (acceptor1.onAccept(proposal)) acceptCount++; &#125; if (acceptCount &lt; quorum) &#123; printInfo("PROPOSER[" + proposal + "]", "VOTE", "NOT ACCEPTED,接受提案的少于半数"); System.out.println("onAccept阶段编号增加 重新发起投票"); proposal = nextProposal(proposal.getVoteNumber(), proposals); continue; &#125; break; &#125; printInfo("PROPOSER[" + proposal + "]", "VOTE", "SUCCESS，接受提案成功"); &#125; &#125; private static class Acceptor &#123; //上次表决结果 private Proposal last = new Proposal(); private String name; public Acceptor(String name) &#123; this.name = name; &#125; public Promise onPrepare(Proposal proposal) throws InterruptedException &#123; //假设这个过程有50%的几率失败 if (Math.random() - 0.5 &gt; 0) &#123; printInfo("ACCEPTER_" + name, "PREPARE", "NO RESPONSE 向" + name + " 发送失败"); return null; &#125; if (proposal == null) throw new IllegalArgumentException("null proposal"); //大于 if (proposal.getVoteNumber() &gt; last.getVoteNumber()) &#123; Promise response = new Promise(true, last); last = proposal; printInfo("ACCEPTER_" + name, "PREPARE", "OK " + name + " 记下编号，返回信息,last=" + last); return response; &#125; else &#123; printInfo("ACCEPTER_" + name, "PREPARE", "REJECTED " + name + " 已保存有选定的编号，拒绝"); return new Promise(false, null); &#125; &#125; public boolean onAccept(Proposal proposal) throws InterruptedException &#123; //假设这个过程有50%的几率失败 if (Math.random() - 0.5 &gt; 0) &#123; printInfo("ACCEPTER_" + name, "ACCEPT", "NO RESPONSE " + name + " 发送提案失败"); return false; &#125; boolean accepted = last.equals(proposal); printInfo("ACCEPTER_" + name, "ACCEPT", name + (accepted ? "OK：通过" : "NO: 不通过") + "last=" + last + ",proposal=" + proposal); return accepted; &#125; &#125; private static class Promise &#123; private final boolean ack; private final Proposal proposal; private Promise(boolean ack, Proposal proposal) &#123; this.ack = ack; this.proposal = proposal; &#125; public boolean isAck() &#123; return ack; &#125; public Proposal getProposal() &#123; return proposal; &#125; &#125; private static class Proposal implements Comparable&lt;Proposal&gt; &#123; private final long voteNumber; private final String content; public Proposal(long voteNumber, String content) &#123; this.voteNumber = voteNumber; this.content = content; &#125; public Proposal() &#123; this(0, null); &#125; public long getVoteNumber() &#123; return voteNumber; &#125; public String getContent() &#123; return content; &#125; public int compareTo(Proposal o) &#123; return Long.compare(voteNumber, o.voteNumber); &#125; @Override public boolean equals(Object obj) &#123; if (obj == null) &#123; return false; &#125; if (!(obj instanceof Proposal)) return false; Proposal proposal = (Proposal) obj; return voteNumber == proposal.voteNumber &amp;&amp; StringUtils.equals(content, proposal.content); &#125; @Override public int hashCode() &#123; return HASH_FUNCTION.newHasher().putLong(voteNumber).putString(content, Charsets.UTF_8) .hash().asInt(); &#125; @Override public String toString() &#123; return new StringBuilder().append(voteNumber).append(':').append(content).toString(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>paxos算法</category>
      </categories>
      <tags>
        <tag>paxos</tag>
        <tag>分布式一致性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paxos-by-example]]></title>
    <url>%2F2019%2F07%2F29%2Fpaxos-by-example%2F</url>
    <content type="text"><![CDATA[Paxos算法在分布式领域具有非常重要的地位。但是Paxos算法有两个比较明显的缺点：1.难以理解 2.工程实现更难。 网上有很多讲解Paxos算法的文章，但是质量参差不齐。看了很多关于Paxos的资料后发现，学习Paxos最好的资料是论文Paxos Made Simple，其次是中、英文版维基百科对Paxos的介绍。本文试图带大家一步步揭开Paxos神秘的面纱。 Paxos是什么 Paxos算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一。 Google Chubby的作者Mike Burrows说过这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品。 虽然Mike Burrows说得有点夸张，但是至少说明了Paxos算法的地位。然而，Paxos算法也因为晦涩难懂而臭名昭著。本文的目的就是带领大家深入浅出理解Paxos算法，不仅理解它的执行流程，还要理解算法的推导过程，作者是怎么一步步想到最终的方案的。只有理解了推导过程，才能深刻掌握该算法的精髓。而且理解推导过程对于我们的思维也是非常有帮助的，可能会给我们带来一些解决问题的思路，对我们有所启发。 最近在看paxos算法, 看了不少博客, 都感觉总结得不够到位,甚至有理解偏差的地方,倒是在博客的引用文章里面找到了一篇英文文献,看完后豁然开朗茅塞顿开. 于是把那篇英文原文翻译了一下, 加深一下理解 英文文献虽是2012年的了, 但是短小精悍,值得一看,原文地址是: angus.nyc/2012/paxos-by-example/ 文章中不全是直译, 有些地方改成了我自己的理解, 所以有可能有理解错的地方, 还望指出. 关于作者: Lamport提到Paxos算法，我们不得不首先来介绍下Paxos算法的作者Leslie Lamport(莱斯利·兰伯特, http://www.lamport.org )及其对计算机科学尤其是分布式计算领域的杰出贡献。作为2013年的新科图灵奖得主，Lamport是计算机科学领域一位拥有杰出成就的传奇人物，其先后多次荣获ACM和IEEE以及其他各类计算机重大奖项。Lamport对时间时钟、面包店算法、拜占庭将军问题以及Paxos算法的创造性研究，极大地推动了计算机科学尤其是分布式计算的发展，全世界无数工程师得益于他的理论，其中Paxos算法的提出，正是Lamport多年的研究成果。 说起Paxos理论的发表，还有一段非常有趣的历史故事。Lamport早在1990年就已经将其对Paxos算法的研究论文《The PartTime Parliament》提交给ACM TOCS Jnl.的评审委员会了，但是由于Lamport“创造性”地使用了故事的方式来进行算法的描述，导致当时委员会的工作人员没有一个能够正确地理解他对算法的描述，时任主编要求Lamport使用严谨的数据证明方式来描述该算法，否则他们将不考虑接受这篇论文。遗憾的是，Lamport并没有接收他们的建议，当然也就拒绝了对论文的修改，并撤销了对这篇论文的提交。在后来的一个会议上，Lamport还对此事耿耿于怀：“为什么这些搞理论的人一点幽默感也没有呢？” 幸运的是，还是有人能够理解Lamport那公认的令人晦涩的算法描述的。1996年，来自微软的Butler Lampson在WDAG96上提出了重新审视这篇分布式论文的建议，在次年的WDAG97上，麻省理工学院的Nancy Lynch也公布了其根据Lamport的原文重新修改后的《Revisiting the Paxos Algorithm》，“帮助”Lamport用数学的形式化术语定义并证明了Paxos算法。于是在1998年的ACM TOCS上，这篇延迟了9年的论文终于被接受了，也标志着Paxos算法正式被计算机科学接受并开始影响更多的工程师解决分布式一致性问题。 后来在2001年，Lamport本人也做出了让步，这次他放弃了故事的描述方式，而是使用了通俗易懂的语言重新讲述了原文，并发表了《Paxos Made Simple》——当然，Lamport甚为固执地认为他自己的表述语言没有歧义，并且也足够让人明白Paxos算法，因此不需要数学来协助描述，于是整篇文章还是没有任何数学符号。好在这篇文章已经能够被更多的人理解，相信绝大多数的Paxos爱好者也都是从这篇文章开始慢慢进入了Paxos的神秘世界。 由于Lamport个人自负固执的性格，使得Paxos理论的诞生可谓一波三折。关于Paxos理论的诞生过程，后来也成为了计算机科学领域被广泛流传的学术趣事。 图0: Lamport本尊 译文开始本文通过一个实例描述了一个叫Paxos的分布式一致性算法 分布式一致性算法通常是用于让多个计算机节点就某个单值的修改达成一致, 例如事务的提交commit或回滚rollback, 典型的思想就是二阶段提交或三阶段提交. 算法并不关心这个单值是什么,只关心最后只有唯一一个值被所有的节点选中, 对该值的修改达成一致 在一个分布式系统中这非常的难, 因为不同机器之前的消息可能会丢失, 或是被无限延迟, 甚至机器本身也会挂掉 Paxos算法可以保证最终只会有一个值会被选中, 但是不能保证如果集群中的大多数节点挂掉后,还能选中一个值 概述Paxos的节点可以是proposer, acceptor,learner中的任何一种角色.proposer会提议一个它想被采纳的值, 一般是通过发送一个包含该提议值的提案给集群中的每个acceptor. 然后acceptor会独立的决定是否接受这个提案–它可能会接收到多个proposer的不同提案, 决定好后会把它的决定发给learner. learner的作用是判断一个值是否已经被接受.在Paxos算法中, 只有一个值被大多数的acceptor选中, 才算被Paxos算法选中.在实际项目中, 一个节点可能担任多种角色, 但是在这个例子里面, 每一种角色都是一个独立的节点. 图1 Paxos的基本架构. 几个proposer向acceptor提交提案. 当一个acceptor接受了一个值后, 它把它的决定发给learner Paxos算法例子在标准的Paxos算法中, 一个proposer会发送两种类型的消息给acceptors: prepare request 和accept request. 在算法的第一阶段, proposer会发送一个prepare request给每一个acceptor, 这个prepare request包含一个提议值value和一个提案号number.每一个proposer的提案号number必须是正数, 单调递增的,独一无二的自然数[1]. 在下面图2的例子中, 有两个proposer, 都在发送prepare request. proposer A的请求([n=2,v=8])比proposer B的请求([n=4,v=5])先到达acceptor X和acceptor Y, 而proposer B的请求先到达acceptor Z 图2: Paxos. proposer A和B各发送了一个prepare request 给每一个accetor. 在这个例子中, proposer A的请求先到达acceptor X和Y, 而proposer B的请求先到达了acceptor Z. 如果一个acceptor接收到了一个prepare request而又没接收过其他的提案, 这个acceptor会回应一个prepare response, 并保证不会接受其他比当前提案号number更小的提案. 下面图3展示了每个acceptor是如何响应它们接收到的prepare request的 图3: 每个acceptor都对它接收到的第一个prepare request 进行prepare response. 最终, acceptor Z接收到了proposer A的请求[2], acceptor X和Y收到了proposer B的请求. 如果一个acceptor之前已经接收了一个提案号更高的prepare request的话, 那么后面收到的prepare request就会被忽略, 这个例子中就是acceptor Z会忽略掉proposer A的请求(Z已经接收了B的提案n=4). 如果acceptor之前没有接收过更高提案号的提案, 它会保证忽略其他提案号比这个更低的请求(注意是包括prepare request和accept request),然后在prepare response中把它已经选中的提案号最高的提议(包括这个提议的值)发送给proposer.在这个例子就是acceptor X和Y给proposer B的响应(X和Y已经接受了一个B的[n=2,v=8]的提案,当再收到[n=4, v=5]的提案时, 它保证会忽略任何n&lt;4的prepare request和accept request, 然后把[n=2, v=8]的prepare response回去给B) 图4: acceptor Z 忽略了proposer A的请求, 因为它已经接收到了更高的提议号(4 &gt; 2). acceptor X和Y给proposer B响应了它见过最高的提议号的提案, 并承诺会忽略提案号更小的提案 一旦proposer 收到了大多数acceptor的prepare response, 它就可以开始给所有的acceptor发送accept request了. 因为proposer A接收到prepare response中表明在它之前没有更早的提案. 所以它的accept request中的提案号和提议值都是跟prepare request中的一样.然而,这个accept request被所有的acceptor都拒绝了,因为它们都承诺了不会接收提议号比4更新的提案(因为都见过proposer B的提案了) Proposer B的accept request情况就跟proposer A的有所不同了. 首先它的提案号还是它之前的提案号(n=4), 但是提议值却不是它之前的提议值了(它之前提议的是v=5), 而是它在prepare response中接收到的提议值(v=8)[3] 图5: proposer B也发送了一个accept request给每一个acceptor.acceptor request中的提案号是它之前的提案号(4), 而提议值是它在prepare response中收到的值(8, 来自proposer A的提案[n=2, v=8]) 如果acceptor收到了一个提案号不小于它见过的accept request, 它会接受这个accept request并且发送一个通知给到learner节点. 当learner节点发现大多数acceptor都已经接受了一个值的时候,Paxos算法就认为这个值已经被选中. 当一个值被paxos选中后,后续的流程中将不能改变这个值.例如,当另一个proposer C发送一个提案号更高的prepare request(例如, [n=6, v=7])过来时,所有的acceptor都会响应它之前已经选中的提议(n=4,v=8).这会要求proposer C在后续的acceptor request中也只能发送[n=6, v=8]的提议, 即只能确认一遍已经选中的这个值. 再后面, 如果有少量的acceptor还没选中一个值的话, 这个流程会保证最终所有的节点都会一致地选中同一个值. 译文结束 提案号的唯一性的保证不是Paxos算法的内容 ↩︎ 这个也可能不会发送, 但是这个算法中也是有可能的 ↩︎ 注意这个是它从prepare response中收到的最高的提案号. 在这个例子中, proposer B的提案号(n=4)是比proposer A的提案号(n=2)大. 但是在它的prepare request的响应中, 它只收到了proposer A的提议([n=2, v=8]). 如果它在prepare response中没有收到其他的提案的话, 它就会发送自己的提案([n=4,v=5]) ↩︎ 参考资料 NEAT ALGORITHMS - PAXOS(小动画可以辅助读懂): http://harry.me/blog/2014/12/27/neat-algorithms-paxos/ paxos-by-example(通过一个例子理解paxos算法): https://juejin.im/post/5d159590f265da1b86089a89 Paxos Made Live: an amazing paper from Google describing the challenges of their Paxos implementation in Chubby: http://static.googleusercontent.com/media/research.google.com/en//archive/paxos_made_live.pdf A Quora thread explaining Paxos in a few different ways: https://www.quora.com/Distributed-Systems/What-is-a-simple-explanation-of-the-Paxos-algorithm Raft - An Understandable Consensus Algorithm. Raft is another conensus algorithm designed for humans to understand, which if you can tell from the above wall of text might be a problem with Paxos. https://ramcloud.stanford.edu/raft.pdf 左耳听风 - 推荐阅读：分布式数据调度相关论文: http://10-02.com/180118-32%20_%20%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87.html]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>paxos算法</category>
      </categories>
      <tags>
        <tag>paxos</tag>
        <tag>分布式一致性</tag>
      </tags>
  </entry>
</search>
