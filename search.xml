<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Java8新特性]]></title>
    <url>%2F2019%2F09%2F05%2Fjava8-new-feature%2F</url>
    <content type="text"><![CDATA[接口的默认方法和静态方法在Java8之前，接口中只能包含抽象方法。那么这有什么样弊端呢？比如，想再Collection接口中添加一个spliterator抽象方法，那么也就意味着之前所有实现Collection接口的实现类，都要重新实现spliterator这个方法才行。而接口的默认方法就是为了解决接口的修改与接口实现类不兼容的问题，作为代码向前兼容的一个方法。 那么如何在接口中定义一个默认方法呢？来看下JDK中Collection中如何定义spliterator方法的： 123default Spliterator&lt;E&gt; spliterator() &#123; return Spliterators.spliterator(this, 0);&#125; 可以看到定义接口的默认方法是通过default关键字。因此，在Java8中接口能够包含抽象方法外还能够包含若干个默认方法（即有完整逻辑的实例方法）。 1234567891011121314151617public interface IAnimal &#123; default void breath()&#123; System.out.println("breath!"); &#125;;&#125;public class DefaultMethodTest implements IAnimal &#123; public static void main(String[] args) &#123; DefaultMethodTest defaultMethod = new DefaultMethodTest(); defaultMethod.breath(); &#125;&#125;输出结果为：breath! 可以看出IAnimal接口中有由default定义的默认方法后，那么其实现类DefaultMethodTest也同样能够拥有实例方法breath。但是如果一个类继承多个接口，多个接口中有相同的方法就会产生冲突该如何解决？实际上默认方法的改进，使得java类能够拥有类似多继承的能力，即一个对象实例，将拥有多个接口的实例方法，自然而然也会存在方法重复冲突的问题。 下面来看一个例子： 1234567891011121314151617181920212223public interface IDonkey&#123; default void run() &#123; System.out.println("IDonkey run"); &#125;&#125;public interface IHorse &#123; default void run()&#123; System.out.println("Horse run"); &#125;&#125;public class DefaultMethodTest implements IDonkey,IHorse &#123; public static void main(String[] args) &#123; DefaultMethodTest defaultMethod = new DefaultMethodTest(); defaultMethod.breath(); &#125;&#125; 定义两个接口：IDonkey和IHorse，这两个接口中都有相同的run方法。DefaultMethodTest实现了这两个接口，由于这两个接口有相同的方法，因此就会产生冲突，不知道以哪个接口中的run方法为准，编译会出错：inherits unrelated defaults for run..... 解决方法 针对由默认方法引起的方法冲突问题，只有通过重写冲突方法，并方法绑定的方式，指定以哪个接口中的默认方法为准。 1234567891011public class DefaultMethodTest implements IAnimal,IDonkey,IHorse &#123; public static void main(String[] args) &#123; DefaultMethodTest defaultMethod = new DefaultMethodTest(); defaultMethod.run(); &#125; @Override public void run() &#123; IHorse.super.run(); &#125;&#125; DefaultMethodTest重写了run方法，并通过 IHorse.super.run();指定以IHorse中的run方法为准。 静态方法 在Java8中还有一个特性就是，接口中还可以声明静态方法，如下例： 123456public interface IAnimal &#123; default void breath()&#123; System.out.println("breath!"); &#125; static void run()&#123;&#125;&#125; 函数式接口FunctionInterface与lambda表达式参见: https://sainpo.top/2019/09/03/java8-lambda/ 方法引用方法引用是为了进一步简化lambda表达式，通过类名或者实例名与方法名的组合来直接访问到类或者实例已经存在的方法或者构造方法。方法引用使用::来定义，::的前半部分表示类名或者实例名，后半部分表示方法名，如果是构造方法就使用NEW来表示。 方法引用在Java8中使用方式相当灵活，总的来说，一共有以下几种形式： 静态方法引用：ClassName::methodName; 实例上的实例方法引用：instanceName::methodName; 超类上的实例方法引用：supper::methodName; 类的实例方法引用：ClassName:methodName; 构造方法引用Class:new; 数组构造方法引用::TypeName[]::new 下面来看一个例子： 123456789101112131415161718192021222324252627282930313233343536public class MethodReferenceTest &#123; public static void main(String[] args) &#123; ArrayList&lt;Car&gt; cars = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 5; i++) &#123; Car car = Car.create(Car::new); cars.add(car); &#125; cars.forEach(Car::showCar); &#125; @FunctionalInterface interface Factory&lt;T&gt; &#123; T create(); &#125; static class Car &#123; public void showCar() &#123; System.out.println(this.toString()); &#125; public static Car create(Factory&lt;Car&gt; factory) &#123; return factory.create(); &#125; &#125;&#125;输出结果：learn.MethodReferenceTest$Car@769c9116learn.MethodReferenceTest$Car@6aceb1a5learn.MethodReferenceTest$Car@2d6d8735learn.MethodReferenceTest$Car@ba4d54learn.MethodReferenceTest$Car@12bc6874 在上面的例子中使用了Car::new，即通过构造方法的方法引用的方式进一步简化了lambda的表达式，Car::showCar，即表示实例方法引用。 StreamJava8中有一种新的数据处理方式，那就是流Stream，结合lambda表达式能够更加简洁高效的处理数据。Stream使用一种类似于SQL语句从数据库查询数据的直观方式，对数据进行如筛选、排序以及聚合等多种操作。 什么是流StreamStream是一个来自数据源的元素队列并支持聚合操作，更像是一个更高版本的Iterator,原始版本的Iterator，只能一个个遍历元素并完成相应操作。而使用Stream，只需要指定什么操作，如“过滤长度大于10的字符串”等操作，Stream会内部遍历并完成指定操作。 Stream中的元素在管道中经过中间操作（intermediate operation）的处理后，最后由最终操作（terminal operation）得到最终的结果。 数据源：是Stream的来源，可以是集合、数组、I/O channel等转换而成的Stream； 基本操作：类似于SQL语句一样的操作，比如filter,map,reduce,find,match,sort等操作。 当我们操作一个流时，实际上会包含这样的执行过程： 获取数据源–&gt;转换成Stream–&gt;执行操作，返回一个新的Stream–&gt;再以新的Stream继续执行操作—&gt;直至最后操作输出最终结果。 生成Stream的方式生成Stream的方式主要有这样几种： 从接口Collection中和Arrays： Collection.stream(); Collection.parallelStream(); //相较于串行流，并行流能够大大提升执行效率 Arrays.stream(T array); Stream中的静态方法： Stream.of()； generate(Supplier s); iterate(T seed, UnaryOperator f); empty(); 其他方法 Random.ints() BitSet.stream() Pattern.splitAsStream(java.lang.CharSequence) JarFile.stream() BufferedReader.lines() 下面对前面常见的两种方式给出示例： 123456789101112131415161718public class StreamTest &#123; public static void main(String[] args) &#123; //1.使用Collection中的方法和Arrays String[] strArr = new String[]&#123;"a", "b", "c"&#125;; List&lt;String&gt; list = Arrays.asList(strArr); Stream&lt;String&gt; stream = list.stream(); Stream&lt;String&gt; stream1 = Arrays.stream(strArr); //2. 使用Stream中提供的静态方法 Stream&lt;String&gt; stream2 = Stream.of(strArr); Stream&lt;Double&gt; stream3 = Stream.generate(Math::random); Stream&lt;Object&gt; stream4 = Stream.empty(); Stream.iterate(1, i -&gt; i++); &#125;&#125; Stream的操作常见的Stream操作有这样几种： Intermediate（中间操作）:中间操作是指对流中数据元素做出相应转换或操作后依然返回为一个流Stream，仍然可以供下一次流操作使用。常用的有：map (mapToInt, flatMap 等)、 filter、 distinct、 sorted、 peek、 limit、 skip。 Termial（结束操作）：是指最终对Stream做出聚合操作，输出结果。 中间操作 filter：对Stream中元素进行过滤 过滤元素为空的字符串： 1long count = stream.filter(str -&gt; str.isEmpty()).count(); map：对Stream中元素按照指定规则映射成另一个元素 将每一个元素都添加字符串“_map” 1stream.map(str -&gt; str + "_map").forEach(System.out::println); map方法是一对一的关系，将stream中的每一个元素按照映射规则成另外一个元素，而如果是一对多的关系的话就需要使用flatmap方法。 concat：对流进行合并操作 concat方法将两个Stream连接在一起，合成一个Stream。若两个输入的Stream都时排序的，则新Stream也是排序的；若输入的Stream中任何一个是并行的，则新的Stream也是并行的；若关闭新的Stream时，原两个输入的Stream都将执行关闭处理。 12Stream.concat(Stream.of(1, 2, 3), Stream.of(4, 5, 6)). forEach(System.out::println); distinct：对流进行去重操作 去除流中重复的元素 1234567Stream&lt;String&gt; stream = Stream.of("a", "a", "b", "c"); stream.distinct().forEach(System.out::println);输出结果：abc limit：限制流中元素的个数 截取流中前两个元素： 123456Stream&lt;String&gt; stream = Stream.of("a", "a", "b", "c"); stream.limit(2).forEach(System.out::println);输出结果：aa skip：跳过流中前几个元素 丢掉流中前两个元素： 12345Stream&lt;String&gt; stream = Stream.of("a", "a", "b", "c"); stream.skip(2).forEach(System.out::println);输出结果：bc peek：对流中每一个元素依次进行操作，类似于forEach操作 JDK中给出的例子： 1234567891011Stream.of("one", "two", "three", "four") .filter(e -&gt; e.length() &gt; 3) .peek(e -&gt; System.out.println("Filtered value: " + e)) .map(String::toUpperCase) .peek(e -&gt; System.out.println("Mapped value: " + e)) .collect(Collectors.toList());输出结果：Filtered value: threeMapped value: THREEFiltered value: fourMapped value: FOUR sorted：对流中元素进行排序，可以通过sorted(Comparator&lt;? super T&gt; comparator)自定义比较规则 123456Stream&lt;Integer&gt; stream = Stream.of(3, 2, 1); stream.sorted(Integer::compareTo).forEach(System.out::println);输出结果：123 match：检查流中元素是否匹配指定的匹配规则 Stream 有三个 match 方法，从语义上说： allMatch：Stream 中全部元素符合传入的 predicate，返回 true； anyMatch：Stream 中只要有一个元素符合传入的 predicate，返回 true； noneMatch：Stream 中没有一个元素符合传入的 predicate，返回 true。 如检查Stream中每个元素是否都大于5： 12345Stream&lt;Integer&gt; stream = Stream.of(3, 2, 1);boolean match = stream.allMatch(integer -&gt; integer &gt; 5);System.out.println(match);输出结果：false 结束操作 Collectors中常见归约方法总结 重构和定制收集器Collectors count：统计Stream中元素的个数 1long count = stream.filter(str -&gt; str.isEmpty()).count(); max/min：找出流中最大或者最小的元素 12345Stream&lt;Integer&gt; stream = Stream.of(3, 2, 1); System.out.println(stream.max(Integer::compareTo).get());输出结果：3 forEach forEach方法前面已经用了好多次，其用于遍历Stream中的所元素，避免了使用for循环，让代码更简洁，逻辑更清晰。 示例： 12345Stream.of(5, 4, 3, 2, 1) .sorted() .forEach(System.out::println); // 打印结果 // 1，2，3,4,5 reduce Stream中的Reduce讲解 Optional为了解决空指针异常，在Java8之前需要使用if-else这样的语句去防止空指针异常，而在Java8就可以使用Optional来解决。Optional可以理解成一个数据容器，甚至可以封装null，并且如果值存在调用isPresent()方法会返回true。为了能够理解Optional。先来看一个例子： 12345678910111213141516171819public class OptionalTest &#123; private String getUserName(User user) &#123; return user.getUserName(); &#125; class User &#123; private String userName; public User(String userName) &#123; this.userName = userName; &#125; public String getUserName() &#123; return userName; &#125; &#125;&#125; 事实上，getUserName方法对输入参数并没有进行判断是否为null，因此，该方法是不安全的。如果在Java8之前，要避免可能存在的空指针异常的话就需要使用if-else进行逻辑处理，getUserName会改变如下： 123456private String getUserName(User user) &#123; if (user != null) &#123; return user.getUserName(); &#125; return null;&#125; 这是十分繁琐的一段代码。而如果使用Optional则会要精简很多： 1234private String getUserName(User user) &#123; Optional&lt;User&gt; userOptional = Optional.ofNullable(user); return userOptional.map(User::getUserName).orElse(null);&#125; Java8之前的if-else的逻辑判断，这是一种命令式编程的方式，而使用Optional更像是一种函数式编程，关注于最后的结果，而中间的处理过程交给JDK内部实现。 到现在，可以直观的知道Optional对避免空指针异常很有效，下面，对Optional的API进行归纳： 创建Optional Optional.empty():通过静态工厂方法Optional.empty，创建一个空的Optional对象； Optional of(T value):如果value为null的话，立即抛出NullPointerException； Optional ofNullable(T value)：使用静态工厂方法Optional.ofNullable，你可以创建一个允许null值的Optional对象。 实例代码： 1234//创建OptionalOptional&lt;Object&gt; optional = Optional.empty();Optional&lt;Object&gt; optional1 = Optional.ofNullable(null);Optional&lt;String&gt; optional2 = Optional.of(null); 常用方法 123456789101112131. boolean equals(Object obj)：判断其他对象是否等于 Optional；2. Optional&lt;T&gt; filter(Predicate&lt;? super &lt;T&gt; predicate)：如果值存在，并且这个值匹配给定的 predicate，返回一个Optional用以描述这个值，否则返回一个空的Optional；3. &lt;U&gt; Optional&lt;U&gt; flatMap(Function&lt;? super T,Optional&lt;U&gt;&gt; mapper)：如果值存在，返回基于Optional包含的映射方法的值，否则返回一个空的Optional；4. T get()：如果在这个Optional中包含这个值，返回值，否则抛出异常：NoSuchElementException；5. int hashCode()：返回存在值的哈希码，如果值不存在 返回 0；6. void ifPresent(Consumer&lt;? super T&gt; consumer)：如果值存在则使用该值调用 consumer , 否则不做任何事情；7. boolean isPresent()：如果值存在则方法会返回true，否则返回 false；8. &lt;U&gt;Optional&lt;U&gt; map(Function&lt;? super T,? extends U&gt; mapper)：如果存在该值，提供的映射方法，如果返回非null，返回一个Optional描述结果；9. T orElse(T other)：如果存在该值，返回值， 否则返回 other；10. T orElseGet(Supplier&lt;? extends T&gt; other)：如果存在该值，返回值， 否则触发 other，并返回 other 调用的结果；11. &lt;X extends Throwable&gt; T orElseThrow(Supplier&lt;? extends X&gt; exceptionSupplier)：如果存在该值，返回包含的值，否则抛出由 Supplier 继承的异常；12. String toString()：返回一个Optional的非空字符串，用来调试复制代码 Date/time API的改进在Java8之前的版本中，日期时间API存在很多的问题，比如： 线程安全问题：java.util.Date是非线程安全的，所有的日期类都是可变的； 设计很差：在java.util和java.sql的包中都有日期类，此外，用于格式化和解析的类在java.text包中也有定义。而每个包将其合并在一起，也是不合理的； 时区处理麻烦：日期类不提供国际化，没有时区支持，因此Java中引入了java.util.Calendar和Java.util.TimeZone类； 针对这些问题，Java8重新设计了日期时间相关的API，Java 8通过发布新的Date-Time API (JSR 310)来进一步加强对日期与时间的处理。在java.util.time包中常用的几个类有： 它通过指定一个时区，然后就可以获取到当前的时刻，日期与时间。Clock可以替换System.currentTimeMillis()与TimeZone.getDefault() Instant:一个instant对象表示时间轴上的一个时间点，Instant.now()方法会返回当前的瞬时点（格林威治时间）； Duration:用于表示两个瞬时点相差的时间量； LocalDate:一个带有年份，月份和天数的日期，可以使用静态方法now或者of方法进行创建； LocalTime:表示一天中的某个时间，同样可以使用now和of进行创建； LocalDateTime：兼有日期和时间； ZonedDateTime：通过设置时间的id来创建一个带时区的时间； DateTimeFormatter：日期格式化类，提供了多种预定义的标准格式； 示例代码如下： 12345678910111213141516171819202122232425public class TimeTest &#123; public static void main(String[] args) &#123; Clock clock = Clock.systemUTC(); Instant instant = clock.instant(); System.out.println(instant.toString()); LocalDate localDate = LocalDate.now(); System.out.println(localDate.toString()); LocalTime localTime = LocalTime.now(); System.out.println(localTime.toString()); LocalDateTime localDateTime = LocalDateTime.now(); System.out.println(localDateTime.toString()); ZonedDateTime zonedDateTime = ZonedDateTime.now(ZoneId.of("Asia/Shanghai")); System.out.println(zonedDateTime.toString()); &#125;&#125;输出结果为：2018-04-14T12:50:27.437Z2018-04-1420:50:27.6462018-04-14T20:50:27.6462018-04-14T20:50:27.647+08:00[Asia/Shanghai] 其他改进Java8还在其他细节上也做出了改变，归纳如下： 之前的版本，注解在同一个位置只能声明一次，而Java8版本中提供@Repeatable注解，来实现可重复注解； String类中提供了join方法来完成字符串的拼接； 在Arrays上提供了并行化处理数组的方式，比如利用Arrays类中的parallelSort可完成并行排序； 在Java8中在并发应用层面上也是下足了功夫：（1）提供功能更强大的Future：CompletableFuture；（2）StampedLock可用来替代ReadWriteLock；（3）性能更优的原子类：：LongAdder,LongAccumulator以及DoubleAdder和DoubleAccumulator； 编译器新增一些特性以及提供一些新的Java工具 参考资料Stream的参考资料： Stream API讲解 Stream讲解系列文章 对Stream的讲解很细致 Java8新特性之Stream API Optional的参考资料： Optional的API讲解很详细 对Optional部分的讲解还不错，值得参考 Java8新特性的介绍： Java8新特性指南，很好的资料 跟上 Java 8 – 你忽略了的新特性 Java8新特性学习系列教程]]></content>
      <categories>
        <category>java</category>
        <category>Java8新特性</category>
      </categories>
      <tags>
        <tag>Java8新特性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出 Java 8 Lambda 表达式]]></title>
    <url>%2F2019%2F09%2F03%2Fjava8-lambda%2F</url>
    <content type="text"><![CDATA[Java 是一流的面向对象语言，除了部分简单数据类型，Java 中的一切都是对象，即使数组也是一种对象，每个类创建的实例也是对象。在 Java 中定义的函数或方法不可能完全独立，也不能将方法作为参数或返回一个方法给实例。 从 Swing 开始，我们总是通过匿名类给方法传递函数功能，以下是旧版的事件监听代码： 1234567someObject.addMouseListener(new MouseAdapter() &#123; public void mouseClicked(MouseEvent e) &#123; //Event listener implementation goes here... &#125; &#125;); 在上面的例子里，为了给 Mouse 监听器添加自定义代码，我们定义了一个匿名内部类 MouseAdapter 并创建了它的对象，通过这种方式，我们将一些函数功能传给 addMouseListener 方法。 简而言之，在 Java 里将普通的方法或函数像参数一样传值并不简单，为此，Java 8 增加了一个语言级的新特性，名为 Lambda 表达式。 为什么 Java 需要 Lambda 表达式？如果忽视注解(Annotations)、泛型(Generics)等特性，自 Java 语言诞生时起，它的变化并不大。Java 一直都致力维护其对象至上的特征，在使用过 JavaScript 之类的函数式语言之后，Java 如何强调其面向对象的本质，以及源码层的数据类型如何严格变得更加清晰可感。其实，函数对 Java 而言并不重要，在 Java 的世界里，函数无法独立存在。 在函数式编程语言中，函数是一等公民，它们可以独立存在，你可以将其赋值给一个变量，或将他们当做参数传给其他函数。JavaScript 是最典型的函数式编程语言。点击此处以及此处可以清楚了解 JavaScript 这种函数式语言的好处。函数式语言提供了一种强大的功能——闭包，相比于传统的编程方法有很多优势，闭包是一个可调用的对象，它记录了一些信息，这些信息来自于创建它的作用域。Java 现在提供的最接近闭包的概念便是 Lambda 表达式，虽然闭包与 Lambda 表达式之间存在显著差别，但至少 Lambda 表达式是闭包很好的替代者。 在 Steve Yegge 辛辣又幽默的博客文章里，描绘了 Java 世界是如何严格地以名词为中心的，如果你还没看过，赶紧去读吧，写得非常风趣幽默，而且恰如其分地解释了为什么 Java 要引进 Lambda 表达式。 Lambda 表达式为 Java 添加了缺失的函数式编程特点，使我们能将函数当做一等公民看待。尽管不完全正确，我们很快就会见识到 Lambda 与闭包的不同之处，但是又无限地接近闭包。在支持一类函数的语言中，Lambda 表达式的类型将是函数。但是，在 Java 中，Lambda 表达式是对象，他们必须依附于一类特别的对象类型——函数式接口(functional interface)。我们会在后文详细介绍函数式接口。 Mario Fusco 的这篇思路清晰的文章介绍了为什么 Java 需要 Lambda 表达式。他解释了为什么现代编程语言必须包含闭包这类特性。 Lambda 表达式简介Lambda 表达式是一种匿名函数(对 Java 而言这并不完全正确，但现在姑且这么认为)，简单地说，它是没有声明的方法，也即没有访问修饰符、返回值声明和名字。 你可以将其想做一种速记，在你需要使用某个方法的地方写上它。当某个方法只使用一次，而且定义很简短，使用这种速记替代之尤其有效，这样，你就不必在类中费力写声明与方法了。 Java 中的 Lambda 表达式通常使用 (argument) -&gt; (body) 语法书写，例如： 123(arg1, arg2...) -&gt; &#123; body &#125;(type1 arg1, type2 arg2...) -&gt; &#123; body &#125; 以下是一些 Lambda 表达式的例子： 123456789(int a, int b) -&gt; &#123; return a + b; &#125;() -&gt; System.out.println("Hello World");(String s) -&gt; &#123; System.out.println(s); &#125;() -&gt; 42() -&gt; &#123; return 3.1415 &#125;; Lambda 表达式的结构让我们了解一下 Lambda 表达式的结构。 一个 Lambda 表达式可以有零个或多个参数 参数的类型既可以明确声明，也可以根据上下文来推断。例如：(int a)与(a)效果相同 所有参数需包含在圆括号内，参数之间用逗号相隔。例如：(a, b) 或 (int a, int b) 或 (String a, int b, float c) 空圆括号代表参数集为空。例如：() -&gt; 42 当只有一个参数，且其类型可推导时，圆括号（）可省略。例如：a -&gt; return a*a Lambda 表达式的主体可包含零条或多条语句 如果 Lambda 表达式的主体只有一条语句，花括号{}可省略。匿名函数的返回类型与该主体表达式一致 如果 Lambda 表达式的主体包含一条以上语句，则表达式必须包含在花括号{}中（形成代码块）。匿名函数的返回类型与代码块的返回类型一致，若没有返回则为空 方法引用从 Lambda 表达式到双冒号操作符使用 Lambda 表达式，我们已经看到代码可以变得非常简洁。 例如，要创建一个比较器，以下语法就足够了 1Comparator c = (Person p1, Person p2) -&gt; p1.getAge().compareTo(p2.getAge()); 然后，使用类型推断： 1Comparator c = (p1, p2) -&gt; p1.getAge().compareTo(p2.getAge()); 但是，我们可以使上面的代码更具表现力和可读性吗？我们来看一下： 1Comparator c = Comparator.comparing(Person::getAge); 使用 :: 运算符作为 Lambda 调用特定方法的缩写，并且拥有更好的可读性。 使用方式双冒号（::）操作符是 Java 中的方法引用。 当们使用一个方法的引用时，目标引用放在 :: 之前，目标引用提供的方法名称放在 :: 之后，即 目标引用::方法。比如： 1Person::getAge; 在 Person 类中定义的方法 getAge 的方法引用。 然后我们可以使用 Function 对象进行操作： 1234// 获取 getAge 方法的 Function 对象Function&lt;Person, Integer&gt; getAge = Person::getAge;// 传参数调用 getAge 方法Integer age = getAge.apply(p); 我们引用 getAge，然后将其应用于正确的参数。 目标引用的参数类型是 Function&lt;T,R&gt;，T 表示传入类型，R 表示返回类型。比如，表达式 person -&gt; person.getAge();，传入参数是 person，返回值是 person.getAge()，那么方法引用 Person::getAge 就对应着 Function&lt;Person,Integer&gt; 类型。 函数式接口在 Java 中，Marker（标记）类型的接口是一种没有方法或属性声明的接口，简单地说，marker 接口是空接口。相似地，函数式接口是只包含一个抽象方法声明的接口。因此经常使用的Runnable，Callable接口就是典型的函数式接口。 java.lang.Runnable 就是一种函数式接口，在 Runnable 接口中只声明了一个方法 void run()，相似地，ActionListener 接口也是一种函数式接口，我们使用匿名内部类来实例化函数式接口的对象，有了 Lambda 表达式，这一方式可以得到简化。 每个 Lambda 表达式都能隐式地赋值给函数式接口，例如，我们可以通过 Lambda 表达式创建 Runnable 接口的引用。 1Runnable r = () -&gt; System.out.println("hello world"); 当不指明函数式接口时，编译器会自动解释这种转化： 123new Thread( () -&gt; System.out.println("hello world")).start(); 因此，在上面的代码中，编译器会自动推断：根据线程类的构造函数签名 public Thread(Runnable r) { }，将该 Lambda 表达式赋给 Runnable 接口。 在 Java8 前，JDK 已经提供哪些函数式接口了， 12345678@FunctionalInterfacepublic interface Runnable &#123; public abstract void run();&#125;java.lang.Runnable,java.lang.Comparable,java.util.concurrent.Callable; JDK8 又新增了哪些通用的？你知道他们各自的使用场景吗？ 1234java.util.function.Consumer&lt;T&gt;,java.util.function.Supplier&lt;T&gt;,java.util.function.Predicate&lt;T&gt;,java.util.function.Function&lt;T,R&gt;, 以下是一些 Lambda 表达式及其函数式接口： 12345Consumer&lt;Integer&gt; c = (int x) -&gt; &#123; System.out.println(x) &#125;;BiConsumer&lt;Integer, String&gt; b = (Integer x, String y) -&gt; System.out.println(x + " : " + y);Predicate&lt;String&gt; p = (String s) -&gt; &#123; s == null &#125;; @FunctionalInterface 是 Java 8 新加入的一种接口，用于指明该接口类型声明是根据 Java 语言规范定义的函数式接口。Java 8 还声明了一些 Lambda 表达式可以使用的函数式接口，当你注释的接口不是有效的函数式接口时，可以使用 @FunctionalInterface 解决编译层面的错误。 以下是一种自定义的函数式接口： 1234@FunctionalInterface public interface WorkerInterface &#123; public void doSomeWork();&#125; 根据定义，函数式接口只能有一个抽象方法，如果你尝试添加第二个抽象方法，将抛出编译时错误。例如： 12345678@FunctionalInterfacepublic interface WorkerInterface &#123; public void doSomeWork(); public void doSomeMoreWork();&#125; 错误： 123Unexpected @FunctionalInterface annotation @FunctionalInterface ^ WorkerInterface is not a functional interface multiple non-overriding abstract methods found in interface WorkerInterface 1 error 但是，函数式接口要求只有一个抽象方法却可以拥有若干个默认方法的（实例方法），比如下例： 12345678@java.lang.FunctionalInterfacepublic interface FunctionalInterface &#123; void handle(); default void run() &#123; System.out.println("run"); &#125;&#125; 该接口中，除了有抽象方法handle外，还有默认方法（实例方法）run。 函数式接口定义好后，我们可以在 API 中使用它，同时利用 Lambda 表达式。例如： 123456789101112131415161718192021222324252627282930 //定义一个函数式接口@FunctionalInterfacepublic interface WorkerInterface &#123; public void doSomeWork();&#125;public class WorkerInterfaceTest &#123;public static void execute(WorkerInterface worker) &#123; worker.doSomeWork();&#125;public static void main(String [] args) &#123; //invoke doSomeWork using Annonymous class execute(new WorkerInterface() &#123; @Override public void doSomeWork() &#123; System.out.println("Worker invoked using Anonymous class"); &#125; &#125;); //invoke doSomeWork using Lambda expression execute( () -&gt; System.out.println("Worker invoked using Lambda expression") );&#125;&#125; 输出： 12Worker invoked using Anonymous class Worker invoked using Lambda expression 这上面的例子里，我们创建了自定义的函数式接口并与 Lambda 表达式一起使用。execute() 方法现在可以将 Lambda 表达式作为参数。 Lambda 表达式举例线程初始化线程可以通过以下方法初始化： 123456789101112// Old waynew Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("Hello world"); &#125;&#125;).start();// New waynew Thread( () -&gt; System.out.println("Hello world")).start(); 事件处理事件处理可以使用 Java 8 的 Lambda 表达式解决。下面的代码中，我们将使用新旧两种方式向一个 UI 组件添加 ActionListener： 123456789101112 //Old way:button.addActionListener(new ActionListener() &#123;@Overridepublic void actionPerformed(ActionEvent e) &#123; System.out.println("The button was clicked using old fashion code!");&#125;&#125;);//New way:button.addActionListener( (e) -&gt; &#123; System.out.println("The button was clicked. From Lambda expressions !");&#125;); 遍例输出（方法引用）以下代码的作用是打印出给定数组中的所有元素。注意，使用 Lambda 表达式的方法不止一种。在下面的例子中，我们先是用常用的箭头语法创建 Lambda 表达式，之后，使用 Java 8 全新的双冒号(::)操作符将一个常规方法转化为 Lambda 表达式： 12345678910111213141516171819202122232425//Old way:List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7);for(Integer n: list) &#123; System.out.println(n);&#125;//1.8 非lambdaintegers.forEach(new Consumer&lt;Integer&gt;() &#123; @Override public void accept(Integer x) &#123; System.out.print(x); &#125;&#125;);//可以将 Lambda 表达式赋值给函数接口的局部变量Consumer&lt;Integer&gt; consumer=x -&gt; System.out.println(x);integers.forEach(consumer);//使用 -&gt; 的 Lambda 表达式List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7);list.forEach(n -&gt; System.out.println(n));//使用 :: 的 Lambda 表达式list.forEach(System.out::println); 逻辑操作在下面的例子中，我们使用断言(Predicate)函数式接口创建一个测试，并打印所有通过测试的元素，这样，你就可以使用 Lambda 表达式规定一些逻辑，并以此为基础有所作为： 123456789101112131415161718192021222324252627282930313233343536package com.wuxianjiezh.demo.lambda;import java.util.Arrays;import java.util.List;import java.util.function.Predicate;public class Main &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7); System.out.print("输出所有数字："); evaluate(list, (n) -&gt; true); System.out.print("不输出："); evaluate(list, (n) -&gt; false); System.out.print("输出偶数："); evaluate(list, (n) -&gt; n % 2 == 0); System.out.print("输出奇数："); evaluate(list, (n) -&gt; n % 2 == 1); System.out.print("输出大于 5 的数字："); evaluate(list, (n) -&gt; n &gt; 5); &#125; public static void evaluate(List&lt;Integer&gt; list, Predicate&lt;Integer&gt; predicate) &#123; for (Integer n : list) &#123; if (predicate.test(n)) &#123; System.out.print(n + " "); &#125; &#125; System.out.println(); &#125;&#125; 输出： 12345输出所有数字：1 2 3 4 5 6 7 不输出：输出偶数：2 4 6 输出奇数：1 3 5 7 输出大于 5 的数字：6 7 Stream API 示例下面的例子使用 Lambda 表达式打印数值中每个元素的平方，注意我们使用了 .stream() 方法将常规数组转化为流。Java 8 增加了一些超棒的流 APIs。java.util.stream.Stream 接口包含许多有用的方法，能结合 Lambda 表达式产生神奇的效果。我们将 Lambda 表达式 x -&gt; x*x 传给 map() 方法，该方法会作用于流中的所有元素。之后，我们使用 forEach 方法打印数据中的所有元素： 12345678910//Old way:List&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);for(Integer n : list) &#123; int x = n * n; System.out.println(x);&#125;//New way:List&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);list.stream().map((x) -&gt; x*x).forEach(System.out::println); 下面的例子会计算给定数值中每个元素平方后的总和。请注意，Lambda 表达式只用一条语句就能达到此功能，这也是 MapReduce 的一个初级例子。我们使用 map() 给每个元素求平方，再使用 reduce() 将所有元素计入一个数值： 12345678910111213//Old way:List&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);int sum = 0;for(Integer n : list) &#123; int x = n * n; sum = sum + x;&#125;System.out.println(sum);//New way:List&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);int sum = list.stream().map(x -&gt; x*x).reduce((x,y) -&gt; x + y).get();System.out.println(sum); Lambda 表达式与匿名类的区别 this 关键字。对于匿名类 this 关键字解析为匿名类，而对于 Lambda 表达式，this 关键字解析为包含写入 Lambda 的类。 编译方式。Java 编译器编译 Lambda 表达式时，会将其转换为类的私有方法，再进行动态绑定。]]></content>
      <categories>
        <category>java</category>
        <category>lambda表达式</category>
      </categories>
      <tags>
        <tag>lambda表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事务]]></title>
    <url>%2F2019%2F08%2F28%2Fdistributed-transaction%2F</url>
    <content type="text"><![CDATA[事务由一组操作构成，我们希望这组操作能够全部正确执行，如果这一组操作中的任意一个步骤发生错误，那么就需要回滚之前已经完成的操作。也就是同一个事务中的所有操作，要么全都正确执行，要么全都不要执行。 事务的四大特性 ACID说到事务，就不得不提一下事务著名的四大特性。 原子性 原子性要求，事务是一个不可分割的执行单元，事务中的所有操作要么全都执行，要么全都不执行。 一致性 一致性要求，事务在开始前和结束后，数据库的完整性约束没有被破坏。 隔离性 事务的执行是相互独立的，它们不会相互干扰，一个事务不会看到另一个正在运行过程中的事务的数据。 持久性 持久性要求，一个事务完成之后，事务的执行结果必须是持久化保存的。即使数据库发生崩溃，在数据库恢复后事务提交的结果仍然不会丢失。 注意：事务只能保证数据库的高可靠性，即数据库本身发生问题后，事务提交后的数据仍然能恢复；而如果不是数据库本身的故障，如硬盘损坏了，那么事务提交的数据可能就丢失了。这属于『高可用性』的范畴。因此，事务只能保证数据库的『高可靠性』，而『高可用性』需要整个系统共同配合实现。 什么是分布式事务？到此为止，所介绍的事务都是基于单数据库的本地事务，目前的数据库仅支持单库事务，并不支持跨库事务。而随着微服务架构的普及，一个大型业务系统往往由若干个子系统构成，这些子系统又拥有各自独立的数据库。往往一个业务流程需要由多个子系统共同完成，而且这些操作可能需要在一个事务中完成。在微服务系统中，这些业务场景是普遍存在的。此时，我们就需要在数据库之上通过某种手段，实现支持跨数据库的事务支持，这也就是大家常说的“分布式事务”。 这里举一个分布式事务的典型例子——用户下单过程。 当我们的系统采用了微服务架构后，一个电商系统往往被拆分成如下几个子系统：商品系统、订单系统、支付系统、积分系统等。整个下单的过程如下： 用户通过商品系统浏览商品，他看中了某一项商品，便点击下单 此时订单系统会生成一条订单 订单创建成功后，支付系统提供支付功能 当支付完成后，由积分系统为该用户增加积分 上述步骤2、3、4需要在一个事务中完成。对于传统单体应用而言，实现事务非常简单，只需将这三个步骤放在一个方法A中，再用Spring的@Transactional注解标识该方法即可。Spring通过数据库的事务支持，保证这些步骤要么全都执行完成，要么全都不执行。但在这个微服务架构中，这三个步骤涉及三个系统，涉及三个数据库，此时我们必须在数据库和应用系统之间，通过某项黑科技，实现分布式事务的支持。 BASE理论CAP理论告诉我们一个悲惨但不得不接受的事实——我们只能在C、A、P中选择两个条件。而对于业务系统而言，我们往往选择牺牲一致性来换取系统的可用性和分区容错性。不过这里要指出的是，所谓的“牺牲一致性”并不是完全放弃数据一致性，而是牺牲强一致性换取弱一致性。下面来介绍下BASE理论。 BA：Basic Available 基本可用 整个系统在某些不可抗力的情况下，仍然能够保证“可用性”，即一定时间内仍然能够返回一个明确的结果。只不过“基本可用”和“高可用”的区别是： “一定时间”可以适当延长 当举行大促时，响应时间可以适当延长 给部分用户返回一个降级页面 给部分用户直接返回一个降级页面，从而缓解服务器压力。但要注意，返回降级页面仍然是返回明确结果。 S：Soft State：柔性状态 同一数据的不同副本的状态，可以不需要实时一致。 E：Eventual Consisstency：最终一致性 同一数据的不同副本的状态，可以不需要实时一致，但一定要保证经过一定时间后仍然是一致的。 酸碱平衡ACID能够保证事务的强一致性，即数据是实时一致的。这在本地事务中是没有问题的，在分布式事务中，强一致性会极大影响分布式系统的性能，因此分布式系统中遵循BASE理论即可。但分布式系统的不同业务场景对一致性的要求也不同。如交易场景下，就要求强一致性，此时就需要遵循ACID理论，而在注册成功后发送短信验证码等场景下，并不需要实时一致，因此遵循BASE理论即可。因此要根据具体业务场景，在ACID和BASE之间寻求平衡。 分布式事务协议两阶段提交协议 2PC说到2PC就不得不聊数据库分布式事务中的 XA Transactions。 分布式系统的一个难点是如何保证架构下多个节点在进行事务性操作的时候保持一致性。为实现这个目的，二阶段提交算法的成立基于以下假设： 该分布式系统中，存在一个节点作为协调者(Coordinator)，其他节点作为参与者(Cohorts)。且节点之间可以进行网络通信。 所有节点都采用预写式日志，且日志被写入后即被保持在可靠的存储设备上，即使节点损坏不会导致日志数据的消失。 所有节点不会永久性损坏，即使损坏后仍然可以恢复。 1. 第一阶段（投票阶段）： 协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。 参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作） 各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。 2. 第二阶段（提交执行阶段）： 当协调者节点从所有参与者节点获得的相应消息都为”同意”时： 协调者节点向所有参与者节点发出”正式提交(commit)”的请求。 参与者节点正式完成操作，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”完成”消息。 协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。 如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时： 协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。 参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”回滚完成”消息。 协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。 不管最后结果如何，第二阶段都会结束当前事务。 二阶段提交看起来确实能够提供原子性的操作，但是不幸的事，二阶段提交还是有几个缺点的： 执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。 参与者发生故障。协调者需要给每个参与者额外指定超时机制，超时后整个事务失败。（没有多少容错机制） 协调者发生故障。参与者会一直阻塞下去。需要额外的备机进行容错。（这个可以依赖后面要讲的Paxos协议实现HA） 二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。 为此，Dale Skeen和Michael Stonebraker在“A Formal Model of Crash Recovery in a Distributed System”中提出了三阶段提交协议（3PC）。 三阶段提交协议 3PC与两阶段提交不同的是，三阶段提交有两个改动点。 引入超时机制。同时在协调者和参与者中都引入超时机制。 在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。 也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。 1. CanCommit阶段 3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。 事务询问 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。 响应反馈 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No 2. PreCommit阶段 协调者根据参与者的反应情况来决定是否可以记性事务的PreCommit操作。根据响应情况，有以下两种可能。 假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。 发送预提交请求 协调者向参与者发送PreCommit请求，并进入Prepared阶段。 事务预提交 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。 响应反馈 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。 假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。 发送中断请求 协调者向所有参与者发送abort请求。 中断事务 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。 3. doCommit阶段 该阶段进行真正的事务提交，也可以分为以下两种情况。 该阶段进行真正的事务提交，也可以分为以下两种情况。 3.1 执行提交 发送提交请求 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。 事务提交 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。 响应反馈 事务提交完之后，向协调者发送Ack响应。 完成事务 协调者接收到所有参与者的ack响应之后，完成事务。 3.2 中断事务 协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。 发送中断请求 协调者向所有参与者发送abort请求 事务回滚 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。 反馈结果 参与者完成事务回滚之后，向协调者发送ACK消息 中断事务 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。 分布式事务的解决方案分布式事务的解决方案有如下几种： 全局消息 基于可靠消息服务的分布式事务 TCC 最大努力通知 方案1：全局事务（DTP模型）全局事务基于DTP模型实现。DTP是由X/Open组织提出的一种分布式事务模型——X/Open Distributed Transaction Processing Reference Model。它规定了要实现分布式事务，需要三种角色： AP：Application 应用系统 它就是我们开发的业务系统，在我们开发的过程中，可以使用资源管理器提供的事务接口来实现分布式事务。 TM：Transaction Manager 事务管理器 分布式事务的实现由事务管理器来完成，它会提供分布式事务的操作接口供我们的业务系统调用。这些接口称为TX接口。 事务管理器还管理着所有的资源管理器，通过它们提供的XA接口来同一调度这些资源管理器，以实现分布式事务。 DTP只是一套实现分布式事务的规范，并没有定义具体如何实现分布式事务，TM可以采用2PC、3PC、Paxos等协议实现分布式事务。 RM：Resource Manager 资源管理器 能够提供数据服务的对象都可以是资源管理器，比如：数据库、消息中间件、缓存等。大部分场景下，数据库即为分布式事务中的资源管理器。 资源管理器能够提供单数据库的事务能力，它们通过XA接口，将本数据库的提交、回滚等能力提供给事务管理器调用，以帮助事务管理器实现分布式的事务管理。 XA是DTP模型定义的接口，用于向事务管理器提供该资源管理器(该数据库)的提交、回滚等能力。 DTP只是一套实现分布式事务的规范，RM具体的实现是由数据库厂商来完成的。 有没有基于DTP模型的分布式事务中间件？ DTP模型有啥优缺点？ 方案2：本地消息表本地消息表这个方案最初是ebay提出的 ebay的完整方案https://queue.acm.org/detail.cfm?id=1394128。 此方案的核心是将需要分布式处理的任务通过消息日志的方式来异步执行。消息日志可以存储到本地文本、数据库或消息队列，再通过业务规则自动或人工发起重试。人工重试更多的是应用于支付场景，通过对账系统对事后问题的处理。 对于本地消息队列来说核心是把大事务转变为小事务。举个简单的例子如果你用100元买了一瓶水， Try阶段:你需要向你的钱包检查是否够100元并锁住这100元，水也是一样的。 1.当你扣钱的时候，你需要在你扣钱的服务器上新增加一个本地消息表，你需要把你扣钱和写入减去水的库存到本地消息表放入同一个事务(依靠数据库本地事务保证一致性)。 2.这个时候有个定时任务去轮询这个本地事务表，把没有发送的消息，扔给商品库存服务器，叫他减去水的库存，到达商品服务器之后这个时候得先写入这个服务器的事务表，然后进行扣减，扣减成功后，更新事务表中的状态。 3.商品服务器通过定时任务扫描消息表或者直接通知扣钱服务器，扣钱服务器本地消息表进行状态更新。 4.针对一些异常情况，定时扫描未成功处理的消息，进行重新发送，在商品服务器接到消息之后，首先判断是否是重复的，如果已经接收，在判断是否执行，如果执行在马上又进行通知事务，如果未执行，需要重新执行需要由业务保证幂等，也就是不会多扣一瓶水。 本地消息队列是BASE理论，是最终一致模型，适用于对一致性要求不高的。实现这个模型时需要注意重试的幂等。 方案3：基于可靠消息服务的分布式事务这种实现分布式事务的方式需要通过消息中间件来实现。假设有A和B两个系统，分别可以处理任务A和任务B。此时系统A中存在一个业务流程，需要将任务A和任务B在同一个事务中处理。下面来介绍基于消息中间件来实现这种分布式事务。 在RocketMQ中实现了分布式事务，实际上其实是对本地消息表的一个封装，将本地消息表移动到了MQ内部，下面简单介绍一下MQ事务。 在系统A处理任务A前，首先向消息中间件发送一条消息 消息中间件收到后将该条消息持久化，但并不投递。此时下游系统B仍然不知道该条消息的存在。 消息中间件持久化成功后，便向系统A返回一个确认应答； 系统A收到确认应答后，则可以开始处理任务A； 任务A处理完成后，向消息中间件发送Commit请求。该请求发送完成后，对系统A而言，该事务的处理过程就结束了，此时它可以处理别的任务了。 但commit消息可能会在传输途中丢失，从而消息中间件并不会向系统B投递这条消息，从而系统就会出现不一致性。这个问题由消息中间件的事务回查机制完成，下文会介绍。 消息中间件收到Commit指令后，便向系统B投递该消息，从而触发任务B的执行； 当任务B执行完成后，系统B向消息中间件返回一个确认应答，告诉消息中间件该消息已经成功消费，此时，这个分布式事务完成。 上述过程可以得出如下几个结论： 1. 消息中间件扮演者分布式事务协调者的角色。 2. 系统A完成任务A后，到任务B执行完成之间，会存在一定的时间差。在这个时间差内，整个系统处于数据不一致的状态，但这短暂的不一致性是可以接受的，因为经过短暂的时间后，系统又可以保持数据一致性，满足BASE理论。 上述过程中，如果任务A处理失败，那么需要进入回滚流程，如下图所示： 若系统A在处理任务A时失败，那么就会向消息中间件发送Rollback请求。和发送Commit请求一样，系统A发完之后便可以认为回滚已经完成，它便可以去做其他的事情。 消息中间件收到回滚请求后，直接将该消息丢弃，而不投递给系统B，从而不会触发系统B的任务B。 此时系统又处于一致性状态，因为任务A和任务B都没有执行。 上面所介绍的Commit和Rollback都属于理想情况，但在实际系统中，Commit和Rollback指令都有可能在传输途中丢失。那么当出现这种情况的时候，消息中间件是如何保证数据一致性呢？——答案就是超时询问机制。 系统A除了实现正常的业务流程外，还需提供一个事务询问的接口，供消息中间件调用。当消息中间件收到一条事务型消息后便开始计时，如果到了超时时间也没收到系统A发来的Commit或Rollback指令的话，就会主动调用系统A提供的事务询问接口询问该系统目前的状态。该接口会返回三种结果： 提交 若获得的状态是“提交”，则将该消息投递给系统B。 回滚 若获得的状态是“回滚”，则直接将条消息丢弃。 处理中 若获得的状态是“处理中”，则继续等待。 消息中间件的超时询问机制能够防止上游系统因在传输过程中丢失Commit/Rollback指令而导致的系统不一致情况，而且能降低上游系统的阻塞时间，上游系统只要发出Commit/Rollback指令后便可以处理其他任务，无需等待确认应答。而Commit/Rollback指令丢失的情况通过超时询问机制来弥补，这样大大降低上游系统的阻塞时间，提升系统的并发度。 下面来说一说消息投递过程的可靠性保证。 当上游系统执行完任务并向消息中间件提交了Commit指令后，便可以处理其他任务了，此时它可以认为事务已经完成，接下来消息中间件一定会保证消息被下游系统成功消费掉！那么这是怎么做到的呢？这由消息中间件的投递流程来保证。 消息中间件向下游系统投递完消息后便进入阻塞等待状态，下游系统便立即进行任务的处理，任务处理完成后便向消息中间件返回应答。消息中间件收到确认应答后便认为该事务处理完毕！ 如果消息在投递过程中丢失，或消息的确认应答在返回途中丢失，那么消息中间件在等待确认应答超时之后就会重新投递，直到下游消费者返回消费成功响应为止。当然，一般消息中间件可以设置消息重试的次数和时间间隔，比如：当第一次投递失败后，每隔五分钟重试一次，一共重试3次。如果重试3次之后仍然投递失败，那么这条消息就需要人工干预。 有的同学可能要问：消息投递失败后为什么不回滚消息，而是不断尝试重新投递？ 这就涉及到整套分布式事务系统的实现成本问题。 我们知道，当系统A将向消息中间件发送Commit指令后，它便去做别的事情了。如果此时消息投递失败，需要回滚的话，就需要让系统A事先提供回滚接口，这无疑增加了额外的开发成本，业务系统的复杂度也将提高。对于一个业务系统的设计目标是，在保证性能的前提下，最大限度地降低系统复杂度，从而能够降低系统的运维成本。 不知大家是否发现，上游系统A向消息中间件提交Commit/Rollback消息采用的是异步方式，也就是当上游系统提交完消息后便可以去做别的事情，接下来提交、回滚就完全交给消息中间件来完成，并且完全信任消息中间件，认为它一定能正确地完成事务的提交或回滚。然而，消息中间件向下游系统投递消息的过程是同步的。也就是消息中间件将消息投递给下游系统后，它会阻塞等待，等下游系统成功处理完任务返回确认应答后才取消阻塞等待。为什么这两者在设计上是不一致的呢？ 首先，上游系统和消息中间件之间采用异步通信是为了提高系统并发度。业务系统直接和用户打交道，用户体验尤为重要，因此这种异步通信方式能够极大程度地降低用户等待时间。此外，异步通信相对于同步通信而言，没有了长时间的阻塞等待，因此系统的并发性也大大增加。但异步通信可能会引起Commit/Rollback指令丢失的问题，这就由消息中间件的超时询问机制来弥补。 那么，消息中间件和下游系统之间为什么要采用同步通信呢？ 异步能提升系统性能，但随之会增加系统复杂度；而同步虽然降低系统并发度，但实现成本较低。因此，在对并发度要求不是很高的情况下，或者服务器资源较为充裕的情况下，我们可以选择同步来降低系统的复杂度。 我们知道，消息中间件是一个独立于业务系统的第三方中间件，它不和任何业务系统产生直接的耦合，它也不和用户产生直接的关联，它一般部署在独立的服务器集群上，具有良好的可扩展性，所以不必太过于担心它的性能，如果处理速度无法满足我们的要求，可以增加机器来解决。而且，即使消息中间件处理速度有一定的延迟那也是可以接受的，因为前面所介绍的BASE理论就告诉我们了，我们追求的是最终一致性，而非实时一致性，因此消息中间件产生的时延导致事务短暂的不一致是可以接受的。 方案4：最大努力通知（定期校对）最大努力通知也被称为定期校对，其实在方案二中已经包含，这里再单独介绍，主要是为了知识体系的完整性。这种方案也需要消息中间件的参与，其过程如下： 上游系统在完成任务后，向消息中间件同步地发送一条消息，确保消息中间件成功持久化这条消息，然后上游系统可以去做别的事情了； 消息中间件收到消息后负责将该消息同步投递给相应的下游系统，并触发下游系统的任务执行； 当下游系统处理成功后，向消息中间件反馈确认应答，消息中间件便可以将该条消息删除，从而该事务完成。 上面是一个理想化的过程，但在实际场景中，往往会出现如下几种意外情况： 消息中间件向下游系统投递消息失败 上游系统向消息中间件发送消息失败 对于第一种情况，消息中间件具有重试机制，我们可以在消息中间件中设置消息的重试次数和重试时间间隔，对于网络不稳定导致的消息投递失败的情况，往往重试几次后消息便可以成功投递，如果超过了重试的上限仍然投递失败，那么消息中间件不再投递该消息，而是记录在失败消息表中，消息中间件需要提供失败消息的查询接口，下游系统会定期查询失败消息，并将其消费，这就是所谓的“定期校对”。 如果重复投递和定期校对都不能解决问题，往往是因为下游系统出现了严重的错误，此时就需要人工干预。 对于第二种情况，需要在上游系统中建立消息重发机制。可以在上游系统建立一张本地消息表，并将 任务处理过程 和 向本地消息表中插入消息 这两个步骤放在一个本地事务中完成。如果向本地消息表插入消息失败，那么就会触发回滚，之前的任务处理结果就会被取消。如果这量步都执行成功，那么该本地事务就完成了。接下来会有一个专门的消息发送者不断地发送本地消息表中的消息，如果发送失败它会返回重试。当然，也要给消息发送者设置重试的上限，一般而言，达到重试上限仍然发送失败，那就意味着消息中间件出现严重的问题，此时也只有人工干预才能解决问题。 对于不支持事务型消息的消息中间件，如果要实现分布式事务的话，就可以采用这种方式。它能够通过重试机制+定期校对实现分布式事务，但相比于第二种方案，它达到数据一致性的周期较长，而且还需要在上游系统中实现消息重试发布机制，以确保消息成功发布给消息中间件，这无疑增加了业务系统的开发成本，使得业务系统不够纯粹，并且这些额外的业务逻辑无疑会占用业务系统的硬件资源，从而影响性能。 因此，尽量选择支持事务型消息的消息中间件来实现分布式事务，如RocketMQ。 方案5：TCC（两阶段型、补偿型）关于TCC（Try-Confirm-Cancel）的概念，最早是由Pat Helland于2007年发表的一篇名为《Life beyond Distributed Transactions:an Apostate’s Opinion》的论文提出。 TCC事务机制相比于上面介绍的XA，解决了其几个缺点: 1.解决了协调者单点，由主业务方发起并完成这个业务活动。业务活动管理器也变成多点，引入集群。 2.同步阻塞:引入超时，超时后进行补偿，并且不会锁定整个资源，将资源转换为业务逻辑形式，粒度变小。 3.数据一致性，有了补偿机制之后，由业务活动管理器控制一致性。在实际生产应用中， 网易严选自研的分布式事务中间件 DTS就是一种典型的 TCC类型事务(从单一架构到分布式交易架构，网易严选的成功实践)。 蚂蚁金服自研的SOFA DTX分布式事务分别基于两种理论实现了两种模式：基于BASE理论的TCC模式和基于ACID理论的FMT模式(蚂蚁金服黑科技：SOFA DTX分布式事务，保障亿级资金操作一致性)。 TCC即为Try Confirm Cancel，它属于补偿型分布式事务。顾名思义，TCC实现分布式事务一共有三个步骤： Try：尝试待执行的业务 这个过程并未执行业务，只是完成所有业务的一致性检查，并预留好执行所需的全部资源 Confirm：执行业务 这个过程真正开始执行业务，由于Try阶段已经完成了一致性检查，因此本过程直接执行，而不做任何检查。并且在执行的过程中，会使用到Try阶段预留的业务资源。Confirm操作满足幂等性。要求具备幂等设计，Confirm失败后需要进行重试。 Cancel：取消执行的业务 若业务执行失败，则进入Cancel阶段，它会释放所有占用的业务资源，并回滚Confirm阶段执行的操作。Cancel操作满足幂等性Cancel阶段的异常和Confirm阶段异常处理方案基本上一致。 下面以一个转账的例子来解释下TCC实现分布式事务的过程。 假设用户A用他的账户余额给用户B发一个100元的红包，并且余额系统和红包系统是两个独立的系统。 Try 创建一条转账流水，并将流水的状态设为交易中 将用户A的账户中扣除100元（预留业务资源） Try成功之后，便进入Confirm阶段 Try过程发生任何异常，均进入Cancel阶段 Confirm 向B用户的红包账户中增加100元 将流水的状态设为交易已完成 Confirm过程发生任何异常，均进入Cancel阶段 Confirm过程执行成功，则该事务结束 Cancel 将用户A的账户增加100元 将流水的状态设为交易失败 在传统事务机制中，业务逻辑的执行和事务的处理，是在不同的阶段由不同的部件来完成的：业务逻辑部分访问资源实现数据存储，其处理是由业务系统负责；事务处理部分通过协调资源管理器以实现事务管理，其处理由事务管理器来负责。二者没有太多交互的地方，所以，传统事务管理器的事务处理逻辑，仅需要着眼于事务完成（commit/rollback）阶段，而不必关注业务执行阶段。 等一等，你有没有想到一个问题？如果有一些意外的情况发生了，比如说订单服务突然挂了，然后再次重启，TCC 分布式事务框架是如何保证之前没执行完的分布式事务继续执行的呢？ 所以，TCC 事务框架都是要记录一些分布式事务的活动日志的，可以在磁盘上的日志文件里记录，也可以在数据库里记录。保存下来分布式事务运行的各个阶段和状态。 问题还没完，万一某个服务的 Cancel 或者 Confirm 逻辑执行一直失败怎么办呢？ 那也很简单，TCC 事务框架会通过活动日志记录各个服务的状态。举个例子，比如发现某个服务的 Cancel 或者 Confirm 一直没成功，会不停的重试调用它的 Cancel 或者 Confirm 逻辑，务必要它成功！ TCC全局事务必须基于RM本地事务来实现全局事务TCC服务是由Try/Confirm/Cancel业务构成的， 其Try/Confirm/Cancel业务在执行时，会访问资源管理器（Resource Manager，下文简称RM）来存取数据。这些存取操作，必须要参与RM本地事务，以使其更改的数据要么都commit，要么都rollback。 这一点不难理解，考虑一下如下场景： 假设图中的服务B没有基于RM本地事务（以RDBS为例，可通过设置auto-commit为true来模拟），那么一旦[B:Try]操作中途执行失败，TCC事务框架后续决定回滚全局事务时，该[B:Cancel]则需要判断[B:Try]中哪些操作已经写到DB、哪些操作还没有写到DB：假设[B:Try]业务有5个写库操作，[B:Cancel]业务则需要逐个判断这5个操作是否生效，并将生效的操作执行反向操作。 不幸的是，由于[B:Cancel]业务也有n（0&lt;=n&lt;=5）个反向的写库操作，此时一旦[B:Cancel]也中途出错，则后续的[B:Cancel]执行任务更加繁重。因为，相比第一次[B:Cancel]操作，后续的[B:Cancel]操作还需要判断先前的[B:Cancel]操作的n（0&lt;=n&lt;=5）个写库中哪几个已经执行、哪几个还没有执行，这就涉及到了幂等性问题。而对幂等性的保障，又很可能还需要涉及额外的写库操作，该写库操作又会因为没有RM本地事务的支持而存在类似问题。。。可想而知，如果不基于RM本地事务，TCC事务框架是无法有效的管理TCC全局事务的。 反之，基于RM本地事务的TCC事务，这种情况则会很容易处理：[B:Try]操作中途执行失败，TCC事务框架将其参与RM本地事务直接rollback即可。后续TCC事务框架决定回滚全局事务时，在知道“[B:Try]操作涉及的RM本地事务已经rollback”的情况下，根本无需执行[B:Cancel]操作。 换句话说，基于RM本地事务实现TCC事务框架时，一个TCC型服务的cancel业务要么执行，要么不执行，不需要考虑部分执行的情况。 TCC事务框架应该提供Confirm/Cancel服务的幂等性保障一般认为，服务的幂等性，是指针对同一个服务的多次(n&gt;1)请求和对它的单次(n=1)请求，二者具有相同的副作用。 在TCC事务模型中，Confirm/Cancel业务可能会被重复调用，其原因很多。比如，全局事务在提交/回滚时会调用各TCC服务的Confirm/Cancel业务逻辑。执行这些Confirm/Cancel业务时，可能会出现如网络中断的故障而使得全局事务不能完成。因此，故障恢复机制后续仍然会重新提交/回滚这些未完成的全局事务，这样就会再次调用参与该全局事务的各TCC服务的Confirm/Cancel业务逻辑。 既然Confirm/Cancel业务可能会被多次调用，就需要保障其幂等性。 那么，应该由TCC事务框架来提供幂等性保障？还是应该由业务系统自行来保障幂等性呢？ 个人认为，应该是由TCC事务框架来提供幂等性保障。如果仅仅只是极个别服务存在这个问题的话，那么由业务系统来负责也是可以的；然而，这是一类公共问题，毫无疑问，所有TCC服务的Confirm/Cancel业务存在幂等性问题。TCC服务的公共问题应该由TCC事务框架来解决；而且，考虑一下由业务系统来负责幂等性需要考虑的问题，就会发现，这无疑增大了业务系统的复杂度。 参考文献 Life beyond Distributed Transactions: an Apostate’s Opinion 关于如何实现一个TCC分布式事务框架的一点思考 How can a requestor ensure a consistent outcome across multiple, independent providers 关于分布式事务、两阶段提交协议、三阶提交协议 Three-phase commit protocol 蚂蚁金服黑科技：SOFA DTX分布式事务，保障亿级资金操作一致性]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>分布式事务</category>
      </categories>
      <tags>
        <tag>分布式事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式锁]]></title>
    <url>%2F2019%2F08%2F28%2Fdistributed-lock%2F</url>
    <content type="text"><![CDATA[1.背景对于锁大家肯定不会陌生，在Java中synchronized关键字和ReentrantLock可重入锁在我们的代码中是经常见的，一般我们用其在多线程环境中控制对资源的并发访问，但是随着分布式的快速发展，本地的加锁往往不能满足我们的需要，在我们的分布式环境中上面加锁的方法就会失去作用。于是人们为了在分布式环境中也能实现本地锁的效果，也是纷纷各出其招，今天让我们来聊一聊一般分布式锁实现的套路。 2.分布式锁2.1为何需要分布式锁Martin Kleppmann是英国剑桥大学的分布式系统的研究员，之前和Redis之父Antirez进行过关于RedLock(红锁，后续有讲到)是否安全的激烈讨论。Martin认为一般我们使用分布式锁有两个场景: 效率:使用分布式锁可以避免不同节点重复相同的工作，这些工作会浪费资源。比如用户付了钱之后有可能不同节点会发出多封短信。 正确性:加分布式锁同样可以避免破坏正确性的发生，如果两个节点在同一条数据上面操作，比如多个节点机器对同一个订单操作不同的流程有可能会导致该笔订单最后状态出现错误，造成损失。 2.2分布式锁的一些特点当我们确定了在不同节点上需要分布式锁，那么我们需要了解分布式锁到底应该有哪些特点: 互斥性:和我们本地锁一样互斥性是最基本，但是分布式锁需要保证在不同节点的不同线程的互斥。 可重入性:同一个节点上的同一个线程如果获取了锁之后那么也可以再次获取这个锁。 锁超时:和本地锁一样支持锁超时，防止死锁。 高效，高可用:加锁和解锁需要高效，同时也需要保证高可用防止分布式锁失效，可以增加降级。 支持阻塞和非阻塞:和ReentrantLock一样支持lock和trylock以及tryLock(long timeOut)。 支持公平锁和非公平锁(可选):公平锁的意思是按照请求加锁的顺序获得锁，非公平锁就相反是无序的。这个一般来说实现的比较少。 2.3常见的分布式锁我们了解了一些特点之后，我们一般实现分布式锁有以下几个方式: MySql Zk Redis 自研分布式锁:如谷歌的Chubby。 下面分开介绍一下这些分布式锁的实现原理。 3.Mysql分布式锁首先来说一下Mysql分布式锁的实现原理，相对来说这个比较容易理解，毕竟数据库和我们开发人员在平时的开发中息息相关。对于分布式锁我们可以创建一个锁表: 前面我们所说的lock(),trylock(long timeout)，trylock()这几个方法可以用下面的伪代码实现。 3.1 lock()lock一般是阻塞式的获取锁，意思就是不获取到锁誓不罢休，那么我们可以写一个死循环来执行其操作: mysqlLock.lcok内部是一个sql,为了达到可重入锁的效果那么我们应该先进行查询，如果有值，那么需要比较node_info是否一致，这里的node_info可以用机器IP和线程名字来表示，如果一致那么就加可重入锁count的值，如果不一致那么就返回false。如果没有值那么直接插入一条数据。伪代码如下: 需要注意的是这一段代码需要加事务，必须要保证这一系列操作的原子性。 3.2 tryLock()和tryLock(long timeout)tryLock()是非阻塞获取锁，如果获取不到那么就会马上返回，代码可以如下: tryLock(long timeout)实现如下: mysqlLock.lock和上面一样，但是要注意的是select … for update这个是阻塞的获取行锁，如果同一个资源并发量较大还是有可能会退化成阻塞的获取锁。 3.3 unlock()unlock的话如果这里的count为1那么可以删除，如果大于1那么需要减去1。 3.4 锁超时我们有可能会遇到我们的机器节点挂了，那么这个锁就不会得到释放，我们可以启动一个定时任务，通过计算一般我们处理任务的一般的时间，比如是5ms，那么我们可以稍微扩大一点，当这个锁超过20ms没有被释放我们就可以认定是节点挂了然后将其直接释放。 3.5 Mysql小结 适用场景: Mysql分布式锁一般适用于资源不存在数据库，如果数据库存在比如订单，那么可以直接对这条数据加行锁，不需要我们上面多的繁琐的步骤，比如一个订单，那么我们可以用select * from order_table where id = ‘xxx’ for update进行加行锁，那么其他的事务就不能对其进行修改。 优点:理解起来简单，不需要维护额外的第三方中间件(比如Redis,Zk)。 缺点:虽然容易理解但是实现起来较为繁琐，需要自己考虑锁超时，加事务等等。性能局限于数据库，一般对比缓存来说性能较低。对于高并发的场景并不是很适合。 3.6 乐观锁前面我们介绍的都是悲观锁，这里想额外提一下乐观锁，在我们实际项目中也是经常实现乐观锁，因为我们加行锁的性能消耗比较大，通常我们会对于一些竞争不是那么激烈，但是其又需要保证我们并发的顺序执行使用乐观锁进行处理，我们可以对我们的表加一个版本号字段，那么我们查询出来一个版本号之后，update或者delete的时候需要依赖我们查询出来的版本号，判断当前数据库和查询出来的版本号是否相等，如果相等那么就可以执行，如果不等那么就不能执行。这样的一个策略很像我们的CAS(Compare And Swap),比较并交换是一个原子操作。这样我们就能避免加select * for update行锁的开销。 4. ZooKeeperZooKeeper也是我们常见的实现分布式锁方法，相比于数据库如果没了解过ZooKeeper可能上手比较难一些。ZooKeeper是以Paxos算法为基础分布式应用程序协调服务。Zk的数据节点和文件目录类似，所以我们可以用此特性实现分布式锁。我们以某个资源为目录，然后这个目录下面的节点就是我们需要获取锁的客户端，未获取到锁的客户端注册需要注册Watcher到上一个客户端，可以用下图表示。 /lock是我们用于加锁的目录,/resource_name是我们锁定的资源，其下面的节点按照我们加锁的顺序排列。 4.1 CuratorCurator封装了Zookeeper底层的Api，使我们更加容易方便的对Zookeeper进行操作，并且它封装了分布式锁的功能，这样我们就不需要再自己实现了。 Curator实现了可重入锁(InterProcessMutex),也实现了不可重入锁(InterProcessSemaphoreMutex)。在可重入锁中还实现了读写锁。 4.2 InterProcessMutexInterProcessMutex是Curator实现的可重入锁，我们可以通过下面的一段代码实现我们的可重入锁: 我们利用acuire进行加锁，release进行解锁。 加锁的流程具体如下: 首先进行可重入的判定:这里的可重入锁记录在ConcurrentMap&lt;Thread, LockData&gt; threadData这个Map里面，如果threadData.get(currentThread)是有值的那么就证明是可重入锁，然后记录就会加1。我们之前的Mysql其实也可以通过这种方法去优化，可以不需要count字段的值，将这个维护在本地可以提高性能。 然后在我们的资源目录下创建一个节点:比如这里创建一个/0000000002这个节点，这个节点需要设置为EPHEMERAL_SEQUENTIAL也就是临时节点并且有序。 获取当前目录下所有子节点，判断自己的节点是否位于子节点第一个。 如果是第一个，则获取到锁，那么可以返回。 如果不是第一个，则证明前面已经有人获取到锁了，那么需要获取自己节点的前一个节点。/0000000002的前一个节点是/0000000001，我们获取到这个节点之后，再上面注册Watcher(这里的watcher其实调用的是object.notifyAll(),用来解除阻塞)。 object.wait(timeout)或object.wait():进行阻塞等待这里和我们第5步的watcher相对应。 解锁的具体流程: 首先进行可重入锁的判定:如果有可重入锁只需要次数减1即可，减1之后加锁次数为0的话继续下面步骤，不为0直接返回。 删除当前节点。 删除threadDataMap里面的可重入锁的数据。 4.3 读写锁Curator提供了读写锁，其实现类是InterProcessReadWriteLock，这里的每个节点都会加上前缀： 12private static final String READ_LOCK_NAME = "__READ__";private static final String WRITE_LOCK_NAME = "__WRIT__"; 根据不同的前缀区分是读锁还是写锁，对于读锁，如果发现前面有写锁，那么需要将watcher注册到和自己最近的写锁。写锁的逻辑和我们之前4.2分析的依然保持不变。 4.4 锁超时Zookeeper不需要配置锁超时，由于我们设置节点是临时节点，我们的每个机器维护着一个ZK的session，通过这个session，ZK可以判断机器是否宕机。如果我们的机器挂掉的话，那么这个临时节点对应的就会被删除，所以我们不需要关心锁超时。 4.5 ZK小结 优点:ZK可以不需要关心锁超时时间，实现起来有现成的第三方包，比较方便，并且支持读写锁，ZK获取锁会按照加锁的顺序，所以其是公平锁。对于高可用利用ZK集群进行保证。 缺点:ZK需要额外维护，增加维护成本，性能和Mysql相差不大，依然比较差。并且需要开发人员了解ZK是什么。 5.Redis大家在网上搜索分布式锁，恐怕最多的实现就是Redis了，Redis因为其性能好，实现起来简单所以让很多人都对其十分青睐。 5.1 Redis分布式锁简单实现熟悉Redis的同学那么肯定对setNx(set if not exist)方法不陌生，如果不存在则更新，其可以很好的用来实现我们的分布式锁。对于某个资源加锁我们只需要 1setNx resourceName value 这里有个问题，加锁了之后如果机器宕机那么这个锁就不会得到释放所以会加入过期时间，加入过期时间需要和setNx同一个原子操作，在Redis2.8之前我们需要使用Lua脚本达到我们的目的，但是redis2.8之后redis支持nx和ex操作是同一原子操作。 1set resourceName value ex 5 nx 5.2 RedissonJavaer都知道Jedis，Jedis是Redis的Java实现的客户端，其API提供了比较全面的Redis命令的支持。Redisson也是Redis的客户端，相比于Jedis功能简单。Jedis简单使用阻塞的I/O和redis交互，Redisson通过Netty支持非阻塞I/O。Jedis最新版本2.9.0是2016年的快3年了没有更新，而Redisson最新版本是2018.10月更新。 Redisson封装了锁的实现，其继承了java.util.concurrent.locks.Lock的接口，让我们像操作我们的本地Lock一样去操作Redisson的Lock，下面介绍一下其如何实现分布式锁。 Redisson不仅提供了Java自带的一些方法(lock,tryLock)，还提供了异步加锁，对于异步编程更加方便。 由于内部源码较多，就不贴源码了，这里用文字叙述来分析他是如何加锁的，这里分析一下tryLock方法: 尝试加锁:首先会尝试进行加锁，由于保证操作是原子性，那么就只能使用lua脚本，相关的lua脚本如下： 可以看见他并没有使用我们的sexNx来进行操作，而是使用的hash结构，我们的每一个需要锁定的资源都可以看做是一个HashMap，锁定资源的节点信息是Key,锁定次数是value。通过这种方式可以很好的实现可重入的效果，只需要对value进行加1操作，就能进行可重入锁。当然这里也可以用之前我们说的本地计数进行优化。 如果尝试加锁失败，判断是否超时，如果超时则返回false。 如果加锁失败之后，没有超时，那么需要在名字为redisson_lock__channel+lockName的channel上进行订阅，用于订阅解锁消息，然后一直阻塞直到超时，或者有解锁消息。 重试步骤1，2，3，直到最后获取到锁，或者某一步获取锁超时。 对于我们的unlock方法比较简单也是通过lua脚本进行解锁，如果是可重入锁，只是减1。如果是非加锁线程解锁，那么解锁失败。 Redisson还有公平锁的实现，对于公平锁其利用了list结构和hashset结构分别用来保存我们排队的节点，和我们节点的过期时间，用这两个数据结构帮助我们实现公平锁，这里就不展开介绍了，有兴趣可以参考源码。 上述 Redis 分布式锁的缺点上面那种方案最大的问题，就是如果你对某个 Redis Master 实例，写入了 myLock 这种锁 Key 的 Value，此时会异步复制给对应的 Master Slave 实例。 但是这个过程中一旦发生 Redis Master 宕机，主备切换，Redis Slave 变为了 Redis Master。 接着就会导致，客户端 2 来尝试加锁的时候，在新的 Redis Master 上完成了加锁，而客户端 1 也以为自己成功加了锁。 此时就会导致多个客户端对一个分布式锁完成了加锁。这时系统在业务语义上一定会出现问题，导致各种脏数据的产生。 所以这个就是 Redis Cluster，或者是 redis master-slave 架构的主从异步复制导致的 Redis 分布式锁的最大缺陷：在 Redis Master 实例宕机的时候，可能导致多个客户端同时完成加锁。 5.3 RedLock我们想象一个这样的场景当机器A申请到一把锁之后，如果Redis主宕机了，这个时候从机并没有同步到这一把锁，那么机器B再次申请的时候就会再次申请到这把锁，为了解决这个问题Redis作者提出了RedLock红锁的算法,在Redisson中也对RedLock进行了实现。 通过上面的代码，我们需要实现多个Redis集群，然后进行红锁的加锁，解锁。具体的步骤如下: 首先生成多个Redis集群的Rlock，并将其构造成RedLock。 依次循环对三个集群进行加锁，加锁的过程和5.2里面一致。 如果循环加锁的过程中加锁失败，那么需要判断加锁失败的次数是否超出了最大值，这里的最大值是根据集群的个数，比如三个那么只允许失败一个，五个的话只允许失败两个，要保证多数成功。 加锁的过程中需要判断是否加锁超时，有可能我们设置加锁只能用3ms，第一个集群加锁已经消耗了3ms了。那么也算加锁失败。 3，4步里面加锁失败的话，那么就会进行解锁操作，解锁会对所有的集群在请求一次解锁。 可以看见RedLock基本原理是利用多个Redis集群，用多数的集群加锁成功，减少Redis某个集群出故障，造成分布式锁出现问题的概率。 5.4 Redis小结 优点:对于Redis实现简单，性能对比ZK和Mysql较好。如果不需要特别复杂的要求，那么自己就可以利用setNx进行实现，如果自己需要复杂的需求的话那么可以利用或者借鉴Redisson。对于一些要求比较严格的场景来说的话可以使用RedLock。 缺点:需要维护Redis集群，如果要实现RedLock那么需要维护更多的集群。 6.分布式锁的安全问题上面我们介绍过红锁，但是Martin Kleppmann认为其依然不安全。有关于Martin反驳的几点，我认为其实不仅仅局限于RedLock,前面说的算法基本都有这个问题，下面我们来讨论一下这些问题: 长时间的GC pause:熟悉Java的同学肯定对GC不陌生，在GC的时候会发生STW(stop-the-world),例如CMS垃圾回收器，他会有两个阶段进行STW防止引用继续进行变化。那么有可能会出现下面图(引用至Martin反驳Redlock的文章)中这个情况： client1获取了锁并且设置了锁的超时时间，但是client1之后出现了STW，这个STW时间比较长，导致分布式锁进行了释放，client2获取到了锁，这个时候client1恢复了锁，那么就会出现client1，2同时获取到锁，这个时候分布式锁不安全问题就出现了。这个其实不仅仅局限于RedLock,对于我们的ZK,Mysql一样的有同样的问题。 时钟发生跳跃:对于Redis服务器如果其时间发生了向跳跃，那么肯定会影响我们锁的过期时间，那么我们的锁过期时间就不是我们预期的了，也会出现client1和client2获取到同一把锁，那么也会出现不安全，这个对于Mysql也会出现。但是ZK由于没有设置过期时间，那么发生跳跃也不会受影响。 长时间的网络I/O:这个问题和我们的GC的STW很像，也就是我们这个获取了锁之后我们进行网络调用，其调用时间由可能比我们锁的过期时间都还长，那么也会出现不安全的问题，这个Mysql也会有，ZK也不会出现这个问题。 对于这三个问题，在网上包括Redis作者在内发起了很多讨论。 6.1 GC的STW对于这个问题可以看见基本所有的都会出现问题，Martin给出了一个解法，对于ZK这种他会生成一个自增的序列，那么我们真正进行对资源操作的时候，需要判断当前序列是否是最新，有点类似于我们乐观锁。当然这个解法Redis作者进行了反驳，你既然都能生成一个自增的序列了那么你完全不需要加锁了，也就是可以按照类似于Mysql乐观锁的解法去做。 我自己认为这种解法增加了复杂性，当我们对资源操作的时候需要增加判断序列号是否是最新，无论用什么判断方法都会增加复杂度，后面会介绍谷歌的Chubby提出了一个更好的方案。 6.2 时钟发生跳跃Martin觉得RedLock不安全很大的原因也是因为时钟的跳跃，因为锁过期强依赖于时间，但是ZK不需要依赖时间，依赖每个节点的Session。Redis作者也给出了解答:对于时间跳跃分为人为调整和NTP自动调整。 人为调整:人为调整影响的那么完全可以人为不调整，这个是处于可控的。 NTP自动调整:这个可以通过一定的优化，把跳跃时间控制的可控范围内，虽然会跳跃，但是是完全可以接受的。 6.3 长时间的网络I/O这一块不是他们讨论的重点，我自己觉得，对于这个问题的优化可以控制网络调用的超时时间，把所有网络调用的超时时间相加，那么我们锁过期时间其实应该大于这个时间，当然也可以通过优化网络调用比如串行改成并行，异步化等。可以参考我的两个文章: 并行化-你的高并发大杀器，异步化-你的高并发大杀器 7. Chubby的一些优化大家搜索ZK的时候，会发现他们都写了ZK是Chubby的开源实现，Chubby内部工作原理和ZK类似。但是Chubby的定位是分布式锁和ZK有点不同。Chubby也是使用上面自增序列的方案用来解决分布式不安全的问题，但是他提供了多种校验方法: CheckSequencer()：调用Chubby的API检查此时这个序列号是否有效。 访问资源服务器检查，判断当前资源服务器最新的序列号和我们的序列号的大小。 lock-delay:为了防止我们校验的逻辑入侵我们的资源服务器，其提供了一种方法当客户端失联的时候，并不会立即释放锁，而是在一定的时间内(默认1min)阻止其他客户端拿去这个锁，那么也就是给予了一定的buffer等待STW恢复，而我们的GC的STW时间如果比1min还长那么你应该检查你的程序，而不是怀疑你的分布式锁了。 8. 每秒上千订单场景下的分布式锁高并发优化实践接着就给大家聊一个有意思的话题：每秒上千订单场景下，如何对分布式锁的并发能力进行优化? 首先，我们一起来看看这个问题的背景?前段时间有个朋友在外面面试，然后有一天找我聊说：有一个国内不错的电商公司，面试官给他出了一个场景题： 假如下单时，用分布式锁来防止库存超卖，但是是每秒上千订单的高并发场景，如何对分布式锁进行高并发优化来应对这个场景? 他说他当时没答上来，因为没做过没什么思路。其实我当时听到这个面试题心里也觉得有点意思，因为如果是我来面试候选人的话，应该会给的范围更大一些。 比如，让面试的同学聊一聊电商高并发秒杀场景下的库存超卖解决方案，各种方案的优缺点以及实践，进而聊到分布式锁这个话题。 因为库存超卖问题是有很多种技术解决方案的，比如悲观锁，分布式锁，乐观锁，队列串行化，Redis 原子操作，等等吧。 但是既然那个面试官兄弟限定死了用分布式锁来解决库存超卖，我估计就是想问一个点：在高并发场景下如何优化分布式锁的并发性能。 我觉得，面试官提问的角度还是可以接受的，因为在实际落地生产的时候，分布式锁这个东西保证了数据的准确性，但是他天然并发能力有点弱。 刚好我之前在自己项目的其他场景下，确实是做过高并发场景下的分布式锁优化方案，因此正好是借着这个朋友的面试题，把分布式锁的高并发优化思路，给大家来聊一聊。 8.1 库存超卖现象是怎么产生的?先来看看如果不用分布式锁，所谓的电商库存超卖是啥意思?大家看看下面的图： 这个图，其实很清晰了，假设订单系统部署在两台机器上，不同的用户都要同时买 10 台 iPhone，分别发了一个请求给订单系统。 接着每个订单系统实例都去数据库里查了一下，当前 iPhone 库存是 12 台。俩大兄弟一看，乐了，12 台库存大于了要买的 10 台数量啊! 于是乎，每个订单系统实例都发送 SQL 到数据库里下单，然后扣减了 10 个库存，其中一个将库存从 12 台扣减为 2 台，另外一个将库存从 2 台扣减为 -8 台。 现在完了，库存出现了负数!泪奔啊，没有 20 台 iPhone 发给两个用户啊!这可如何是好。 8.2 用分布式锁如何解决库存超卖问题?我们用分布式锁如何解决库存超卖问题呢?其实很简单，回忆一下上次我们说的那个分布式锁的实现原理： 同一个锁 Key，同一时间只能有一个客户端拿到锁，其他客户端会陷入无限的等待来尝试获取那个锁，只有获取到锁的客户端才能执行下面的业务逻辑。 代码大概就是上面那个样子，现在我们来分析一下，为啥这样做可以避免库存超卖? 大家可以顺着上面的那个步骤序号看一遍，马上就明白了。 从上图可以看到，只有一个订单系统实例可以成功加分布式锁，然后只有他一个实例可以查库存、判断库存是否充足、下单扣减库存，接着释放锁。 释放锁之后，另外一个订单系统实例才能加锁，接着查库存，一下发现库存只有 2 台了，库存不足，无法购买，下单失败。不会将库存扣减为 -8 的。 8.3 有没其他方案解决库存超卖问题?当然有啊!比如悲观锁，分布式锁，乐观锁，队列串行化，异步队列分散，Redis 原子操作，等等，很多方案，我们对库存超卖有自己的一整套优化机制。 但是前面说过了，这篇文章就聊一个分布式锁的并发优化，不是聊库存超卖的解决方案，所以库存超卖只是一个业务场景而已。 8.4 分布式锁的方案在高并发场景下好，现在我们来看看，分布式锁的方案在高并发场景下有什么问题? 问题很大啊!兄弟，不知道你看出来了没有。分布式锁一旦加了之后，对同一个商品的下单请求，会导致所有客户端都必须对同一个商品的库存锁 Key 进行加锁。 比如，对 iPhone 这个商品的下单，都必对“iphone_stock”这个锁 Key 来加锁。这样会导致对同一个商品的下单请求，就必须串行化，一个接一个的处理。 大家再回去对照上面的图反复看一下，应该能想明白这个问题。 假设加锁之后，释放锁之前，查库存→创建订单→扣减库存，这个过程性能很高吧，算他全过程 20 毫秒，这应该不错了。 那么 1 秒是 1000 毫秒，只能容纳 50 个对这个商品的请求依次串行完成处理。 比如一秒钟来 50 个请求，都是对 iPhone 下单的，那么每个请求处理 20 毫秒，一个一个来，最后 1000 毫秒正好处理完 50 个请求。 大家看一眼下面的图，加深一下感觉。 所以看到这里，大家起码也明白了，简单的使用分布式锁来处理库存超卖问题，存在什么缺陷。 缺陷就是同一个商品多用户同时下单的时候，会基于分布式锁串行化处理，导致没法同时处理同一个商品的大量下单的请求。 这种方案，要是应对那种低并发、无秒杀场景的普通小电商系统，可能还可以接受。 因为如果并发量很低，每秒就不到 10 个请求，没有瞬时高并发秒杀单个商品的场景的话，其实也很少会对同一个商品在 1 秒内瞬间下 1000 个订单，因为小电商系统没那场景。 8.5 如何对分布式锁进行高并发优化?好了，终于引入正题了，那么现在怎么办呢? 面试官说，我现在就卡死，库存超卖就是用分布式锁来解决，而且一秒对一个 iPhone 下上千订单，怎么优化? 现在按照刚才的计算，你 1 秒钟只能处理针对 iPhone 的 50 个订单。其实说出来也很简单，相信很多人看过 Java 里的 ConcurrentHashMap 的源码和底层原理，应该知道里面的核心思路，就是分段加锁! 把数据分成很多个段，每个段是一个单独的锁，所以多个线程过来并发修改数据的时候，可以并发的修改不同段的数据。不至于说，同一时间只能有一个线程独占修改 ConcurrentHashMap 中的数据。 另外，Java 8 中新增了一个 LongAdder 类，也是针对 Java 7 以前的 AtomicLong 进行的优化，解决的是 CAS 类操作在高并发场景下，使用乐观锁思路，会导致大量线程长时间重复循环。 LongAdder 中也是采用了类似的分段 CAS 操作，失败则自动迁移到下一个分段进行 CAS 的思路。 其实分布式锁的优化思路也是类似的，之前我们是在另外一个业务场景下落地了这个方案到生产中，不是在库存超卖问题里用的。 但是库存超卖这个业务场景不错，很容易理解，所以我们就用这个场景来说一下。 大家看看下面的图： 这就是分段加锁。假如你现在 iPhone 有 1000 个库存，那么你完全可以给拆成 20 个库存段。 要是你愿意，可以在数据库的表里建 20 个库存字段，比如 stock_01，stock_02，类似这样的，也可以在 Redis 之类的地方放 20 个库存 Key。 总之，就是把你的 1000 件库存给他拆开，每个库存段是 50 件库存，比如 stock_01 对应 50 件库存，stock_02 对应 50 件库存。 接着，每秒 1000 个请求过来了，好!此时其实可以是自己写一个简单的随机算法，每个请求都是随机在 20 个分段库存里，选择一个进行加锁。 Bingo!这样就好了，同时可以有最多 20 个下单请求一起执行，每个下单请求锁了一个库存分段，然后在业务逻辑里面，就对数据库或者是 Redis 中的那个分段库存进行操作即可，包括查库存→判断库存是否充足→扣减库存。 这相当于什么呢?相当于一个 20 毫秒，可以并发处理掉 20 个下单请求，那么 1 秒，也就可以依次处理掉 20 * 50 = 1000 个对 iPhone 的下单请求了。 一旦对某个数据做了分段处理之后，有一个坑大家一定要注意：就是如果某个下单请求，咔嚓加锁，然后发现这个分段库存里的库存不足了，此时咋办? 这时你得自动释放锁，然后立马换下一个分段库存，再次尝试加锁后尝试处理。这个过程一定要实现。 8.6 分布式锁并发优化方案有什么不足?不足肯定是有的，最大的不足，很不方便，实现太复杂了： 首先，你得对一个数据分段存储，一个库存字段本来好好的，现在要分为 20 个库存字段。 其次，你在每次处理库存的时候，还得自己写随机算法，随机挑选一个分段来处理。 最后，如果某个分段中的数据不足了，你还得自动切换到下一个分段数据去处理。 这个过程都是要手动写代码实现的，还是有点工作量，挺麻烦的。 不过我们确实在一些业务场景里，因为用到了分布式锁，然后又必须要进行锁并发的优化，又进一步用到了分段加锁的技术方案，效果当然是很好的了，一下子并发性能可以增长几十倍。 该优化方案的后续改进：以我们本文所说的库存超卖场景为例，你要是这么玩，会把自己搞的很痛苦!再次强调，我们这里的库存超卖场景，仅仅只是作为演示场景而已。 参考资料 再有人问你分布式锁，这篇文章扔给他: https://juejin.im/post/5bbb0d8df265da0abd3533a5#heading-5]]></content>
      <categories>
        <category>并发控制</category>
        <category>分布式锁</category>
      </categories>
      <tags>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[乐观锁]]></title>
    <url>%2F2019%2F08%2F28%2Foptimistic-lock%2F</url>
    <content type="text"><![CDATA[简介乐观锁通俗的理解就是，数据有一个版本号，第一次读的时候将获取数据的版本号；当需要对数据进行更新时，需要检查数据库中的版本号与第一次获取的版本号是否一致。如果一致则更新数据，否则不更新。也就是说，要保证数据在中间没被修改过。乐观锁一般来说有以下2种方式： 使用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式。何谓数据版本？即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 “version” 字段来实现。当读取数据时，将version字段的值一同读出，数据每更新一次，对此version值加一。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比对，如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据。 使用时间戳（timestamp）。乐观锁定的第二种实现方式和第一种差不多，同样是在需要乐观锁控制的table中增加一个字段，名称无所谓，字段类型使用时间戳（timestamp）, 和上面的version类似，也是在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果一致则OK，否则就是版本冲突。 Java JUC中的atomic包就是乐观锁的一种实现，AtomicInteger 通过CAS（Compare And Set）操作实现线程安全的自增。 实例牙膏库存牙膏库存为 100 件，用户 A （线程 A ）和用户 B （线程 B ）同时购买一件牙膏。 线程 A 读取牙膏库存（ 100 件，版本号为 1 ，不加锁），线程 B 读取牙膏库存（ 100 件，版本号为 1 ，不加锁）； 线程 A 将库存减 1 ， 100-1=99 件；线程 B 将库存减 1 ， 100-1=99 件（注意还没更新到牙膏库存）； 因为线程 A 、线程 B 总有一个线程必先执行，假设是线程 A 。线程 A 将库存减 1 后（库存为 100-1=99 件），通过本地版本号与数据库版本号比对（都为 1 ），因为版本号一致，所以更新牙膏库存（牙膏库存变为 99 件，版本号变为 2 ），线程 A 执行完毕； 线程 B 将库存减 1 后（库存为 100-1=99 件），通过本地版本号（为 1 ）与数据库版本号（为 2 ）比对，因为版本号不一致，所以不更新牙膏库存，线程 B 重新读取牙膏库存（库存为 99 ，版本号为 2 ），重复执行。 线程 B 将库存减 1 后（库存为 99-1=98 件），通过本地版本号（为 2 ）与数据库版本号（为 2 ）对比，因为版本号一致，所以更新牙膏库存（牙膏库存变为 98 件，版本号变为 3 ），线程 B 执行完毕。 电商下单考虑电商系统中的下单流程，商品的库存量是固定的，如何保证商品数量不超卖？ 其实需要保证数据一致性：某个人点击秒杀后系统中查出来的库存量和实际扣减库存时库存量的一致性就可以。 假设，MySQL数据库中商品库存表tb_product_stock 结构定义如下： 123456789CREATE TABLE `tb_product_stock` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '自增ID', `product_id` bigint(32) NOT NULL COMMENT '商品ID', `number` INT(8) NOT NULL DEFAULT 0 COMMENT '库存数量', `create_time` DATETIME NOT NULL COMMENT '创建时间', `modify_time` DATETIME NOT NULL COMMENT '更新时间', PRIMARY KEY (`id`), UNIQUE KEY `index_pid` (`product_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='商品库存表'; 对应的POJO类： 1234567891011121314151617181920class ProductStock &#123; private Long productId; //商品id private Integer number; //库存量 public Long getProductId() &#123; return productId; &#125; public void setProductId(Long productId) &#123; this.productId = productId; &#125; public Integer getNumber() &#123; return number; &#125; public void setNumber(Integer number) &#123; this.number = number; &#125;&#125; 悲观锁12345678910111213141516/** * 更新库存(使用悲观锁) * @param productId * @return */ public boolean updateStock(Long productId)&#123; //先锁定商品库存记录 ProductStock product = query("SELECT * FROM tb_product_stock WHERE product_id=#&#123;productId&#125; FOR UPDATE", productId); if (product.getNumber() &gt; 0) &#123; int updateCnt = update("UPDATE tb_product_stock SET number=number-1 WHERE product_id=#&#123;productId&#125;", productId); if(updateCnt &gt; 0)&#123; //更新库存成功 return true; &#125; &#125; return false; &#125; 乐观锁1234567891011121314151617181920/** * 下单减库存 * @param productId * @return */public boolean updateStock(Long productId)&#123; int updateCnt = 0; while (updateCnt == 0) &#123; ProductStock product = query("SELECT * FROM tb_product_stock WHERE product_id=#&#123;productId&#125;", productId); if (product.getNumber() &gt; 0) &#123; updateCnt = update("UPDATE tb_product_stock SET number=number-1 WHERE product_id=#&#123;productId&#125; AND number=#&#123;number&#125;", productId, product.getNumber()); if(updateCnt &gt; 0)&#123; //更新库存成功 return true; &#125; &#125; else &#123; //卖完啦 return false; &#125; &#125; return false;&#125; 使用乐观锁更新库存的时候不加锁，当提交更新时需要判断数据是否已经被修改（AND number=#{number}），只有在 number等于上一次查询到的number时 才提交更新。 注意：UPDATE 语句的WHERE 条件字句上需要建索引 乐观锁与悲观锁的区别乐观锁的思路一般是表中增加版本字段，更新时where语句中增加版本的判断，算是一种CAS（Compare And Swep）操作，商品库存场景中number起到了版本控制（相当于version）的作用（ AND number=#{number}）。 悲观锁之所以是悲观，在于他认为本次操作会发生并发冲突，所以一开始就对商品加上锁（SELECT … FOR UPDATE），然后就可以安心的做判断和更新，因为这时候不会有别人更新这条商品库存。 CAS：compare and swap CAS的含义是，我认为V的值应该为A，如果是，那么将V的值更新为B，否则不修改并告诉V的值实际为多少。 附上java.util.concurrent.atomic.AtomicLong中的源码 123456789101112public final long getAndAccumulate(long x, LongBinaryOperator accumulatorFunction) &#123; long prev, next; do &#123; prev = get(); next = accumulatorFunction.applyAsLong(prev, x); &#125; while (!compareAndSet(prev, next)); return prev;&#125;public final boolean compareAndSet(long expect, long update) &#123; return unsafe.compareAndSwapLong(this, valueOffset, expect, update);&#125; 参考资料 MySQL 乐观锁与悲观锁: https://www.jianshu.com/p/f5ff017db62a]]></content>
      <categories>
        <category>并发控制</category>
        <category>乐观锁</category>
      </categories>
      <tags>
        <tag>乐观锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL探秘:InnoDB一致性非锁定读]]></title>
    <url>%2F2019%2F08%2F27%2Finnodb-consistent-nonlocking-read%2F</url>
    <content type="text"><![CDATA[一致性非锁定读一致性非锁定读(consistent nonlocking read)是指InnoDB存储引擎通过多版本控制(MVVC)读取当前数据库中行数据的方式。如果读取的行正在执行DELETE或UPDATE操作，这时读取操作不会因此去等待行上锁的释放。相反地，InnoDB会去读取行的一个快照。 上图直观地展现了InnoDB一致性非锁定读的机制。之所以称其为非锁定读，是因为不需要等待行上排他锁的释放。快照数据是指该行的之前版本的数据，每行记录可能有多个版本，一般称这种技术为行多版本技术。由此带来的并发控制，称之为多版本并发控制(Multi Version Concurrency Control, MVVC)。InnoDB是通过undo log来实现MVVC。undo log本身用来在事务中回滚数据，因此快照数据本身是没有额外开销。此外，读取快照数据是不需要上锁的，因为没有事务需要对历史的数据进行修改操作。 事务隔离级别 一致性非锁定读是InnoDB默认的读取方式，即读取不会占用和等待行上的锁。但是并不是在每个事务隔离级别下都是采用此种方式。此外，即使都是使用一致性非锁定读，但是对于快照数据的定义也各不相同。 在事务隔离级别READ COMMITTED和REPEATABLE READ下，InnoDB使用一致性非锁定读。然而，对于快照数据的定义却不同。在READ COMMITTED事务隔离级别下，一致性非锁定读总是读取被锁定行的最新一份快照数据(可能会发生在一个事务内两次读到的数据是不一样的，因此称为是不可重复读)。而在REPEATABLE READ事务隔离级别下，则读取事务开始时的行数据版本。 我们下面举个例子来详细说明一下上述的情况。 123# session Amysql&gt; BEGIN;mysql&gt; SELECT * FROM test WHERE id = 1; 我们首先在会话A中显示地开启一个事务，然后读取test表中的id为1的数据，但是事务并没有结束。于此同时，用户在开启另一个会话B，这样可以模拟并发的操作，然后对会话B做出如下的操作： 123# session Bmysql&gt; BEGIN;mysql&gt; UPDATE test SET id = 3 WHERE id = 1; 在会话B的事务中，将test表中id为1的记录修改为id=3，但是事务同样也没有提交，这样id=1的行其实加了一个排他锁。由于InnoDB在READ COMMITTED和REPEATABLE READ事务隔离级别下使用一致性非锁定读，这时如果会话A再次读取id为1的记录，仍然能够读取到相同的数据。此时，READ COMMITTED和REPEATABLE READ事务隔离级别没有任何区别。 如上图所示，当会话B提交事务后，会话A再次运行SELECT * FROM test WHERE id = 1的SQL语句时，两个事务隔离级别下得到的结果就不一样了。对于READ COMMITTED的事务隔离级别，它总是读取行的最新版本，如果行被锁定了，则读取该行版本的最新一个快照。因为会话B的事务已经提交，所以在该隔离级别下上述SQL语句的结果是最新的。对于REPEATABLE READ的事务隔离级别，总是读取事务开始时的行数据，因此，在该隔离级别下，上述SQL语句仍然会获得相同的数据。 不可重复读和幻读的区别很多人容易搞混不可重复读和幻读，确实这两者有些相似。但不可重复读重点在于update和delete，而幻读的重点在于insert。 如果使用锁机制来实现这两种隔离级别，在可重复读中，该sql第一次读取到数据后，就将这些数据加锁，其它事务无法修改这些数据，就可以实现可重复读了。但这种方法却无法锁住insert的数据，所以当事务A先前读取了数据，或者修改了全部数据，事务B还是可以insert数据提交，这时事务A就会发现莫名其妙多了一条之前没有的数据，这就是幻读，不能通过行锁来避免。需要Serializable隔离级别 ，读用读锁，写用写锁，读锁和写锁互斥，这么做可以有效的避免幻读、不可重复读、脏读等问题，但会极大的降低数据库的并发能力。 MySQL采用了gap lock，所以实际上MySQL的REPEATABLE READ隔离级别也解决了幻读的问题，也就是Mysql InnoDB在Read repeatable级别上使用next-key locking 策略来避免幻读现象的产生。详见: Innodb锁机制：Next-Key Lock 浅谈 上文说的，是使用悲观锁机制来处理这两种问题，但是MySQL、ORACLE、PostgreSQL等成熟的数据库，出于性能考虑，都是使用了以乐观锁为理论基础的MVCC（多版本并发控制）来避免这两种问题。 MVVC我们首先来看一下wiki上对MVVC的定义： Multiversion concurrency control (MCC or MVCC), is a concurrency controlmethod commonly used by database management systems to provideconcurrent access to the database and in programming languages toimplement transactional memory. 由定义可知，MVVC是用于数据库提供并发访问控制的并发控制技术。数据库的并发控制机制有很多，最为常见的就是锁机制。锁机制一般会给竞争资源加锁，阻塞读或者写操作来解决事务之间的竞争条件，最终保证事务的可串行化。而MVVC则引入了另外一种并发控制，它让读写操作互不阻塞，每一个写操作都会创建一个新版本的数据，读操作会从有限多个版本的数据中挑选一个最合适的结果直接返回，由此解决了事务的竞争条件。考虑一个现实场景。管理者要查询所有用户的存款总额，假设除了用户A和用户B之外，其他用户的存款总额都为0，A、B用户各有存款1000，所以所有用户的存款总额为2000。但是在查询过程中，用户A会向用户B进行转账操作。转账操作和查询总额操作的时序图如下图所示。 如果没有任何的并发控制机制，查询总额事务先读取了用户A的账户存款，然后转账事务改变了用户A和用户B的账户存款，最后查询总额事务继续读取了转账后的用户B的账号存款，导致最终统计的存款总额多了100元，发生错误。 使用锁机制可以解决上述的问题。查询总额事务会对读取的行加锁，等到操作结束后再释放所有行上的锁。因为用户A的存款被锁，导致转账操作被阻塞，直到查询总额事务提交并将所有锁都释放。 但是这时可能会引入新的问题，当转账操作是从用户B向用户A进行转账时会导致死锁。转账事务会先锁住用户B的数据，等待用户A数据上的锁，但是查询总额的事务却先锁住了用户A数据，等待用户B的数据上的锁。 使用MVVC机制也可以解决这个问题。查询总额事务先读取了用户A的账户存款，然后转账事务会修改用户A和用户B账户存款，查询总额事务读取用户B存款时不会读取转账事务修改后的数据，而是读取本事务开始时的数据副本(在REPEATABLE READ隔离等级下)。 MVCC使得数据库读不会对数据加锁，普通的SELECT请求不会加锁，提高了数据库的并发处理能力。借助MVCC，数据库可以实现READ COMMITTED，REPEATABLE READ等隔离级别，用户可以查看当前数据的前一个或者前几个历史版本，保证了ACID中的I特性（隔离性)。 通过MVCC机制，虽然让数据变得可重复读，但我们读到的数据可能是历史数据，是不及时的数据，不是数据库当前的数据！这在一些对于数据的时效特别敏感的业务中，就很可能出问题。对于这种读取历史数据的方式，我们叫它快照读 (snapshot read)，而读取数据库当前版本数据的方式，叫当前读 (current read)。很显然，在MVCC中： 快照读：就是select select * from table ….; 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，处理的都是当前的数据，需要加锁。 select * from table where ? lock in share mode; select * from table where ? for update; insert; update ; delete; InnoDB的MVVC实现多版本并发控制仅仅是一种技术概念，并没有统一的实现标准， 其的核心理念就是数据快照，不同的事务访问不同版本的数据快照，从而实现不同的事务隔离级别。虽然字面上是说具有多个版本的数据快照，但这并不意味着数据库必须拷贝数据，保存多份数据文件，这样会浪费大量的存储空间。InnoDB通过事务的undo日志巧妙地实现了多版本的数据快照。 数据库的事务有时需要进行回滚操作，这时就需要对之前的操作进行undo。因此，在对数据进行修改时，InnoDB会产生undo log。当事务需要进行回滚时，InnoDB可以利用这些undo log将数据回滚到修改之前的样子。 根据行为的不同 undo log 分为两种 insert undo log和update undo log。insert undo log 是在 insert 操作中产生的 undo log。因为 insert 操作的记录只对事务本身可见，对于其它事务此记录是不可见的，所以 insert undo log 可以在事务提交后直接删除而不需要进行 purge 操作。 update undo log 是 update 或 delete 操作中产生的 undo log，因为会对已经存在的记录产生影响，为了提供 MVCC机制，因此 update undo log 不能在事务提交时就进行删除，而是将事务提交时放到入 history list 上，等待 purge 线程进行最后的删除操作。 为了保证事务并发操作时，在写各自的undo log时不产生冲突，InnoDB采用回滚段的方式来维护undo log的并发写入和持久化。回滚段实际上是一种 Undo 文件组织方式。 InnoDB行记录有三个隐藏字段：分别对应该行的rowid、事务号db_trx_id和回滚指针db_roll_ptr，其中db_trx_id表示最近修改的事务的id，db_roll_ptr指向回滚段中的undo log。如下图所示。 当事务2使用UPDATE语句修改该行数据时，会首先使用排他锁锁定改行，将该行当前的值复制到undo log中，然后再真正地修改当前行的值，最后填写事务ID，使用回滚指针指向undo log中修改前的行。如下图所示。 当事务3进行修改与事务2的处理过程类似，如下图所示。 REPEATABLE READ隔离级别下事务开始后使用MVVC机制进行读取时，会将当时活动的事务id记录下来，记录到Read View中。READ COMMITTED隔离级别下则是每次读取时都创建一个新的Read View。Read View是InnoDB中用于判断记录可见性的数据结构，记录了一些用于判断可见性的属性。 low_limit_id：某行记录的db_trx_id &lt; 该值，则该行对于当前Read View是一定可见的 up_limit_id：某行记录的db_trx_id &gt;= 该值，则该行对于当前read view是一定不可见的 low_limit_no：用于purge操作的判断 rw_trx_ids：读写事务数组 Read View创建后，事务再次进行读操作时比较记录的db_trx_id和Read View中的low_limit_id，up_limit_id和读写事务数组来判断可见性。 如果该行中的db_trx_id等于当前事务id，说明是事务内部发生的更改，直接返回该行数据。否则的话，如果db_trx_id小于up_limit_id，说明是事务开始前的修改，则该记录对当前Read View是可见的，直接返回该行数据。 如果db_trx_id大于或者等于low_limit_id，则该记录对于该Read View一定是不可见的。如果db_trx_id位于[up_limit_id, low_limit_id)范围内，需要在活跃读写事务数组(rw_trx_ids)中查找db_trx_id是否存在，如果存在，记录对于当前Read View是不可见的。如果记录对于Read View不可见，需要通过记录的DB_ROLL_PTR指针遍历undo log，构造对当前Read View可见版本数据。简单来说，Read View记录读开始时及其之后，所有的活动事务，这些事务所做的修改对于Read View是不可见的。除此之外，所有其他的小于创建Read View的事务号的所有记录均可见。 参考资料 MySQL探秘(六):InnoDB一致性非锁定读: https://segmentfault.com/a/1190000017055118 Innodb锁机制：Next-Key Lock 浅谈: https://www.cnblogs.com/zhoujinyi/p/3435982.html]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper的服务器角色]]></title>
    <url>%2F2019%2F08%2F25%2Fzookeeper-server-role%2F</url>
    <content type="text"><![CDATA[LeaderLeader服务器是Zookeeper集群工作的核心，其主要工作如下 事务请求的唯一调度和处理者，保证集群事务处理的顺序性。 集群内部各服务器的调度者。 请求处理链使用责任链来处理每个客户端的请求时Zookeeper的特色，Leader服务器的请求处理链如下 PrepRequestProcessor。请求预处理器。在Zookeeper中，那些会改变服务器状态的请求称为事务请求（创建节点、更新数据、删除节点、创建会话等），PrepRequestProcessor能够识别出当前客户端请求是否是事务请求。对于事务请求，PrepRequestProcessor处理器会对其进行一系列预处理，如创建请求事务头、事务体、会话检查、ACL检查和版本检查等。 ProposalRequestProcessor。事务投票处理器。Leader服务器事务处理流程的发起者，对于非事务性请求，ProposalRequestProcessor会直接将请求转发到CommitProcessor处理器，不再做任何处理，而对于事务性请求，处理将请求转发到CommitProcessor外，还会根据请求类型创建对应的Proposal提议，并发送给所有的Follower服务器来发起一次集群内的事务投票。同时，ProposalRequestProcessor还会将事务请求交付给SyncRequestProcessor进行事务日志的记录。 SyncRequestProcessor。事务日志记录处理器。用来将事务请求记录到事务日志文件中，同时会触发Zookeeper进行数据快照。 AckRequestProcessor。负责在SyncRequestProcessor完成事务日志记录后，向Proposal的投票收集器发送ACK反馈，以通知投票收集器当前服务器已经完成了对该Proposal的事务日志记录。 CommitProcessor。事务提交处理器。对于非事务请求，该处理器会直接将其交付给下一级处理器处理；对于事务请求，其会等待集群内针对Proposal的投票直到该Proposal可被提交，利用CommitProcessor，每个服务器都可以很好地控制对事务请求的顺序处理。 ToBeCommitProcessor。该处理器有一个toBeApplied队列，用来存储那些已经被CommitProcessor处理过的可被提交的Proposal。其会将这些请求交付给FinalRequestProcessor处理器处理，待其处理完后，再将其从toBeApplied队列中移除。 FinalRequestProcessor。用来进行客户端请求返回之前的操作，包括创建客户端请求的响应，针对事务请求，该处理还会负责将事务应用到内存数据库中去。 LearnerHandler为了保证整个集群内部的实时通信，同时为了确保可以控制所有的Follower/Observer服务器，Leader服务器会与每个Follower/Observer服务器建立一个TCP长连接。同时也会为每个Follower/Observer服务器创建一个名为LearnerHandler的实体。LearnerHandler是Learner服务器的管理者，主要负责Follower/Observer服务器和Leader服务器之间的一系列网络通信，包括数据同步、请求转发和Proposal提议的投票等。Leader服务器中保存了所有Follower/Observer对应的LearnerHandler。 FollowerFollower是Zookeeper集群的跟随者，其主要工作如下 处理客户端非事务性请求（读取数据），转发事务请求给Leader服务器。 参与事务请求Proposal的投票。 参与Leader选举投票。 Follower也采用了责任链模式组装的请求处理链来处理每一个客户端请求，由于不需要对事务请求的投票处理，因此Follower的请求处理链会相对简单，其处理链如下 FollowerRequestProcessor。其用作识别当前请求是否是事务请求，若是，那么Follower就会将该请求转发给Leader服务器，Leader服务器是在接收到这个事务请求后，就会将其提交到请求处理链，按照正常事务请求进行处理。 SendAckRequestProcessor。其承担了事务日志记录反馈的角色，在完成事务日志记录后，会向Leader服务器发送ACK消息以表明自身完成了事务日志的记录工作。 ObserverObserver充当观察者角色，观察Zookeeper集群的最新状态变化并将这些状态同步过来，其对于非事务请求可以进行独立处理，对于事务请求，则会转发给Leader服务器进行处理。Observer不会参与任何形式的投票，包括事务请求Proposal的投票和Leader选举投票。其处理链如下 集群间消息通信Zookeeper的消息类型大体分为数据同步型、服务器初始化型、请求处理型和会话管理型。 数据同步型指在Learner和Leader服务器进行数据同步时，网络通信所用到的消息，通常有DIFF、TRUNC、SNAP、UPTODATE。 服务器初始化型指在整个集群或是某些新机器初始化时，Leader和Learner之间相互通信所使用的消息类型，常见的有OBSERVERINFO、FOLLOWERINFO、LEADERINFO、ACKEPOCH和NEWLEADER五种。 请求处理型指在进行清理时，Leader和Learner服务器之间互相通信所使用的消息，常见的有REQUEST、PROPOSAL、ACK、COMMIT、INFORM和SYNC六种。 会话管理型指Zookeeper在进行会话管理时和Learner服务器之间互相通信所使用的消息，常见的有PING和REVALIDATE两种。 参考资料 【分布式】Zookeeper的服务器角色: https://www.cnblogs.com/leesf456/p/6139266.html]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统CAP定理]]></title>
    <url>%2F2019%2F08%2F23%2Fdistributed-systems-cap%2F</url>
    <content type="text"><![CDATA[2000年7月，加州大学伯克利分校的Eric Brewer教授在ACM PODC会议上提出CAP猜想。2年后，麻省理工学院的Seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认定理。 CAP理论概述分布式领域中存在CAP理论： ① C：Consistency，一致性，数据一致更新，所有数据变动都是同步的。 ② A：Availability，可用性，系统具有好的响应性能。 ③ P：Partition tolerance，分区容错性。以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择，也就是说无论任何消息丢失，系统都可用。 这三个基本需求，最多只能同时满足其中的两项，在分布式系统中，因为P是必须的,因此往往选择就在CP或者AP中。理解CAP理论的最简单方式是想象两个节点分处分区两侧。允许至少一个节点更新状态会导致数据不一致，即丧失了C性质。如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了A性质。除非两个节点可以互相通信，才能既保证C又保证A，这又会导致丧失P性质。 一致性（C:Consistency）在分布式环境中，一致性指“all nodes see the same data at the same time”。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。例如一个将数据副本分布在不同分布式节点上的系统来说，如果对第一个节点的数据进行了更新操作并且更新成功后，其他节点上的数据也应该得到更新，并且所有用户都可以读取到其最新的值，那么这样的系统就被认为具有强一致性（或严格的一致性，最终一致性）。 可用性（A:Available）可用性是指系统提供的服务必须一直处于可用的状态(Reads and writes always succeed)，对于用户的每一个操作请求总是能够在有限的时间内返回结果。“有效的时间内”是指，对于用户的一个操作请求，系统必须能够在指定的时间（即响应时间）内返回对应的处理结果，如果超过了这个时间范围，那么系统就被认为是不可用的。 “返回结果”是可用性的另一个非常重要的指标，它要求系统在完成对用户请求的处理后，返回一个正常的响应结果。正常的响应结果通常能够明确的反映出对请求的处理结果，即成功或失败，而不是一个让用户感到困惑的返回结果。 分区容错性（P:Partition Tolerance）分区容错性约束了一个分布式系统需要具有如下特性：分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障(the system continues to operate despite arbitrary message loss or failure of part of the system)。 网络分区是指在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络等）中，由于一些特殊的原因导致这些子网络之间出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域。需要注意的是，组成一个分布式系统的每个节点的加入与退出都可以看作是一个特殊的网络分区。 由于一个分布式系统无法同时满足上面的三个需求，而只能满足其中的两项，因此在进行对CAP定理的应用的时候，需要根据业务的要求抛弃其中的一项，下表所示是抛弃CAP定理中任意一项特性的场景说明。 一致性的分类一致性是指从系统外部读取系统内部的数据时，在一定约束条件下相同，即数据变动在系统内部各节点应该是同步的。根据一致性的强弱程度不同，可以将一致性的分类为如下几种： ① 强一致性（strong consistency）。任何时刻，任何用户都能读取到最近一次成功更新的数据。 ② 单调一致性（monotonic consistency）。任何时刻，任何用户一旦读到某个数据在某次更新后的值，那么就不会再读到比这个值更旧的值。也就是说，可获取的数据顺序必是单调递增的。 ③ 会话一致性（session consistency）。任何用户在某次会话中，一旦读到某个数据在某次更新后的值，那么在本次会话中就不会再读到比这个值更旧的值。会话一致性是在单调一致性的基础上进一步放松约束，只保证单个用户单个会话内的单调性，在不同用户或同一用户不同会话间则没有保障。 ④ 最终一致性（eventual consistency）。用户只能读到某次更新后的值，但系统保证数据将最终达到完全一致的状态，只是所需时间不能保障。 ⑤ 弱一致性（weak consistency）。用户无法在确定时间内读到最新更新的值。 CAP权衡通过CAP理论及前面的证明，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？ 我们分三种情况来阐述一下。 CA without P这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。这也是为什么在前面的CAP证明中，我们以系统满足P为前提论述了无法同时满足C和A。 比如我们熟知的关系型数据库，如MySql和Oracle就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把P也考虑进来。 其实，在CAP理论中。C，A，P三者并不是平等的，CAP之父在《Spanner, TrueTime and the CAP Theorem》一文中写到： 如果说Spanner真有什么特别之处，那就是谷歌的广域网。Google通过建立私有网络以及强大的网络工程能力来保证P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。 从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。 所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。 CP without A如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。 一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。 设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。 无论是像Redis、HBase这种分布式存储系统，还是像Zookeeper这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？ ZooKeeper是个CP（一致性+分区容错性）的，即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。 AP wihtout C要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。 这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。 你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。 但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。 对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>CAP理论</category>
      </categories>
      <tags>
        <tag>CAP理论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper一致性保证]]></title>
    <url>%2F2019%2F08%2F23%2Fzookeeper-consistency%2F</url>
    <content type="text"><![CDATA[zookeeper（简称zk），顾名思义，为动物园管理员的意思，动物对应服务节点，zk是这些节点的管理者。在分布式场景中，zk的应用非常广泛，如：数据发布/订阅、命名服务、配置中心、分布式锁、集群管理、选主与服务发现等等。这不仅得益于zk类文件系统的数据模型和基于Watcher机制的分布式事件通知，也得益于zk特殊的高容错数据一致性协议。 这里的一致性，是指数据在多个副本之间保持一致的特性。分布式环境里，多个副本处于不同的节点上，如果对副本A的更新操作，未同步到副本B上，外界获取数据时，A与B的返回结果会不一样，这是典型的分布式数据不一致情况。而强一致性，是指分布式系统中，如果某个数据更新成功，则所有用户都能读取到最新的值。CAP定理告诉我们，在分布式系统设计中，P（分区容错性）是不可缺少的，因此只能在A（可用性）与C（一致性）间做取舍。本文主要探究zk在数据一致性方面的处理逻辑。 基本概念 数据节点（dataNode）：zk数据模型中的最小数据单元，数据模型是一棵树，由斜杠（/）分割的路径名唯一标识，数据节点可以存储数据内容及一系列属性信息，同时还可以挂载子节点，构成一个层次化的命名空间。 会话（Session）：指zk客户端与zk服务器之间的会话，在zk中，会话是通过客户端和服务器之间的一个TCP长连接来实现的。通过这个长连接，客户端能够使用心跳检测与服务器保持有效的会话，也能向服务器发送请求并接收响应，还可接收服务器的Watcher事件通知。Session的sessionTimeout，是会话超时时间，如果这段时间内，客户端未与服务器发生任何沟通（心跳或请求），服务器端会清除该session数据，客户端的TCP长连接将不可用，这种情况下，客户端需要重新实例化一个Zookeeper对象。 事务及ZXID：事务是指能够改变Zookeeper服务器状态的操作，一般包括数据节点的创建与删除、数据节点内容更新和客户端会话创建与失效等操作。对于每个事务请求，zk都会为其分配一个全局唯一的事务ID，即ZXID，是一个64位的数字，高32位表示该事务发生的集群选举周期（集群每发生一次leader选举，值加1），低32位表示该事务在当前选择周期内的递增次序（leader每处理一个事务请求，值加1，发生一次leader选择，低32位要清0）。 事务日志：所有事务操作都是需要记录到日志文件中的，可通过 dataLogDir配置文件目录，文件是以写入的第一条事务zxid为后缀，方便后续的定位查找。zk会采取“磁盘空间预分配”的策略，来避免磁盘Seek频率，提升zk服务器对事务请求的影响能力。默认设置下，每次事务日志写入操作都会实时刷入磁盘，也可以设置成非实时（写到内存文件流，定时批量写入磁盘），但那样断电时会带来丢失数据的风险。 数据快照：数据快照是zk数据存储中另一个非常核心的运行机制。数据快照用来记录zk服务器上某一时刻的全量内存数据内容，并将其写入到指定的磁盘文件中，可通过dataDir配置文件目录。可配置参数snapCount，设置两次快照之间的事务操作个数，zk节点记录完事务日志时，会统计判断是否需要做数据快照（距离上次快照，事务操作次数等于snapCount/2~snapCount 中的某个值时，会触发快照生成操作，随机值是为了避免所有节点同时生成快照，导致集群影响缓慢）。 过半：所谓“过半”是指大于集群机器数量的一半，即大于或等于（n/2+1），此处的“集群机器数量”不包括observer角色节点。leader广播一个事务消息后，当收到半数以上的ack信息时，就认为集群中所有节点都收到了消息，然后leader就不需要再等待剩余节点的ack，直接广播commit消息，提交事务。选举中的投票提议及数据同步时，也是如此，leader不需要等到所有learner节点的反馈，只要收到过半的反馈就可进行下一步操作。 数据模型zk维护的数据主要有：客户端的会话（session）状态及数据节点（dataNode）信息。zk在内存中构造了个DataTree的数据结构，维护着path到dataNode的映射以及dataNode间的树状层级关系。为了提高读取性能，集群中每个服务节点都是将数据全量存储在内存中。可见，zk最适于读多写少且轻量级数据（默认设置下单个dataNode限制为1MB大小）的应用场景。数据仅存储在内存是很不安全的，zk采用事务日志文件及快照文件的方案来落盘数据，保障数据在不丢失的情况下能快速恢复。 集群架构zk集群由多个节点组成，其中有且仅有一个leader，处理所有事务请求；follower及observer统称learner。learner需要同步leader的数据。follower还参与选举及事务决策过程。zk客户端会打散配置文件中的serverAddress 顺序并随机组成新的list，然后循环按序取一个服务器地址进行连接，直到成功。follower及observer会将事务请求转交给leader处理。 要搭建一个高可用的zk集群，我们首先需要确定好集群规模。一般我们将节点（指leader及follower节点，不包括observer节点）个数设置为 2*n+1 ，n为可容忍宕机的个数。 zk使用“过半”设计原则，很好地解决了单点问题，提升了集群容灾能力。但是zk的集群伸缩不是很灵活，集群中所有机器ip及port都是事先配置在每个服务的zoo.cfg 文件里的。如果要往集群增加一个follower节点，首先需要更改所有机器的zoo.cfg，然后逐个重启。集群模式下，单个zk服务节点启动时的工作流程大体如下： 统一由QuorumPeerMain作为启动类，加载解析zoo.cfg配置文件； 初始化核心类：ServerCnxnFactory（IO操作）、FileTxnSnapLog（事务日志及快照文件操作）、QuorumPeer实例（代表zk集群中的一台机器）、ZKDatabase（内存数据库）等； 加载本地快照文件及事务日志，恢复内存数据； 完成leader选举，节点间通过一系列投票，选举产生最合适的机器成为leader，同时其余机器成为follower或是observer。关于选举算法，就是集群中哪个机器处理的数据越新（通过ZXID来比较，ZXID越大，数据越新），其越有可能被选中； 完成leader与learner间的数据同步：集群中节点角色确定后，leader会重新加载本地快照及日志文件，以此作为基准数据，再结合各个learner的本地提交数据，leader再确定需要给具体learner回滚哪些数据及同步哪些数据； 当leader收到过半的learner完成数据同步的ACK，集群开始正常工作，可以接收并处理客户端请求，在此之前集群不可用。 ZooKeeper的读写机制概述ZooKeeper的核心思想是，提供一个非锁机制的Wait Free的用于分布式系统同步的核心服务。提供简单的文件创建、读写操作接口，其系统核心本身对文件读写并不提供加锁互斥的服务，但是提供基于版本比对的更新操作，客户端可以基于此自己实现加锁逻辑。如下图所示: ZK集群服务 Zookeeper是一个由多个Server组成的集群，该集群有一个Leader，多个Follower。客户端可以连接任意ZooKeeper服务节点来读写数据，如下图所示: ZK集群中每个Server，都保存一份数据副本。Zookeeper使用简单的同步策略，通过以下两条基本保证来实现数据的一致性： ① 全局串行化所有的写操作 ② 保证同一客户端的指令被FIFO执行（以及消息通知的FIFO） 所有的读请求由Zk Server 本地响应，所有的更新请求将转发给Leader，由Leader实施。 ZK组件ZK组件如下图所示。ZK组件除了请求处理器（Request Processor）以外，组成ZK服务的每一个Server会复制这些组件的副本。 ReplicatedDatabase是一个内存数据库，它包含了整个Data Tree。为了恢复，更新会被记录到磁盘，并且写在被应用到内存数据库之前，先被序列化到磁盘。 每一个ZK Server，可服务于多个Client。Client可以连接到一台Server，来提交请求。读请求，由每台Server数据库的本地副本来进行服务。改变服务器的状态的写请求，需要通过一致性协议来处理。 作为一致性协议的一部分，来自Client的所有写请求，都要被转发到一个单独的Server，称作Leader。ZK集群中其他Server 称作Follower，负责接收Leader发来的提议消息，并且对消息转发达成一致。消息层处理leader失效，同步Followers和Leader。 ZooKeeper使用自定义的原子性消息协议。由于消息传送层是原子性的，ZooKeeper能够保证本地副本不产生分歧。当leader收到一个写请求，它会计算出当写操作完成后系统将会是什么状态，接着将之转变为一个捕获状态的事务。 ZK性能ZooKeeper被应用程序广泛使用，并有数以千计的客户端同时的访问它，所以我们需要高吞吐量。我们为ZooKeeper 设计的工作负载的读写比例是 2：1以上。然而我们发现，ZooKeeper的高写入吞吐量，也允许它被用于一些写占主导的工作负载。ZooKeeper通过每台Server上的本地 ZK的状态副本，来提供高读取吞吐量。因此，容错性和读吞吐量是以添加到该服务的服务器数量为尺度。写吞吐量并不以添加到该服务的机器数量为尺度。 例如，在它的诞生地Yahoo公司，对于写占主导的工作负载来说，ZooKeeper的基准吞吐量已经超过每秒10000个操作；对于常规的以读为主导的工作负载来说，吞吐量更是高出了好几倍。 zookeeper一致性协议zookeeper实现数据一致性的核心是ZAB协议（Zookeeper原子消息广播协议）。该协议需要做到以下几点： 集群在半数以下节点宕机的情况下，能正常对外提供服务； 客户端的写请求全部转交给leader来处理，leader需确保写变更能实时同步给所有follower及observer； leader宕机或整个集群重启时，需要确保那些已经在leader服务器上提交的事务最终被所有服务器都提交，确保丢弃那些只在leader服务器上被提出的事务，并保证集群能快速恢复到故障前的状态。 Zab协议有两种模式， 崩溃恢复（选主+数据同步）和消息广播（事务操作）。任何时候都需要保证只有一个主进程负责进行事务操作，而如果主进程崩溃了，就需要迅速选举出一个新的主进程。主进程的选举机制与事务操作机制是紧密相关的。下面详细讲解这三个场景的协议规则，从细节去探索ZAB协议的数据一致性原理。 崩溃恢复(选主)leader选举是zk中最重要的技术之一，也是保证分布式数据一致性的关键所在。当集群中的一台服务器处于如下两种情况之一时，就会进入leader选举阶段——服务器初始化启动、服务器运行期间无法与leader保持连接。 选举阶段，集群间互传的消息称为投票，投票Vote主要包括二个维度的信息：ID、ZXIDID 被推举的leader的服务器ID，集群中的每个zk节点启动前就要配置好这个全局唯一的ID。ZXID 被推举的leader的事务ID ，该值是从机器DataTree内存中取的，即事务已经在机器上被commit过了。 节点进入选举阶段后的大体执行逻辑如下：设置状态为LOOKING，初始化内部投票Vote (id,zxid) 数据至内存，并将其广播到集群其它节点。节点首次投票都是选举自己作为leader，将自身的服务ID、处理的最近一个事务请求的ZXID（ZXID是从内存数据库里取的，即该节点最近一个完成commit的事务id）及当前状态广播出去。然后进入循环等待及处理其它节点的投票信息的流程中。 循环等待流程中，节点每收到一个外部的Vote信息，都需要将其与自己内存Vote数据进行PK，规则为取ZXID大的，若ZXID相等，则取ID大的那个投票。若外部投票胜选，节点需要将该选票覆盖之前的内存Vote数据，并再次广播出去；同时还要统计是否有过半的赞同者与新的内存投票数据一致，无则继续循环等待新的投票，有则需要判断leader是否在赞同者之中，在则退出循环，选举结束，根据选举结果及各自角色切换状态，leader切换成LEADING、follower切换到FOLLOWING、observer切换到OBSERVING状态。 算法细节可参照FastLeaderElection.lookForLeader()，主要有三个线程在工作：选举线程（主动调用lookForLeader方法的线程，通过阻塞队列sendqueue及recvqueue与其它两个线程协作）、WorkerReceiver线程（选票接收器，不断获取其它服务器发来的选举消息，筛选后会保存到recvqueue队列中。zk服务器启动时，开始正常工作，不停止）以及WorkerSender线程（选票发送器，会不断地从sendqueue队列中获取待发送的选票，并广播至集群）。WorkerReceiver线程一直在工作，即使当前节点处于LEADING或者FOLLOWING状态，它起到了一个过滤的作用，当前节点为LOOKING时，才会将外部投票信息转交给选举线程处理；如果当前节点处于非LOOKING状态，收到了处于LOOKING状态的节点投票数据（外部节点重启或网络抖动情况下），说明发起投票的节点数据跟集群不一致，这时，当前节点需要向集群广播出最新的内存Vote(id，zxid)，落后节点收到该Vote后，会及时注册到leader上，并完成数据同步，跟上集群节奏，提供正常服务。 崩溃恢复(选主后的数据同步)选主算法中的zxid是从内存数据库中取的最新事务id，事务操作是分两阶段的（提出阶段和提交阶段），leader生成提议并广播给followers，收到半数以上的ACK后，再广播commit消息，同时将事务操作应用到内存中。follower收到提议后先将事务写到本地事务日志，然后反馈ACK，等接到leader的commit消息时，才会将事务操作应用到内存中。可见，选主只是选出了内存数据是最新的节点，仅仅靠这个是无法保证已经在leader服务器上提交的事务最终被所有服务器都提交。比如leader发起提议P1,并收到半数以上follower关于P1的ACK后，在广播commit消息之前宕机了，选举产生的新leader之前是follower，未收到关于P1的commit消息，内存中是没有P1的数据。而ZAB协议的设计是需要保证选主后，P1是需要应用到集群中的。这块的逻辑是通过选主后的数据同步来弥补。 选主后，节点需要切换状态，leader切换成LEADING状态后的流程如下： 重新加载本地磁盘上的数据快照至内存，并从日志文件中取出快照之后的所有事务操作，逐条应用至内存，并添加到已提交事务缓存commitedProposals。这样能保证日志文件中的事务操作，必定会应用到leader的内存数据库中。 获取learner发送的FOLLOWERINFO/OBSERVERINFO信息，并与自身commitedProposals比对，确定采用哪种同步方式，不同的learner可能采用不同同步方式（DIFF同步、TRUNC+DIFF同步、SNAP同步）。这里是拿learner内存中的zxid与leader内存中的commitedProposals（min、max）比对，如果zxid介于min与max之间，但又不存在于commitedProposals中时，说明该zxid对应的事务需要TRUNC回滚；如果 zxid 介于min与max之间且存在于commitedProposals中，则leader需要将zxid+1~max 间所有事务同步给learner，这些内存缺失数据，很可能是因为leader切换过程中造成commit消息丢失，learner只完成了事务日志写入，未完成提交事务，未应用到内存。 leader主动向所有learner发送同步数据消息，每个learner有自己的发送队列，互不干扰。同步结束时，leader会向learner发送NEWLEADER指令，同时learner会反馈一个ACK。当leader接收到来自learner的ACK消息后，就认为当前learner已经完成了数据同步，同时进入“过半策略”等待阶段。当leader统计到收到了一半已上的ACK时，会向所有已经完成数据同步的learner发送一个UPTODATE指令，用来通知learner集群已经完成了数据同步，可以对外服务了。细节可参照Leader.lead() 、Follower.followLeader()及LearnerHandler类。 恢复阶段的保证该恢复过程的复杂部分是在一个给定的时间内，提议冲突的绝对数量。最大数量冲突提议是一个可配置的选项，但是默认是1000。为了使该协议能够即使在Leader故障的情况下也能正常运作。我们需要做出两条具体的保证： ① 我们绝不能遗忘已经被deliver的消息，若一条消息在一台机器上被deliver，那么该消息必须将在每台机器上deliver，即使那台机器故障了。 例如，出现了这样一种情况：Leader发送了commit消息，但在该commit消息到达其他任何机器之前，Leader发生了故障。也就是说，只有Leader自己收到了commit消息。如图6中的C2。 “第一条保证”（deliver消息不能忘记）的一个示例。在该图中Server1是一个Leader，我们用L1表示，Server2和Server3为Follower。首先Leader发起了两个Proposal，P1和P2，并将P1、P2发送给了Server1和Server2。然后Leader对P1发起了Commit即C1，之后又发起了一个Proposal即P3，再后来又对P2发起了commit即C2，就在此时我们的Leader挂了。那么这时候，P3和C2这两个消息只有Leader自己收到了。 因为Leader已经deliver了该C2消息，client能够在消息中看到该事务的结果。所以该事务必须能够在其他所有的Server中deliver，最终使得client看到了一个一致性的服务视图。 ② 我们必须丢弃已经被skip的消息。一个被skip的消息，必须仍然需要被skip。 例如，发生了这样一种情况：Leader发送了propose消息，但在该propose消息到达其他任何机器之前，Leader发生了故障。也就是说，只有Leader自己收到了propose消息。如图6的P3所示。 在图6中没有任何一个server能够看到3号提议，所以在图7中当server 1恢复时他需要在系统恢复时丢弃三号提议P3。 “第二条保证”（skip消息必须被丢弃）的一个示例。Server1挂掉以后，Server3被选举为Leader，我们用L2表示。L2中还有未被deliver的消息P1、P2，所以，L2在发出新提议P10000001、P10000002之前，L2先将P1、P2两个消息deliver。因此，L2先发出了两个commit消息C1、C2，之后L2才发出了新的提议P10000001和P10000002。 如果Server1 恢复之后再次成为了Leader，此时再次将P3在P10000001和P10000002之后deliver，那么将违背顺序性的保障。 保证的实现如果Leader选举协议保证了新Leader在Quorum Server中具有最高的提议编号，即Zxid最高。那么新选举出来的leader将具有所有已deliver的消息。新选举出来的Leader，在提出一个新消息之前，首先要保证事务日志中的所有消息都由Quorum Follower已Propose并deliver。需要注意的是，我们可以让新Leader成为一个用最高zxid来处理事务的server，来作为一个优化。这样，作为新被选举出来的Leader，就不必去从一组Followers中找出包含最高zxid的Followers和获取丢失的事务。 ① 第一条 所有的正确启动的Servers，将会成为Leader或者跟随一个Leader。Leader能够确保它的Followers看到所有的提议，并deliver所有已经deliver的消息。通过将新连接上的Follower所没有见过的所有PROPOSAL进行排队，并之后对该Proposals的COMMIT消息进行排队，直到最后一个COMMIT消息。在所有这样的消息已经排好队之后，Leader将会把Follower加入到广播列表，以便今后的提议和确认。这一条是为了保证一致性，因为如果一条消息P已经在旧Leader-Server1中deliver了，即使它刚刚将消息P deliver之后就挂了，但是当旧Leader-Server1重启恢复之后，我们的Client就可以从该Server中看到该消息P deliver的事务，所以为了保证每一个client都能看到一个一致性的视图，我们需要将该消息在每个Server上deliver。 ② 第二条 skip已经Propose，但不能deliver的消息，处理起来也比较简单。在我们的实现中，Zxid是由64位数字组成的，低32位用作简单计数器。高32位是一个epoch。每当新Leader接管它时，将获取日志中Zxid最大的epoch，新Leader Zxid的epoch位设置为epoch+1，counter位设置0。用epoch来标记领导关系的改变,并要求Quorum Servers 通过epoch来识别该leader，避免了多个Leader用同一个Zxid发布不同的提议。 这个方案的一个优点就是，我们可以skip一个失败的领导者的实例，从而加速并简化了恢复过程。如果一台宕机的Server重启，并带有未发布的 Proposal，那么先前的未发布的所有提议将永不会被deliver。并且它不能够成为一个新leader，因为任何一种可能的 Quorum Servers ，都会有一个Server其Proposal 来自与一个新epoch因此它具有一个较高的zxid。当Server以Follower的身份连接，领导者检查自身最后提交的提议，该提议的epoch 为Follower的最新提议的epoch（也就是图7中新Leader-Server2中deliver的C2提议），并告诉Follower截断事务日志直到该epoch在新Leader中deliver的最后的Proposal即C2。在图7中，当旧Leader-Server1连接到了新leader-Server2，leader将告诉他从事务日志中清除3号提议P3，具体点就是清除P2之后的所有提议，因为P2之后的所有提议只有旧Leader-Server1知道，其他Server不知道。 Paxos与Zab① Paxos一致性 Paxos的一致性不能达到ZooKeeper的要求，我们可以下面一个例子。我们假设ZK集群由三台机器组成，Server1、Server2、Server3。Server1为Leader，他生成了三条Proposal，P1、P2、P3。但是在发送完P1之后，Server1就挂了。如下图8所示。 Server1挂掉之后，Server3被选举成为Leader，因为在Server3里只有一条Proposal—P1。所以，Server3在P1的基础之上又发出了一条新Proposal—P2＇，P2＇的Zxid为02。如下图9所示。 Server2发送完P2＇之后，它也挂了。此时Server1已经重启恢复，并再次成为了Leader。那么，Server1将发送还没有被deliver的Proposal—P2和P3。由于Follower-Server2中P2＇的Zxid为02和Leader-Server1中P2的Zxid相等，所以P2会被拒绝。而P3，将会被Server2接受。如图10所示。 我们分析一下Follower-Server2中的Proposal，由于P2’将P2的内容覆盖了。所以导致，Server2中的Proposal-P3无法生效，因为他的父节点并不存在。 ② Zab一致性 首先来分析一下，上面的示例中为什么不满足ZooKeeper需求。ZooKeeper是一个树形结构，很多操作都要先检查才能确定能不能执行，比如，在图10中Server2有三条Proposal。P1的事务是创建节点”/zk”，P2’是创建节点”/c”，而P3是创建节点 “/a/b”,由于”/a”还没建，创建”a/b”就搞不定了。那么，我们就能从此看出Paxos的一致性达不到ZooKeeper一致性的要求。 为了达到ZooKeeper所需要的一致性，ZooKeeper采用了Zab协议。Zab做了如下几条保证，来达到ZooKeeper要求的一致性。 (a) Zab要保证同一个leader的发起的事务要按顺序被apply，同时还要保证只有先前的leader的所有事务都被apply之后，新选的leader才能在发起事务。 (b) 一些已经Skip的消息，需要仍然被Skip。 我想对于第一条保证大家都能理解，它主要是为了保证每 个Server的数据视图的一致性。我重点解释一下第二条，它是如何实现。为了能够实现，Skip已经被skip的消息。我们在Zxid中引入了 epoch，如下图所示。每当Leader发生变换时，epoch位就加1，counter位置0。 我们继续使用上面的例子，看一下他是如何实现Zab的第二条保证的。我们假设ZK集群由三台机器组成，Server1、Server2、Server3。Server1为Leader，他生成了三条 Proposal，P1、P2、P3。但是在发送完P1之后，Server1就挂了。如下图12所示。 Server1挂掉之后，Server3被选举成为 Leader，因为在Server3里只有一条Proposal—P1。所以，Server3在P1的基础之上又发出了一条新Proposal—P2＇， 由于Leader发生了变换，epoch要加1，所以epoch由原来的0变成了1，而counter要置0。那么，P2＇的Zxid为10。如下图13所示。 Server3发送完P2＇之后，它也挂了。此时Server1已经重启恢复，并再次成为了Leader。那么，Server1将发送还没有被deliver的Proposal—P2和P3。由于Server2中P2＇的Zxid为10，而Leader-Server1中P2和P3的Zxid分别为02和03，P2＇的epoch位高于P2和P3。所以此时Leader-Server1的P2和P3都会被拒绝,那么我们Zab的第二条保证也就实现了。如图14所示。 消息广播(事务操作)一旦Leader已经和多数的Follower进行了状态同步后，他就可以开始广播消息了，即进入广播状态。ZooKeeper服务一直维持在Broadcast状态，直到Leader崩溃了或者Leader失去了大部分的Followers支持。 ZAB协议对于事务操作的处理是一个类似于二阶段提交过程。针对客户端的事务请求，leader服务器会为其生成对应的事务proposal，并将其发送给集群中所有follower机器，然后收集各自的选票，最后进行事务提交。流程如下图。 ZAB协议的二阶段提交过程中，移除了中断(aborts)逻辑（事务回滚），所有follower服务器要么正常反馈leader提出的事务proposal，要么丢弃该Leader的Propose。没有”aborts”意味着，只要有指定数量的机器确认了该Propose，而不是等待所有机器的回应。 广播协议在所有的通讯过程中使用TCP的FIFO信道，通过使用该信道，使保持有序性变得非常的容易。通过FIFO信道，消息被有序的deliver。只要收到的消息一被处理，其顺序就会被保存下来。 Leader会广播已经被deliver的Proposal消息。在发出一个Proposal消息前，Leader会分配给Proposal一个单调递增的唯一id，称之为zxid。因为Zab保证了因果有序， 所以递交的消息也会按照zxid进行排序。广播是把Proposal封装到消息当中，并添加到指向Follower的输出队列中，通过FIFO信道发送到 Follower。当Follower收到一个Proposal时，会将该proposal写入到事务日志，然后立马反馈ACK给leader，也就是说如果不是网络、内存或磁盘等问题，follower肯定会写入成功，并正常反馈ACK。leader收到过半follower的ACK后，会广播commit消息给所有learner，并将事务应用到内存；learner收到commit消息后会将事务应用到内存。这个用来达成共识的协议被设计成具有原子性，因此每个修改要么成功要么失败。 ZAB协议中多次用到“过半”设计策略 ，该策略是zk在A（可用性）与C（一致性）间做的取舍，也是zk具有高容错特性的本质。相较分布式事务中的2PC（二阶段提交协议）的“全量通过”，ZAB协议可用性更高（牺牲了部分一致性），能在集群半数以下服务宕机时正常对外提供服务。 ZooKeeper的数据一致性保障顺序一致性: 来自任意特定客户端的更新都会按其发送顺序被提交保持一致。也就是说，针对同一个Follower A提交的写请求request1、request2，某些Follower虽然可能不能在请求提交成功后立即看到（也就是强一致性），但经过自身与Leader之间的同步后，这些Follower在看到这两个请求时，一定是先看到request1，然后再看到request2，两个请求之间不会乱序，即顺序一致性。如下图: Leader在处理第4步Follower的ack回复时，采用过半数响应即成功原则，也就是这时候有的Follower是还没有处理或者处理成功这个请求的 原子性：每个更新要么成功，要么失败。这意味着如果一个更新失败，则不会有客户端会看到这个更新的结果。 单一系统映像：一个客户端无论连接到哪一台服务器，它看到的都是同样的系统视图。这意味着，如果一个客户端在同一个会话中连接到一台新的服务器，它所看到的系统状态不会比在之前服务器上所看到的更老。当一台服务器出现故障，导致它的一个客户端需要尝试连接集合体中其他的服务器时，所有滞后于故障服务器的服务器都不会接受该连接请求，除非这些服务器赶上故障服务器。 持久性：一个更新一旦成功，其结果就会持久存在并且不会被撤销。这表明更新不会受到服务器故障的影响。 实时性：在特定的一段时间内，客户端看到的系统需要被保证是实时的（在十几秒的时间里）。在此时间段内，任何系统的改变将被客户端看到，或者被客户端侦测到。 如何保证顺序一致性 FollowerRequestProcessor为Follower的首个处理器，如果是写请求，先将请求写入commitprocessor的queuedRequests（方便后续commit时判断是否本Follower提交的写请求），然后转Leader Leader为每个请求生成zxid，下发proposal给Follower，Follower会将请求写入到pendingTxns阻塞队列及txnLog中，然后发送ack给Leader 1234567public void logRequest(TxnHeader hdr, Record txn) &#123; Request request = new Request(hdr.getClientId(), hdr.getCxid(), hdr.getType(), hdr, txn, hdr.getZxid()); if ((request.zxid &amp; 0xffffffffL) != 0) &#123; pendingTxns.add(request); &#125; syncProcessor.processRequest(request);&#125; proposal这步是会发给所有的follower的（放到LearnerHandler的请求处理队列中，一个Follower一个LearnerHandler），之后Follower的ack就不一定全返回了 ack过半，Leader发送commit请求给所有Follower，Follower对比commit request的zxid和前面提到的pendingTxns的zxid，不一致的话Follower退出，重新跟Leader同步 1234567long firstElementZxid = pendingTxns.element().zxid;if (firstElementZxid != zxid) &#123; LOG.error("Committing zxid 0x" + Long.toHexString(zxid) + " but next pending txn 0x" + Long.toHexString(firstElementZxid)); System.exit(12);&#125; Follower处理commit请求，如果不是本Follower提交的写请求，直接调用FinalRequestProcessor做持久化，触发watches；如果是本Follower提交，则做一些特殊处理（主要针对客户端连接断开的场景），然后调用FinalRequestProcessor等后续处理流程 FinalRequestProcessor做持久化，返回客户端 总之：Follower通过队列和zxid等顺序标识保证请求的顺序处理，一言不合就会退出或者重新同步Leader 用CAP理论来分析ZooKeeper在此ZooKeeper保证的是CP，参见CAP定理。 分析：可用性（A:Available） 不能保证每次服务请求的可用性。任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性；但是它不能保证每次服务请求的可用性（注：也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果）。所以说，ZooKeeper不能保证服务可用性。 进行leader选举时集群都是不可用。在使用ZooKeeper获取服务列表时，当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30 ~ 120s, 且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。所以说，ZooKeeper不能保证服务可用性。 参考资料 详细投票的过程参见https://dbaplus.cn/news-141-1875-1.html?spm=ata.13261165.0.0.3e572d4cQLT4Cu ，建议阅读这篇论文: ZooKeeper’s atomic broadcast protocol:Theory and practice 浅析Zookeeper的一致性原理: https://zhuanlan.zhihu.com/p/25594630 深入解读zookeeper一致性原理: https://www.cnblogs.com/wuxl360/p/5817646.html 分布式系统CAP定理: https://sainpo.top/2019/08/23/distributed-systems-cap]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper请求处理]]></title>
    <url>%2F2019%2F08%2F19%2Fzookeeper-request-processor%2F</url>
    <content type="text"><![CDATA[请求处理链对于请求处理链而言，所有请求处理器的父接口为RequestProcessor: 1234567891011public interface RequestProcessor &#123; public static class RequestProcessorException extends Exception &#123; public RequestProcessorException(String msg, Throwable t) &#123; super(msg, t); &#125; &#125; void processRequest(Request request) throws RequestProcessorException; void shutdown();&#125; 处理的主要逻辑在processRequest中，通过processRequest方法可以将请求传递到下个处理器，通常是单线程的。而shutdown表示关闭处理器，其意味着该处理器要关闭和其他处理器的连接。 PrepRequestProcessor。请求预处理器。在Zookeeper中，那些会改变服务器状态的请求称为事务请求（创建节点、更新数据、删除节点、创建会话等），PrepRequestProcessor能够识别出当前客户端请求是否是事务请求。对于事务请求，PrepRequestProcessor处理器会对其进行一系列预处理，如创建请求事务头、事务体、会话检查、ACL检查和版本检查等。 SyncRequestProcessor。事务日志记录处理器。用来将事务请求记录到事务日志文件中，同时会触发Zookeeper进行数据快照。 FinalRequestProcessor。用来进行客户端请求返回之前的操作，包括创建客户端请求的响应，针对事务请求，该处理还会负责将事务应用到内存数据库中去。 会话创建请求Zookeeper服务端对于会话创建的处理，大体可以分为请求接收、会话创建、预处理、事务处理、事务应用和会话响应六大环节，其大体流程如下: 请求接收 I/O层接收来自客户端的请求。NIOServerCnxn维护每一个客户端连接，客户端与服务器端的所有通信都是由NIOServerCnxn负责，其负责统一接收来自客户端的所有请求，并将请求内容从底层网络I/O中完整地读取出来。 判断是否是客户端会话创建请求。每个会话对应一个NIOServerCnxn实体，对于每个请求，Zookeeper都会检查当前NIOServerCnxn实体是否已经被初始化，如果尚未被初始化，那么就可以确定该客户端一定是会话创建请求。 反序列化ConnectRequest请求。一旦确定客户端请求是否是会话创建请求，那么服务端就可以对其进行反序列化，并生成一个ConnectRequest载体。 判断是否是ReadOnly客户端。如果当前Zookeeper服务器是以ReadOnly模式启动，那么所有来自非ReadOnly型客户端的请求将无法被处理。因此，服务端需要先检查是否是ReadOnly客户端，并以此来决定是否接受该会话创建请求。 检查客户端ZXID。正常情况下，在一个Zookeeper集群中，服务端的ZXID必定大于客户端的ZXID，因此若发现客户端的ZXID大于服务端ZXID，那么服务端不接受该客户端的会话创建请求。 协商sessionTimeout。在客户端向服务器发送超时时间后，服务器会根据自己的超时时间限制最终确定该会话超时时间，这个过程就是sessionTimeout协商过程。 判断是否需要重新激活创建会话。服务端根据客户端请求中是否包含sessionID来判断该客户端是否需要重新创建会话，若客户单请求中包含sessionID，那么就认为该客户端正在进行会话重连，这种情况下，服务端只需要重新打开这个会话，否则需要重新创建。 会话创建 为客户端生成sessionID。在为客户端创建会话之前，服务端首先会为每个客户端分配一个sessionID，服务端为客户端分配的sessionID是全局唯一的。 注册会话。向SessionTracker中注册会话，SessionTracker中维护了sessionsWithTimeout和sessionsById，在会话创建初期，会将客户端会话的相关信息保存到这两个数据结构中。 激活会话。激活会话涉及Zookeeper会话管理的分桶策略，其核心是为会话安排一个区块，以便会话清理程序能够快速高效地进行会话清理。 生成会话密码。服务端在创建一个客户端会话时，会同时为客户端生成一个会话密码，连同sessionID一同发给客户端，作为会话在集群中不同机器间转移的凭证。 预处理 将请求交给PrepRequestProcessor处理器处理。在提交给第一个请求处理器之前，Zookeeper会根据该请求所属的会话，进行一次激活会话操作，以确保当前会话处于激活状态，完成会话激活后，则提交请求至处理器。 创建请求事务头。对于事务请求，Zookeeper会为其创建请求事务头，服务端后续的请求处理器都是基于该请求头来识别当前请求是否是事务请求，请求事务头包含了一个事务请求最基本的一些信息，包括sessionID、ZXID（事务请求对应的事务ZXID）、CXID（客户端的操作序列）和请求类型（如create、delete、setData、createSession等）等。 创建请求事务体。由于此时是会话创建请求，其事务体是CreateSessionTxn。 注册于激活会话。处理由非Leader服务器转发过来的会话创建请求。 事务处理 将请求交给ProposalRequestProcessor处理器。与提议相关的处理器，从ProposalRequestProcessor开始，请求的处理将会进入三个子处理流程，分别是Sync流程、Proposal流程、Commit流程。 Sync流程 使用SyncRequestProcessor处理器记录事务日志，针对每个事务请求，都会通过事务日志的形式将其记录，完成日志记录后，每个Follower都会向Leader发送ACK消息，表明自身完成了事务日志的记录，以便Leader统计每个事务请求的投票情况。 Proposal流程 每个事务请求都需要集群中过半机器投票认可才能被真正应用到内存数据库中，这个投票与统计过程就是Proposal流程。 发起投票。若当前请求是事务请求，Leader会发起一轮事务投票，在发起事务投票之前，会检查当前服务端的ZXID是否可用。 生成提议Proposal。若ZXID可用，Zookeeper会将已创建的请求头和事务体以及ZXID和请求本身序列化到Proposal对象中，此Proposal对象就是一个提议。 广播提议。Leader以ZXID作为标识，将该提议放入投票箱outstandingProposals中，同时将该提议广播给所有Follower。 收集投票。Follower接收到Leader提议后，进入Sync流程进行日志记录，记录完成后，发送ACK消息至Leader服务器，Leader根据这些ACK消息来统计每个提议的投票情况，当一个提议获得半数以上投票时，就认为该提议通过，进入Commit阶段。 将请求放入toBeApplied队列中。 广播Commit消息。Leader向Follower和Observer发送COMMIT消息。向Observer发送INFORM消息，向Leader发送ZXID。 Commit流程 将请求交付CommitProcessor。CommitProcessor收到请求后，将其放入queuedRequests队列中。 处理queuedRequest队列请求。CommitProcessor中单独的线程处理queuedRequests队列中的请求。 标记nextPending。若从queuedRequests中取出的是事务请求，则需要在集群中进行投票处理，同时将nextPending标记位当前请求。 等待Proposal投票。在进行Commit流程的同时，Leader会生成Proposal并广播给所有Follower服务器，此时，Commit流程等待，直到投票结束。 投票通过。若提议获得过半机器认可，则进入请求提交阶段，该请求会被放入commitedRequests队列中，同时唤醒Commit流程。 提交请求。若commitedRequests队列中存在可以提交的请求，那么Commit流程则开始提交请求，将请求放入toProcess队列中，然后交付下一个请求处理器: FinalRequestProcessor。 事务应用 交付给FinalRequestProcessor处理器。FinalRequestProcessor处理器检查outstandingChanges队列中请求的有效性，若发现这些请求已经落后于当前正在处理的请求，那么直接从outstandingChanges队列中移除。 事务应用。之前的请求处理仅仅将事务请求记录到了事务日志中，而内存数据库中的状态尚未改变，因此，需要将事务变更应用到内存数据库。 将事务请求放入队列commitProposal。完成事务应用后，则将该请求放入commitProposal队列中，commitProposal用来保存最近被提交的事务请求，以便集群间机器进行数据的快速同步。 会话响应 统计处理。Zookeeper计算请求在服务端处理所花费的时间，统计客户端连接的基本信息，如lastZxid(最新的ZXID)、lastOp(最后一次和服务端的操作)、lastLatency(最后一次请求处理所花费的时间)等。 创建响应ConnectResponse。会话创建成功后的响应，包含了当前客户端和服务端之间的通信协议版本号、会话超时时间、sessionID和会话密码。 序列化ConnectResponse。 I/O层发送响应给客户端。 SetData请求服务端对于SetData请求大致可以分为四步，预处理、事务处理、事务应用、请求响应。 预处理 I/O层接收来自客户端的请求。 判断是否是客户端”会话创建”请求。对于SetData请求，按照正常事务请求进行处理。 将请求交给PrepRequestProcessor处理器进行处理。 创建请求事务头。 会话检查。检查该会话是否有效。 反序列化请求，并创建ChangeRecord记录。反序列化并生成特定的SetDataRequest请求，请求中包含了数据节点路径path、更新的内容data和期望的数据节点版本version。同时根据请求对应的path，Zookeeper生成一个ChangeRecord记录，并放入outstandingChanges队列中。 ACL检查。检查客户端是否具有数据更新的权限。 数据版本检查。通过version属性来实现乐观锁机制的写入校验。 创建请求事务体SetDataTxn。 保存事务操作到outstandingChanges队列中。 事务处理对于事务请求，服务端都会发起事务处理流程。所有事务请求都是由ProposalRequestProcessor处理器处理，通过Sync、Proposal、Commit三个子流程相互协作完成。 事务应用 交付给FinalRequestProcessor处理器。 事务应用。将请求事务头和事务体直接交给内存数据库ZKDatabase进行事务应用，同时返回ProcessTxnResult对象，包含了数据节点内容更新后的stat。 将事务请求放入commitProposal队列。 请求响应 创建响应体SetDataResponse。其包含了当前数据节点的最新状态stat。 创建响应头。包含当前响应对应的事务ZXID和请求处理是否成功的标识。 序列化响应。 I/O层发送响应给客户端。 GetData请求服务端对于GetData请求的处理，大致分为三步，预处理、非事务处理、请求响应。 预处理 I/O层接收来自客户端的请求。 判断是否是客户端”会话创建”请求。 将请求交给PrepRequestProcessor处理器进行处理。 会话检查。 非事务处理 反序列化GetDataRequest请求。 获取数据节点。 ACL检查。 获取数据内容和stat，注册Watcher。 请求响应 创建响应体GetDataResponse。响应体包含当前数据节点的内容和状态stat。 创建响应头。 统计处理。 序列化响应。 I/O层发送响应给客户端。 参考资料 【分布式】Zookeeper请求处理]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper会话]]></title>
    <url>%2F2019%2F08%2F19%2Fzookeeper-session%2F</url>
    <content type="text"><![CDATA[用途客户端与服务端之间任何交互操作都与会话息息相关，如临时节点的生命周期、客户端请求的顺序执行、Watcher通知机制等。Zookeeper的连接与会话就是客户端通过实例化Zookeeper对象来实现客户端与服务端创建并保持TCP连接的过程。本质上，就是：Session 映射到一个 TCP 长连接，并且标识这条长连接 通过 TCP 长连接，发送请求、接受响应 接收来自 Server 的 Watcher 事件通知 会话状态在Zookeeper客户端与服务端成功完成连接创建后，就创建了一个会话，Zookeeper会话在整个运行期间的生命周期中，会在不同的会话状态中之间进行切换，这些状态可以分为CONNECTING、CONNECTED、RECONNECTING、RECONNECTED、CLOSE等。 一旦客户端开始创建Zookeeper对象，那么客户端状态就会变成CONNECTING状态，同时客户端开始尝试连接服务端，连接成功后，客户端状态变为CONNECTED，通常情况下，由于断网或其他原因，客户端与服务端之间会出现断开情况，一旦碰到这种情况，Zookeeper客户端会自动进行重连服务，同时客户端状态再次变成CONNCTING，直到重新连上服务端后，状态又变为CONNECTED，在通常情况下，客户端的状态总是介于CONNECTING和CONNECTED之间。但是，如果出现诸如会话超时、权限检查或是客户端主动退出程序等情况，客户端的状态就会直接变更为CLOSE状态。 会话创建Session是Zookeeper中的会话实体，代表了一个客户端会话，其包含了如下四个属性: sessionID：服务节点启动时，时间戳 + sid（myid中的数字），经过一定算法计算得到 基准 sessionID 之后的 SessionID，都为基准 SessionID自增得到。 TimeOut：超时时间，时间段 TickTime：下次超时时间点，TickTime 约为 currentTime + TimeOut，方便 ZK Server 对会话分桶策略管理，高效的进行会话检查和清理。 isClosing：ZK Server 判定 Session 超时后，将会话标记为已关闭，确保不再处理其新请求； Zookeeper为了保证请求会话的全局唯一性，在SessionTracker初始化时，调用initializeNextSession方法生成一个sessionID，之后在Zookeeper运行过程中，会在该sessionID的基础上为每个会话进行分配，初始化算法如下 12345678910111213141516171819public class SessionTrackerImpl extends ZooKeeperCriticalThread implements SessionTracker &#123;... /** * Generates an initial sessionId. High order byte is serverId, next 5 * 5 bytes are from timestamp, and low order 2 bytes are 0s. */ public static long initializeNextSession(long id) &#123; long nextSid; nextSid = (Time.currentElapsedTime() &lt;&lt; 24) &gt;&gt;&gt; 8; nextSid = nextSid | (id &lt;&lt;56); if (nextSid == EphemeralType.CONTAINER_EPHEMERAL_OWNER) &#123; ++nextSid; // this is an unlikely edge case, but check it just in case &#125; return nextSid; &#125;&#125; 其中的id表示配置在myid文件中的值，通常是一个整数，如1、2、3。该算法的高8位确定了所在机器，后56位使用当前时间的毫秒表示进行随机。SessionTracker是Zookeeper服务端的会话管理器，负责会话的创建、管理和清理等工作。 会话管理分桶策略Zookeeper的会话管理主要是通过SessionTracker来负责，其采用了分桶策略（将类似的会话放在同一区块中进行管理）进行管理，以便Zookeeper对会话进行不同区块的隔离处理以及同一区块的统一处理。 Zookeeper将所有的会话都分配在不同的区块之中，分配的原则是每个会话的下次超时时间点（ExpirationTime）。ExpirationTime指该会话最近一次可能超时的时间点。同时，Zookeeper Leader服务器在运行过程中会定时地进行会话超时检查，时间间隔是ExpirationInterval，默认为tickTime的值，ExpirationTime的计算时间如下 ExpirationTime = ((CurrentTime + SessionTimeOut) / ExpirationInterval + 1) * ExpirationInterval 会话激活为了保持客户端会话的有效性，客户端会在会话超时时间过期范围内向服务端发送PING请求来保持会话的有效性（心跳检测）。同时，服务端需要不断地接收来自客户端的心跳检测，并且需要重新激活对应的客户端会话，这个重新激活过程称为TouchSession。会话激活不仅能够使服务端检测到对应客户端的存货性，同时也能让客户端自己保持连接状态，其流程如下: 如上图所示，整个流程分为四步 检查该会话是否已经被关闭。若已经被关闭，则直接返回即可。 计算该会话新的超时时间ExpirationTime_New。使用上面提到的公式计算下一次超时时间点。 获取该会话上次超时时间ExpirationTime_Old。计算该值是为了定位其所在的区块。 迁移会话。将该会话从老的区块中取出，放入ExpirationTime_New对应的新区块中。 在上面会话激活过程中，只要客户端发送心跳检测，服务端就会进行一次会话激活，心跳检测由客户端主动发起，以PING请求形式向服务端发送，在Zookeeper的实际设计中，只要客户端有请求发送到服务端，那么就会触发一次会话激活，以下两种情况都会触发会话激活。 客户端向服务端发送请求，包括读写请求，就会触发会话激活。 客户端发现在sessionTimeout/3时间内尚未和服务端进行任何通信，那么就会主动发起PING请求，服务端收到该请求后，就会触发会话激活。 会话超时检查对于会话的超时检查而言，Zookeeper使用SessionTracker来负责，SessionTracker使用单独的线程（超时检查线程）专门进行会话超时检查，即逐个一次地对会话桶中剩下的会话进行清理。如果一个会话被激活，那么Zookeeper就会将其从上一个会话桶迁移到下一个会话桶中，如ExpirationTime 1 的session n 迁移到ExpirationTime n 中，此时ExpirationTime 1中留下的所有会话都是尚未被激活的，超时检查线程就定时检查这个会话桶中所有剩下的未被迁移的会话，超时检查线程只需要在这些指定时间点（ExpirationTime 1、ExpirationTime 2…）上进行检查即可，这样提高了检查的效率，性能也非常好。 会话清理当SessionTracker的会话超时线程检查出已经过期的会话后，就开始进行会话清理工作，大致可以分为如下七步。 标记会话状态为已关闭。由于会话清理过程需要一段时间，为了保证在此期间不再处理来自该客户端的请求，SessionTracker会首先将该会话的isClosing标记为true，这样在会话清理期间接收到该客户端的心情求也无法继续处理了。 发起会话关闭请求。为了使对该会话的关闭操作在整个服务端集群都生效，Zookeeper使用了提交会话关闭请求的方式，并立即交付给PreRequestProcessor进行处理。 收集需要清理的临时节点。一旦某个会话失效后，那么和该会话相关的临时节点都需要被清理，因此，在清理之前，首先需要将服务器上所有和该会话相关的临时节点都整理出来。Zookeeper在内存数据库中会为每个会话都单独保存了一份由该会话维护的所有临时节点集合，在Zookeeper处理会话关闭请求之前，若正好有以下两类请求到达了服务端并正在处理中。 节点删除请求，删除的目标节点正好是上述临时节点中的一个。 临时节点创建请求，创建的目标节点正好是上述临时节点中的一个。 对于第一类请求，需要将所有请求对应的数据节点路径从当前临时节点列表中移出，以避免重复删除，对于第二类请求，需要将所有这些请求对应的数据节点路径添加到当前临时节点列表中，以删除这些即将被创建但是尚未保存到内存数据库中的临时节点。 添加节点删除事务变更。完成该会话相关的临时节点收集后，Zookeeper会逐个将这些临时节点转换成”节点删除”请求，并放入事务变更队列outstandingChanges中。 删除临时节点。FinalRequestProcessor会触发内存数据库，删除该会话对应的所有临时节点。 移除会话。完成节点删除后，需要将会话从SessionTracker中删除。 关闭NIOServerCnxn。最后，从NIOServerCnxnFactory找到该会话对应的NIOServerCnxn，将其关闭。 重连当客户端与服务端之间的网络连接断开时，Zookeeper客户端会自动进行反复的重连，直到最终成功连接上Zookeeper集群中的一台机器。此时，再次连接上服务端的客户端有可能处于以下两种状态之一 CONNECTED。如果在会话超时时间内重新连接上集群中一台服务器 。 EXPIRED。如果在会话超时时间以外重新连接上，那么服务端其实已经对该会话进行了会话清理操作，此时会话被视为非法会话。 在客户端与服务端之间维持的是一个长连接，在sessionTimeout时间内，服务端会不断地检测该客户端是否还处于正常连接，服务端会将客户端的每次操作视为一次有效的心跳检测来反复地进行会话激活。因此，在正常情况下，客户端会话时一直有效的。然而，当客户端与服务端之间的连接断开后，用户在客户端可能主要看到两类异常：CONNECTION_LOSS（连接断开）和SESSION_EXPIRED（会话过期）。 连接断开: CONNECTION_LOSS此时，客户端会自动从地址列表中重新逐个选取新的地址并尝试进行重新连接，直到最终成功连接上服务器。若客户端在setData时出现了CONNECTION_LOSS现象，此时客户端会收到None-Disconnected通知，同时会抛出异常。应用程序需要捕捉异常并且等待Zookeeper客户端自动完成重连，一旦重连成功，那么客户端会收到None-SyncConnected通知，之后就可以重试setData操作。 会话失效: SESSION_EXPIRED客户端与服务端断开连接后，重连时间耗时太长，超过了会话超时时间限制后没有成功连上服务器，服务器会进行会话清理，此时，客户端不知道会话已经失效，状态还是DISCONNECTED，如果客户端重新连上了服务器，此时状态为SESSION_EXPIRED，用于需要重新实例化Zookeeper对象，并且看应用的复杂情况，重新恢复临时数据。 会话转移: SESSION_MOVED客户端会话从一台服务器转移到另一台服务器，即客户端与服务端S1断开连接后，重连上了服务端S2，此时会话就从S1转移到了S2。当多个客户端使用相同的sessionId/sessionPasswd创建会话时，会收到SessionMovedException异常。因为一旦有第二个客户端连接上了服务端，就被认为是会话转移了。 参考资料 ZooKeeper 技术内幕：会话]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper数据与存储]]></title>
    <url>%2F2019%2F08%2F16%2Fzookeeper-data-and-storage%2F</url>
    <content type="text"><![CDATA[数据分类整体分为 3 类： 内存数据 磁盘数据 快照 事务日志 内存数据Zookeeper的数据模型是树结构，在内存数据库中，存储了整棵树的内容，包括所有的节点路径、节点数据、ACL信息，Zookeeper会定时将这个数据存储到磁盘上。 DataTree DataTree是内存数据存储的核心，是一个树结构，代表了内存中一份完整的数据。DataTree不包含任何与网络、客户端连接及请求处理相关的业务逻辑，是一个独立的组件。 DataNode DataNode是数据存储的最小单元，其内部保存了: 节点数据 节点 ACL 信息 节点的路径 除此之外还记录了父节点的引用和子节点列表两个属性，其也提供了对子节点列表进行操作的接口。 ZKDatabase Zookeeper的内存数据库，管理Zookeeper的所有会话、DataTree存储和事务日志。ZKDatabase会定时向磁盘dump快照数据，同时在Zookeeper启动时，会通过磁盘的事务日志和快照文件恢复成一个完整的内存数据库。 具体实现：DataTree 和 DataNode，见下图 事务日志文件存储在配置Zookeeper集群时需要配置dataDir目录，其用来存储事务日志文件。也可以为事务日志单独分配一个文件存储目录:dataLogDir。若配置dataLogDir为/home/admin/zkData/zk_log，那么Zookeeper在运行过程中会在该目录下建立一个名字为version-2的子目录，该目录确定了当前Zookeeper使用的事务日志格式版本号，当下次某个Zookeeper版本对事务日志格式进行变更时，此目录也会变更，即在version-2子目录下会生成一系列文件大小一致(64MB)的文件。 日志格式在配置好日志文件目录，启动Zookeeper后，完成如下操作 (1) 创建/test_log节点，初始值为v1。 (2) 更新/test_log节点的数据为v2。 (3) 创建/test_log/c节点，初始值为v1。 (4) 删除/test_log/c节点。 经过四步操作后，会在/log/version-2/目录下生成一个日志文件，笔者下是log.cec。 将Zookeeper下的zookeeper-3.4.6.jar和slf4j-api-1.6.1.jar复制到/log/version-2目录下，使用如下命令打开log.cec文件。 1java -classpath ./zookeeper-3.4.6.jar:./slf4j-api-1.6.1.jar org.apache.zookeeper.server.LogFormatter log.cec ZooKeeper Transactional Log File with dbid 0 txnlog format version 2 。是文件头信息，主要是事务日志的DBID和日志格式版本号。 …session 0x159…0xcec createSession 30000。表示客户端会话创建操作。 …session 0x159…0xced create ‘/test_log,… 。表示创建/test_log节点，数据内容为#7631(v1)。 …session 0x159…0xcee setData ‘/test_log,…。表示设置了/test_log节点数据，内容为#7632(v2)。 …session 0x159…0xcef create ’/test_log/c,…。表示创建节点/test_log/c。 …session 0x159…0xcf0 delete ‘/test_log/c。表示删除节点/test_log/c。 其他几行事务日志类似，具体可以对照Zookeeper源码类org.apache.zookeeper.server.LogFormatter自行分析。通过可视化这个文件，我们还注意到一点，由于这是一个记录事务操作的日志文件，因此里面没有任何读操作的日志记录。 事务日志格式如下所示: (参见$FileTxnLog$类): 12345678910111213141516171819202122232425262728293031323334353637* The format of a Transactional log is as follows:* &lt;blockquote&gt;&lt;pre&gt;* LogFile:* FileHeader TxnList ZeroPad** FileHeader: &#123;* magic 4bytes (ZKLG)* version 4bytes* dbid 8bytes* &#125;** TxnList:* Txn || Txn TxnList** Txn:* checksum Txnlen TxnHeader Record 0x42** checksum: 8bytes Adler32 is currently used* calculated across payload -- Txnlen, TxnHeader, Record and 0x42** Txnlen:* len 4bytes** TxnHeader: &#123;* sessionid 8bytes* cxid 4bytes* zxid 8bytes* time 8bytes* type 4bytes* &#125;** Record:* See Jute definition file for details on the various record types** ZeroPad:* 0 padded to EOF (filled during preallocation stage)* &lt;/pre&gt;&lt;/blockquote&gt; 日志写入FileTxnLog负责维护事务日志对外的接口，包括事务日志的写入和读取等。事务操作写入事务日志的工作主要由append方法来负责: 12public synchronized boolean append(TxnHeader hdr, Record txn) throws IOException Zookeeper的事务日志写入过程大体可以分为如下6个步骤。 确定是否有事务日志可写。当Zookeeper服务器启动完成需要进行第一次事务日志的写入，或是上一次事务日志写满时，都会处于与事务日志文件断开的状态，即Zookeeper服务器没有和任意一个日志文件相关联。因此在进行事务日志写入前，Zookeeper首先会判断FileTxnLog组件是否已经关联上一个可写的事务日志文件。若没有，则会使用该事务操作关联的ZXID作为后缀创建一个事务日志文件，同时构建事务日志的文件头信息，并立即写入这个事务日志文件中去，同时将该文件的文件流放入streamToFlush集合，该集合用来记录当前需要强制进行数据落盘的文件流。 确定事务日志文件是否需要扩容(预分配)。Zookeeper会采用磁盘空间预分配策略。当检测到当前事务日志文件剩余空间不足4096字节时，就会开始进行文件空间扩容，即在现有文件大小上，将文件增加65536KB(64MB)，然后使用”0”填充被扩容的文件空间。 事务序列化。对事务头和事务体的序列化，其中事务体又可分为会话创建事务、节点创建事务、节点删除事务、节点数据更新事务等。 生成Checksum。为保证日志文件的完整性和数据的准确性，Zookeeper在将事务日志写入文件前，会计算生成Checksum。 写入事务日志文件流。将序列化后的事务头、事务体和Checksum写入文件流中，此时并为写入到磁盘上。 事务日志刷入磁盘。由于步骤5中的缓存原因，无法实时地写入磁盘文件中，因此需要将缓存数据强制刷入磁盘。 日志截断在Zookeeper运行过程中，可能出现非Leader记录的事务ID比Leader上大，这是非法运行状态。此时，需要保证所有机器必须与该Leader的数据保持同步，即Leader会发送TRUNC命令给该机器，要求进行日志截断，Learner收到该命令后，就会删除所有包含或大于该事务ID的事务日志文件。 snapshot-数据快照数据快照是Zookeeper数据存储中非常核心的运行机制，数据快照用来记录Zookeeper服务器上某一时刻的全量内存数据内容，并将其写入指定的磁盘文件中。 文件存储与事务文件类似，Zookeeper快照文件也可以指定特定磁盘目录，通过dataDir属性来配置。若指定dataDir为/home/admin/zkData/zk_data，则在运行过程中会在该目录下创建version-2的目录，该目录确定了当前Zookeeper使用的快照数据格式版本号。在Zookeeper运行时，会生成一系列文件。 数据快照FileSnap负责维护快照数据对外的接口，包括快照数据的写入和读取等，将内存数据库写入快照数据文件其实是一个序列化过程。针对客户端的每一次事务操作，Zookeeper都会将他们记录到事务日志中，同时也会将数据变更应用到内存数据库中，Zookeeper在进行若干次事务日志记录后，将内存数据库的全量数据Dump到本地文件中，这就是数据快照。其步骤如下: 确定是否需要进行数据快照。每进行一次事务日志记录之后，Zookeeper都会检测当前是否需要进行数据快照，考虑到数据快照对于Zookeeper机器的影响，需要尽量避免Zookeeper集群中的所有机器在同一时刻进行数据快照。采用过半随机策略进行数据快照操作。 切换事务日志文件。表示当前的事务日志已经写满，需要重新创建一个新的事务日志。 创建数据快照异步线程。创建单独的异步线程来进行数据快照以避免影响Zookeeper主流程。 获取全量数据和会话信息。从ZKDatabase中获取到DataTree和会话信息。 生成快照数据文件名。Zookeeper根据当前已经提交的最大ZXID来生成数据快照文件名。 数据序列化。首先序列化文件头信息，然后再对会话信息和DataTree分别进行序列化，同时生成一个Checksum，一并写入快照数据文件中去。 关键点： 异步：异步线程生成快照文件 Fuzzy 快照： 快照文件生成过程中，仍然有新的事务提交， 因此，快照文件不是精确到某一时刻的快照文件，而是模糊的， 这就要求事务操作是幂等的，否则产生不一致。 初始化在Zookeeper服务器启动期间，首先会进行数据初始化工作，用于将存储在磁盘上的数据文件加载到Zookeeper服务器内存中。 初始化流程 数据的初始化工作是从磁盘上加载数据的过程，主要包括了从快照文件中加载快照数据和根据事务日志进行数据修正两个过程。 初始化FileTxnSnapLog。FileTxnSnapLog是Zookeeper事务日志和快照数据访问层，用于衔接上层业务和底层数据存储，底层数据包含了事务日志和快照数据两部分。FileTxnSnapLog中对应FileTxnLog和FileSnap。 初始化ZKDatabase。首先构建DataTree，同时将FileTxnSnapLog交付ZKDatabase，以便内存数据库能够对事务日志和快照数据进行访问。在ZKDatabase初始化时，DataTree也会进行相应的初始化工作，如创建一些默认结点，如/、/zookeeper、/zookeeper/quota三个节点。 创建PlayBackListener。其主要用来接收事务应用过程中的回调，在Zookeeper数据恢复后期，会有事务修正过程，此过程会回调PlayBackListener来进行对应的数据修正。 处理快照文件。此时可以从磁盘中恢复数据了，首先从快照文件开始加载。 获取最新的100个快照文件。更新时间最晚的快照文件包含了最新的全量数据。 解析快照文件。逐个解析快照文件，此时需要进行反序列化，生成DataTree和sessionsWithTimeouts，同时还会校验Checksum及快照文件的正确性。对于100个快找文件，如果正确性校验通过时，通常只会解析最新的那个快照文件。只有最新快照文件不可用时，才会逐个进行解析，直至100个快照文件全部解析完。若将100个快照文件解析完后还是无法成功恢复一个完整的DataTree和sessionWithTimeouts，此时服务器启动失败。 获取最新的ZXID。此时根据快照文件的文件名即可解析出最新的ZXID：zxid_for_snap。该ZXID代表了Zookeeper开始进行数据快照的时刻。 处理事务日志。此时服务器内存中已经有了一份近似全量的数据，现在开始通过事务日志来更新增量数据。 获取所有zxid_for_snap之后提交的事务。此时，已经可以获取快照数据的最新ZXID。只需要从事务日志中获取所有ZXID比步骤7得到的ZXID大的事务操作。 事务应用。获取大于zxid_for_snap的事务后，将其逐个应用到之前基于快照数据文件恢复出来的DataTree和sessionsWithTimeouts。每当有一个事务被应用到内存数据库中后，Zookeeper同时会回调PlayBackListener，将这事务操作记录转换成Proposal，并保存到ZKDatabase的committedLog中，以便Follower进行快速同步。 获取最新的ZXID。待所有的事务都被完整地应用到内存数据库中后，也就基本上完成了数据的初始化过程，此时再次获取ZXID，用来标识上次服务器正常运行时提交的最大事务ID。 校验epoch。epoch标识了当前Leader周期，集群机器相互通信时，会带上这个epoch以确保彼此在同一个Leader周期中。完成数据加载后，Zookeeper会从步骤11中确定ZXID中解析出事务处理的Leader周期：epochOfZxid。同时也会从磁盘的currentEpoch和acceptedEpoch文件中读取上次记录的最新的epoch值，进行校验。 数据同步ZK 集群服务器启动之后，会进行 2 个动作： 选举 Leader：分配角色 Learner 向 Leader 服务器注册，然后进行数据同步 数据同步的本质：Leader将那些没有在Learner服务器上提交过的事务请求同步给Learner。大体过程如下: 获取Learner状态。在注册Learner的最后阶段，Learner服务器会发送给Leader服务器一个ACKEPOCH数据包，Leader会从这个数据包中解析出该Learner的currentEpoch和lastZxid。 数据同步初始化。首先从Zookeeper内存数据库中提取出事务请求对应的提议缓存队列proposals，同时完成peerLastZxid(该Learner最后处理的ZXID)、minCommittedLog(Leader提议缓存队列commitedLog中最小的ZXID)、maxCommittedLog(Leader提议缓存队列commitedLog中的最大ZXID)三个ZXID值的初始化。 对于集群数据同步而言，通常分为四类，直接差异化同步(DIFF同步)、先回滚再差异化同步(TRUNC+DIFF同步)、仅回滚同步(TRUNC同步)、全量同步(SNAP同步)，在初始化阶段，Leader会优先以全量同步方式来同步数据。同时，会根据Leader和Learner之间的数据差异情况来决定最终的数据同步方式。 下面一张图，能够清晰描述发生上述同步的时机： 关键点：Learner 上的 zxid 与 Leader Proposals 中 min 和 max 的关系 直接差异化同步(DIFF同步)场景: peerLastZxid介于minCommittedLog和maxCommittedLog之间 Leader首先向这个Learner发送一个DIFF指令，用于通知Learner进入差异化数据同步阶段，Leader即将把一些Proposal同步给自己，针对每个Proposal，Leader都会通过发送PROPOSAL内容数据包和COMMIT指令数据包来完成， 先回滚再差异化同步(TRUNC+DIFF同步)场景: Leader已经将事务记录到本地事务日志中，但是没有成功发起Proposal流程 当Leader发现某个Learner包含了一条自己没有的事务记录，那么就需要该Learner进行事务回滚，回滚到Leader服务器上存在的，同时也是最接近于peerLastZxid的ZXID。 仅回滚同步(TRUNC同步)场景: peerLastZxid大于maxCommittedLog Leader要求Learner回滚到ZXID值为maxCommittedLog对应的事务操作。 全量同步(SNAP同步)场景: peerLastZxid小于minCommittedLog或peerLastZxid不等于lastProcessedZxid Leader无法直接使用提议缓存队列和Learner进行同步，因此只能进行全量同步。Leader将本机的全量内存数据同步给Learner。Leader首先向Learner发送一个SNAP指令，通知Learner即将进行全量同步，随后，Leader会从内存数据库中获取到全量的数据节点和会话超时时间记录器，将他们序列化后传输给Learner。Learner接收到该全量数据后，会对其反序列化后载入到内存数据库中。 参考资料 ZooKeeper 技术内幕：数据的存储 【Zookeeper】源码分析之持久化（一）之FileTxnLog 【Zookeeper】源码分析之持久化（二）之FileSnap 【Zookeeper】源码分析之持久化（三）之FileTxnSnapLog]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper系统模型]]></title>
    <url>%2F2019%2F08%2F16%2Fzookeeper-system-model%2F</url>
    <content type="text"><![CDATA[数据模型Zookeeper的数据节点称为ZNode，ZNode是Zookeeper中数据的最小单元，每个ZNode都可以保存数据，同时还可以挂载子节点，因此构成了一个层次化的命名空间，称为树。 在Zookeeper中，事务是指能够改变Zookeeper服务器状态的操作，一般包括节点创建与删除，数据节点内容更新和客户端会话创建与失效，对于每个事务请求，Zookeeper都会为其分配一个全局唯一的事务ID，用 zxid表示，通常是64位的数字，每个 zxid对应一次更新操作，从这些ZXID中可以间接地识别出Zookeeper处理这些更新操作请求的全局顺序。 节点特性在Zookeeper中，每个数据节点都是由生命周期的，类型不同则会不同的生命周期，节点类型可以分为持久节点（PERSISTENT）、临时节点（EPHEMERAL）、顺序节点（SEQUENTIAL）三大类，可以通过组合生成如下四种类型节点 持久节点（PERSISTENT）: 节点创建后便一直存在于Zookeeper服务器上，直到有删除操作来主动清除该节点。 持久顺序节点（PERSISTENT_SEQUENTIAL）: 相比持久节点，其新增了顺序特性，每个父节点都会为它的第一级子节点维护一份顺序，用于记录每个子节点创建的先后顺序。在创建节点时，会自动添加一个数字后缀，作为新的节点名，该数字后缀的上限是整形的最大值。 临时节点（EPEMERAL）: 临时节点的生命周期与客户端会话绑定，客户端失效，节点会被自动清理。同时，Zookeeper规定不能基于临时节点来创建子节点，即临时节点只能作为叶子节点。 临时顺序节点（EPEMERAL_SEQUENTIAL）: 在临时节点的基础添加了顺序特性。 每个节点除了存储数据外，还存储了节点本身的一些状态信息，可通过get命令获取。 版本–保证分布式数据原子性操作每个数据节点都具有三种类型的版本信息，对数据节点的任何更新操作都会引起版本号的变化。 每个 ZNode 都有 3 类版本信息： version：当前数据节点数据内容的版本号 cversion：子节点列表，当前数据子节点的版本号，只会感知子节点列表变更信息，新增子节点、删除子节点，而不会感知子节点数据内容的变更 aversion：当前数据节点ACL变更版本号 上述各版本号都是表示修改次数，如version为1表示对数据节点的内容变更了一次。即使前后两次变更并没有改变数据内容，version的值仍然会改变。version可以用于写入验证，类似于CAS。 版本号，什么作用？目标：解决 ZNode 的并发更新问题，实现 CAS（Compare And Switch）乐观锁。 补充：乐观锁事务，分为 3 个典型阶段： 数据读取 写入校验 数据写入 Watcher–数据变更通知Zookeeper使用Watcher机制实现分布式数据的发布/订阅功能。 实现原理ZooKeeper 允许客户端向服务端注册一个 Watcher 监听，当服务端的一些指定事件触发了这个 Watcher，那么就会向指定客户端发送一个事件通知来实现分布式的通知功能。 ZooKeeper 的 Watcher 机制主要包括客户端线程、客户端 WatchManager 和 ZooKeeper 服务器三部分。在具体工作流程上，简单地讲，客户端在向 ZooKeeper 服务器注册 Watcher 的同时，会将 Watcher 对象存储在客户端的 WatchManager 中。当 ZooKeeper 服务器端触发 Watcher 事件后，会向客户端发送通知，客户端线程从 WatchManager 中取出对应的 Watcher 对象来执行回调逻辑。如下面WatchManager所示，WatchManager 创建了一个 HashMap，这个 HashMap 被用来存放 Watcher 对象。 WatchManager 类 12345678910111213141516171819202122232425private final HashMap&lt;String, HashSet&lt;Watcher&gt;&gt; watchTable = new HashMap&lt;String, HashSet&lt;Watcher&gt;&gt;();private final HashMap&lt;Watcher, HashSet&lt;String&gt;&gt; watch2Paths = new HashMap&lt;Watcher, HashSet&lt;String&gt;&gt;();synchronized void addWatch(String path, Watcher watcher) &#123; HashSet&lt;Watcher&gt; list = watchTable.get(path); if (list == null) &#123; // don't waste memory if there are few watches on a node // rehash when the 4th entry is added, doubling size thereafter // seems like a good compromise list = new HashSet&lt;Watcher&gt;(4); watchTable.put(path, list); &#125; list.add(watcher); HashSet&lt;String&gt; paths = watch2Paths.get(watcher); if (paths == null) &#123; // cnxns typically have many watches, so use default cap here paths = new HashSet&lt;String&gt;(); watch2Paths.put(watcher, paths); &#125; paths.add(path);&#125; 整个 Watcher 注册和通知流程如下图所示: 总体框图 说明： Watcher，接口类型，其定义了process方法，需子类实现。 Event，接口类型，Watcher的内部类，无任何方法。 KeeperState，枚举类型，Event的内部类，表示Zookeeper所处的状态。 EventType，枚举类型，Event的内部类，表示Zookeeper中发生的事件类型。 WatchedEvent，表示对ZooKeeper上发生变化后的反馈，包含了KeeperState和EventType。 ClientWatchManager，接口类型，表示客户端的Watcher管理者，其定义了materialized方法，需子类实现。 ZKWatchManager，Zookeeper的内部类，继承ClientWatchManager。 MyWatcher，ZooKeeperMain的内部类，继承Watcher。 ServerCnxn，接口类型，继承Watcher，表示客户端与服务端的一个连接。 WatchManager，管理Watcher。 Watcher 接口Watcher 的理念是启动一个客户端去接收从 ZooKeeper 服务端发过来的消息并且同步地处理这些信息。ZooKeeper 的 Java API 提供了公共接口 Watcher，具体操作类通过实现这个接口相关的方法来实现从所连接的 ZooKeeper 服务端接收数据。如果要处理这个消息，需要为客户端注册一个 CallBack（回调）对象。Watcher 接口定义在 org.apache.zookeeper 包里面，如下所示: 123456789101112131415161718192021222324252627282930313233343536373839404142public interface Watcher &#123; // Event、WatchType定义 public interface Event &#123; /** * Enumeration of states the ZooKeeper may be at the event */ @InterfaceAudience.Public public enum KeeperState &#123; /** Unused, this state is never generated by the server */ @Deprecated Unknown (-1), /** The client is in the disconnected state - it is not connected * to any server in the ensemble. */ Disconnected (0), /** Unused, this state is never generated by the server */ @Deprecated NoSyncConnected (1), ... /** * Enumeration of types of events that may occur on the ZooKeeper */ @InterfaceAudience.Public public enum EventType &#123; None (-1), NodeCreated (1), NodeDeleted (2), NodeDataChanged (3), NodeChildrenChanged (4), DataWatchRemoved (5), ChildWatchRemoved (6); private final int intValue; // Integer representation of value // for sending over wire abstract public void process(WatchedEvent event);&#125; 在 Watcher 接口里面，除了回调函数 process 以外，还包含 KeeperState 和 EventType 两个枚举类，分别代表了通知状态和事件类型，如下图所示。 process 方法是 Watcher 接口中的一个回调方法，当 ZooKeeper 向客户端发送一个 Watcher 事件通知时，客户端就会对相应的 process 方法进行回调，从而实现对事件的处理。 process 方法包含 WatcherEvent 类型的参数，WatchedEvent 包含了每一个事件的三个基本属性：通知状态（KeeperState）、事件类型（EventType）和节点路径（Path），ZooKeeper 使用 WatchedEvent 对象来封装服务端事件并传递给 Watcher，从而方便回调方法 process 对服务端事件进行处理。 WatchedEvent 和 WatcherEvent 都表示的是同一个事物，都是对一个服务端事件的封装。不同的是，WatchedEvent 是一个逻辑事件，用于服务端和客户端程序执行过程中所需的逻辑对象，而 WatcherEvent 因为实现了序列化接口，因此可以用于网络传输。 服务端在线程 WatchedEvent 事件之后，会调用 getWrapper 方法将自己包装成一个可序列化的 WatcherEvent 事件，如清单 7 所示，以便通过网络传输到客户端。客户端在接收到服务端的这个事件对象后，首先会将 WatcherEvent 事件还原成一个 WatchedEvent 事件，并传递给 process 方法处理，回调方法 process 根据入参就能够解析出完整的服务端事件了。 1234567891011121314151617181920public class WatchedEvent &#123; final private KeeperState keeperState; final private EventType eventType; private String path; /** * Create a WatchedEvent with specified type, state and path */ public WatchedEvent(EventType eventType, KeeperState keeperState, String path) &#123; this.keeperState = keeperState; this.eventType = eventType; this.path = path; &#125; // 可序列化的事件 public WatcherEvent getWrapper() &#123; return new WatcherEvent(eventType.getIntValue(), keeperState.getIntValue(), path); &#125; 客户端注册 Watcher 流程创建一个Zookeeper客户端对象实例时，可以向构造方法中传入一个默认的Watcher(如: zk = new ZooKeeper(HostPort,2000,connectionWatcher)): 12345678910public class ZooKeeper implements AutoCloseable &#123; // 构造函数 public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher) throws IOException &#123; this(connectString, sessionTimeout, watcher, false); &#125; ... &#125; 第三个参数是连接到 ZooKeeper 服务端的 connectionWatcher 事件监听，这个 Watcher 将作为整个 ZooKeeper 会话期间的默认 Watcher，会一直被保存在客户端 ZKWatchManager 的 defaultWatcher 里面。 这里我们以getData这个接口为例来说明。getData接口用于获取指定节点的数据内容: 12345678910111213141516171819202122232425262728293031public byte[] getData(final String path, Watcher watcher, Stat stat) throws KeeperException, InterruptedException &#123; final String clientPath = path; PathUtils.validatePath(clientPath); // the watch contains the un-chroot path WatchRegistration wcb = null; if (watcher != null) &#123; wcb = new DataWatchRegistration(watcher, clientPath); &#125; final String serverPath = prependChroot(clientPath); RequestHeader h = new RequestHeader(); h.setType(ZooDefs.OpCode.getData); GetDataRequest request = new GetDataRequest(); request.setPath(serverPath); // 对当前客户端请求进行标记，将其设置为使用 Watcher 监听 request.setWatch(watcher != null); GetDataResponse response = new GetDataResponse(); ReplyHeader r = cnxn.submitRequest(h, request, response, wcb); if (r.getErr() != 0) &#123; throw KeeperException.create(KeeperException.Code.get(r.getErr()), clientPath); &#125; if (stat != null) &#123; DataTree.copyStat(response.getStat(), stat); &#125; return response.getData(); &#125; 客户端的请求基本都是在 ClientCnxn 里面进行操作，当收到请求后，客户端会对当前客户端请求进行标记，将其设置为使用 Watcher 监听，同时会封装一个 Watcher 的注册信息 WatchRegistration 对象，用于暂时保存数据节点的路径和 Watcher 的对应关系。 在 ZooKeeper 中，Packet 是一个最小的通信协议单元，即数据包。Pakcet 用于进行客户端与服务端之间的网络传输，任何需要传输的对象都需要包装成一个 Packet 对象。在 ClientCnxn 中 WatchRegistration 也会被封装到 Pakcet 中，然后由 SendThread 线程调用 queuePacke 方法把 Packet 放入发送队列中等待客户端发送，这又是一个异步过程，分布式系统采用异步通信是一个普遍认同的观念。随后，SendThread 线程会通过 readResponse 方法接收来自服务端的响应，异步地调用 finishPacket 方法从 Packet 中取出对应的 Watcher 并注册到 ZKWatchManager 中去: 1234567891011121314151617protected void finishPacket(Packet p) &#123; int err = p.replyHeader.getErr(); if (p.watchRegistration != null) &#123; p.watchRegistration.register(err); &#125; ... if (p.cb == null) &#123; synchronized (p) &#123; p.finished = true; p.notifyAll(); &#125; &#125; else &#123; p.finished = true; eventThread.queuePacket(p); &#125; &#125; 除了上面介绍的方式以外，ZooKeeper 客户端也可以通过 getData、getChildren 和 exist 三个接口来向 ZooKeeper 服务器注册 Watcher，无论使用哪种方式，注册 Watcher 的工作原理是一致的。如下所示，getChildren 方法调用了 WatchManager 类的 addWatch 方法添加了 watcher 事件。 12345678910111213141516171819202122232425262728293031public List&lt;String&gt; getChildren(final String path, Watcher watcher, Stat stat) throws KeeperException, InterruptedException &#123; final String clientPath = path; PathUtils.validatePath(clientPath); // the watch contains the un-chroot path WatchRegistration wcb = null; if (watcher != null) &#123; wcb = new ChildWatchRegistration(watcher, clientPath); &#125; final String serverPath = prependChroot(clientPath); RequestHeader h = new RequestHeader(); h.setType(ZooDefs.OpCode.getChildren2); GetChildren2Request request = new GetChildren2Request(); request.setPath(serverPath); request.setWatch(watcher != null); GetChildren2Response response = new GetChildren2Response(); ReplyHeader r = cnxn.submitRequest(h, request, response, wcb); if (r.getErr() != 0) &#123; throw KeeperException.create(KeeperException.Code.get(r.getErr()), clientPath); &#125; if (stat != null) &#123; DataTree.copyStat(response.getStat(), stat); &#125; return response.getChildren(); &#125; 如下代码所示，现在需要从这个封装对象中再次提取出 Watcher 对象来，在 register 方法里面，客户端将 Watcher 对象转交给 ZKWatchManager，并最终保存在一个 Map 类型的数据结构 dataWatches 里面，用于将数据节点的路径和 Watcher 对象进行一一映射后管理起来。 12345678910111213public void register(int rc) &#123; if (shouldAddWatch(rc)) &#123; Map&lt;String, Set&lt;Watcher&gt;&gt; watches = getWatches(rc); synchronized(watches) &#123; Set&lt;Watcher&gt; watchers = watches.get(clientPath); if (watchers == null) &#123; watchers = new HashSet&lt;Watcher&gt;(); watches.put(clientPath, watchers); &#125; watchers.add(watcher); &#125; &#125; &#125; 注意，WatcherRegistation 除了 Header 和 request 两个属性被传递到了服务端，其他都没有到服务端，否则服务端就容易出现内存紧张甚至溢出的危险，因为数据量太大了。这就是 ZooKeeper 为什么适用于分布式环境的原因，它在网络中传输的是消息，而不是数据包实体。 服务端处理 Watcher 流程 注意，以下所有代码均为精简版，去除了日志、判断分支，只在源码上保留了主线代码。 FinalRequestProcessor 类接收到客户端请求后，会调用 processRequest 方法进行处理，会进一步转向 ZooKeeperServer 的 processRequest 进行进一步处理，处理结由 ZKDatabase 类返回: 12345678910111213public class FinalRequestProcessor implements RequestProcessor &#123; private static final Logger LOG = LoggerFactory.getLogger(FinalRequestProcessor.class); ZooKeeperServer zks; public void processRequest(Request request) &#123; ... ProcessTxnResult rc = null; synchronized (zks.outstandingChanges) &#123; // Need to process local session requests rc = zks.processTxn(request); ...&#125; 1234567891011121314151617181920212223242526272829303132public class ZooKeeperServer implements SessionExpirer, ServerStats.Provider &#123; ... private ProcessTxnResult processTxn(Request request, TxnHeader hdr, Record txn) &#123; ProcessTxnResult rc; int opCode = request != null ? request.type : hdr.getType(); long sessionId = request != null ? request.sessionId : hdr.getClientId(); if (hdr != null) &#123; rc = getZKDatabase().processTxn(hdr, txn); &#125; else &#123; rc = new ProcessTxnResult(); &#125; if (opCode == OpCode.createSession) &#123; if (hdr != null &amp;&amp; txn instanceof CreateSessionTxn) &#123; CreateSessionTxn cst = (CreateSessionTxn) txn; sessionTracker.addGlobalSession(sessionId, cst.getTimeOut()); &#125; else if (request != null &amp;&amp; request.isLocalSession()) &#123; request.request.rewind(); int timeout = request.request.getInt(); request.request.rewind(); sessionTracker.addSession(request.sessionId, timeout); &#125; else &#123; LOG.warn("*****&gt;&gt;&gt;&gt;&gt; Got " + txn.getClass() + " " + txn.toString()); &#125; &#125; else if (opCode == OpCode.closeSession) &#123; sessionTracker.removeSession(sessionId); &#125; return rc; &#125;&#125; 123456789101112131415public class ZKDatabase &#123; private static final Logger LOG = LoggerFactory.getLogger(ZKDatabase.class); /** * the process txn on the data * @param hdr the txnheader for the txn * @param txn the transaction that needs to be processed * @return the result of processing the transaction on this * datatree/zkdatabase */ public ProcessTxnResult processTxn(TxnHeader hdr, Record txn) &#123; return dataTree.processTxn(hdr, txn); &#125;&#125; ServerCnxn存储对于注册 Watcher 请求，FinalRequestProcessor 的 ProcessRequest 方法会判断当前请求是否需要注册 Watcher，如果为 true，就会将当前的 ServerCnxn 对象和数据节点路径传入 getData 方法中去: 123456789101112131415161718case OpCode.getData: &#123; lastOp = "GETD"; GetDataRequest getDataRequest = new GetDataRequest(); ByteBufferInputStream.byteBuffer2Record(request.request, getDataRequest); DataNode n = zks.getZKDatabase().getNode(getDataRequest.getPath()); if (n == null) &#123; throw new KeeperException.NoNodeException(); &#125; PrepRequestProcessor.checkACL(zks, zks.getZKDatabase().aclForNode(n), ZooDefs.Perms.READ, request.authInfo); Stat stat = new Stat(); byte b[] = zks.getZKDatabase().getData(getDataRequest.getPath(), stat, getDataRequest.getWatch() ? cnxn : null); rsp = new GetDataResponse(b, stat); break; &#125; ServerCnxn 是一个 ZooKeeper 客户端和服务器之间的连接接口，代表了一个客户端和服务器的连接，我们后面讲到的 process 回调方法，实际上也是从这里回调的，所以可以把 ServerCnxn 看作是一个 Watcher 对象。数据节点的节点路径和 ServerCnxn 最终会被存储在 WatchManager 的 watchTable 和 watch2Paths 中。 如前所述，WatchManager 负责 Watcher 事件的触发，它是一个统称，在服务端 DataTree 会托管两个 WatchManager，分别是 dataWatches 和 childWatches，分别对应数据变更 Watcher 和子节点变更 Watcher。 123456789class WatchManager &#123; private static final Logger LOG = LoggerFactory.getLogger(WatchManager.class); private final HashMap&lt;String, HashSet&lt;Watcher&gt;&gt; watchTable = new HashMap&lt;String, HashSet&lt;Watcher&gt;&gt;(); private final HashMap&lt;Watcher, HashSet&lt;String&gt;&gt; watch2Paths = new HashMap&lt;Watcher, HashSet&lt;String&gt;&gt;();&#125; Watcher触发当发生 Create、Delete、NodeChange（数据变更）这样的事件后，DataTree 会调用相应方法去触发 WatchManager 的 triggerWatch 方法，该方法返回 ZNODE 的信息，自此进入到回调本地 process 的序列： 123456789101112131415161718192021222324public class DataTree &#123; private static final Logger LOG = LoggerFactory.getLogger(DataTree.class); public ProcessTxnResult processTxn(TxnHeader header, Record txn, boolean isSubTxn) &#123; ProcessTxnResult rc = new ProcessTxnResult(); try &#123; rc.clientId = header.getClientId(); rc.cxid = header.getCxid(); rc.zxid = header.getZxid(); rc.type = header.getType(); rc.err = 0; rc.multiResult = null; switch (header.getType()) &#123; case OpCode.setData: SetDataTxn setDataTxn = (SetDataTxn) txn; rc.path = setDataTxn.getPath(); rc.stat = setData(setDataTxn.getPath(), setDataTxn .getData(), setDataTxn.getVersion(), header .getZxid(), header.getTime()); break;&#125; 12345678910111213141516171819202122232425public Stat setData(String path, byte data[], int version, long zxid, long time) throws KeeperException.NoNodeException &#123; Stat s = new Stat(); DataNode n = nodes.get(path); if (n == null) &#123; throw new KeeperException.NoNodeException(); &#125; byte lastdata[] = null; synchronized (n) &#123; lastdata = n.data; n.data = data; n.stat.setMtime(time); n.stat.setMzxid(zxid); n.stat.setVersion(version); n.copyStat(s); &#125; // now update if the path is in a quota subtree. String lastPrefix = getMaxPrefixWithQuota(path); if(lastPrefix != null) &#123; this.updateBytes(lastPrefix, (data == null ? 0 : data.length) - (lastdata == null ? 0 : lastdata.length)); &#125; dataWatches.triggerWatch(path, EventType.NodeDataChanged); return s;&#125; 1234567891011121314151617181920212223242526272829Set&lt;Watcher&gt; triggerWatch(String path, EventType type, Set&lt;Watcher&gt; supress) &#123; WatchedEvent e = new WatchedEvent(type, KeeperState.SyncConnected, path); HashSet&lt;Watcher&gt; watchers; synchronized (this) &#123; watchers = watchTable.remove(path); if (watchers == null || watchers.isEmpty()) &#123; if (LOG.isTraceEnabled()) &#123; ZooTrace.logTraceMessage(LOG, ZooTrace.EVENT_DELIVERY_TRACE_MASK, "No watchers for " + path); &#125; return null; &#125; for (Watcher w : watchers) &#123; HashSet&lt;String&gt; paths = watch2Paths.get(w); if (paths != null) &#123; paths.remove(path); &#125; &#125; &#125; for (Watcher w : watchers) &#123; if (supress != null &amp;&amp; supress.contains(w)) &#123; continue; &#125; w.process(e); &#125; return watchers; &#125; 从上面的代码我们可以总结出，如果想要处理一个 Watcher，需要执行的步骤如下所示： 将事件类型（EventType）、通知状态（WatchedEvent）、节点路径封装成一个 WatchedEvent 对象。 根据数据节点的节点路径从 watchTable 里面取出对应的 Watcher。如果没有找到 Watcher 对象，说明没有任何客户端在该数据节点上注册过 Watcher，直接退出。如果找到了 Watcher 就将其提取出来，同时会直接从 watchTable 和 watch2Paths 里删除 Watcher，即 Watcher 是一次性的，触发一次就失效了。 对于需要注册 Watcher 的请求，ZooKeeper 会把请求对应的 ServerCnxn 作为一个 Watcher 存储，所以这里调用的 process 方法实质上是 ServerCnxn 的对应方法，如下所示: NIOServerCnxn 类代码 1234567891011121314public void process(WatchedEvent event) &#123; ReplyHeader h = new ReplyHeader(-1, -1L, 0); if (LOG.isTraceEnabled()) &#123; ZooTrace.logTraceMessage(LOG, ZooTrace.EVENT_DELIVERY_TRACE_MASK, &quot;Deliver event &quot; + event + &quot; to 0x&quot; + Long.toHexString(this.sessionId) + &quot; through &quot; + this); &#125; // Convert WatchedEvent to a type that can be sent over the wire WatcherEvent e = event.getWrapper(); sendResponse(h, e, &quot;notification&quot;); &#125; 从上面的代码片段中，我们可以看出在请求头标记“-1”表示当前是一个通知，将 WatchedEvent 包装成 WatcherEvent 用于网络传输序列化，向客户端发送通知，真正的回调方法在客户端。 客户端回调Watcher客户端收到消息后，会调用 ClientCnxn 的 SendThread.readResponse 方法来进行统一处理，如清单所示。如果响应头 replyHdr 中标识的 Xid 为 02，表示是 ping，如果为-4，表示是验证包，如果是-1，表示这是一个通知类型的响应，然后进行反序列化、处理 chrootPath、还原 WatchedEvent、回调 Watcher 等步骤，其中回调 Watcher 步骤将 WacthedEvent 对象交给 EventThread 线程，在下一个轮询周期中进行 Watcher 回调。 123456789101112131415161718192021222324class SendThread extends ZooKeeperThread &#123; private long lastPingSentNs; private final ClientCnxnSocket clientCnxnSocket; private Random r = new Random(); private boolean isFirstConnect = true; void readResponse(ByteBuffer incomingBuffer) throws IOException &#123; ByteBufferInputStream bbis = new ByteBufferInputStream( incomingBuffer); BinaryInputArchive bbia = BinaryInputArchive.getArchive(bbis); ReplyHeader replyHdr = new ReplyHeader(); replyHdr.deserialize(bbia, "header"); if (replyHdr.getXid() == -2) &#123; // -2 is the xid for pings if (LOG.isDebugEnabled()) &#123; LOG.debug("Got ping response for sessionid: 0x" + Long.toHexString(sessionId) + " after " + ((System.nanoTime() - lastPingSentNs) / 1000000) + "ms"); &#125; return; &#125; SendThread 接收到服务端的通知事件后，会通过调用 EventThread 类的 queueEvent 方法将事件传给 EventThread 线程，queueEvent 方法根据该通知事件，从 ZKWatchManager 中取出所有相关的 Watcher: 12345678910111213141516171819202122232425class EventThread extends ZooKeeperThread &#123; private final LinkedBlockingQueue&lt;Object&gt; waitingEvents = new LinkedBlockingQueue&lt;Object&gt;(); ... private void queueEvent(WatchedEvent event, Set&lt;Watcher&gt; materializedWatchers) &#123; if (event.getType() == EventType.None &amp;&amp; sessionState == event.getState()) &#123; return; &#125; sessionState = event.getState(); final Set&lt;Watcher&gt; watchers; if (materializedWatchers == null) &#123; // materialize the watchers based on the event watchers = watcher.materialize(event.getState(), event.getType(), event.getPath()); &#125; else &#123; watchers = new HashSet&lt;Watcher&gt;(); watchers.addAll(materializedWatchers); &#125; WatcherSetEventPair pair = new WatcherSetEventPair(watchers, event); // queue the pair (watch set &amp; event) for later processing waitingEvents.add(pair); &#125; 客户端在识别出事件类型 EventType 之后，会从相应的 Watcher 存储中删除对应的 Watcher，获取到相关的 Watcher 之后，会将其放入 waitingEvents 队列，该队列从字面上就能理解是一个待处理队列，线程的 run 方法会不断对该该队列进行处理，这就是一种异步处理思维的实现。 ZKWatchManager 取出 Watcher 1234567891011121314151617public Set&lt;Watcher&gt; materialize(Watcher.Event.KeeperState state, Watcher.Event.EventType type, String clientPath) &#123; Set&lt;Watcher&gt; result = new HashSet&lt;Watcher&gt;(); switch (type) &#123; case NodeDataChanged: case NodeCreated: synchronized (dataWatches) &#123; addTo(dataWatches.remove(clientPath), result); &#125; synchronized (existWatches) &#123; addTo(existWatches.remove(clientPath), result); &#125; break;&#125; EventThread 线程的 run 方法 12345678910111213141516171819202122public void run() &#123; try &#123; isRunning = true; while (true) &#123; Object event = waitingEvents.take(); if (event == eventOfDeath) &#123; wasKilled = true; &#125; else &#123; processEvent(event); &#125; if (wasKilled) synchronized (waitingEvents) &#123; if (waitingEvents.isEmpty()) &#123; isRunning = false; break; &#125; &#125; &#125; &#125; catch (InterruptedException e) &#123; LOG.error("Event thread exiting due to interruption", e); &#125; &#125; ZooKeeper Watcher 特性总结1. 注册只能确保一次消费 无论是服务端还是客户端，一旦一个 Watcher 被触发，ZooKeeper 都会将其从相应的存储中移除。因此，开发人员在 Watcher 的使用上要记住的一点是需要反复注册。这样的设计有效地减轻了服务端的压力。如果注册一个 Watcher 之后一直有效，那么针对那些更新非常频繁的节点，服务端会不断地向客户端发送事件通知，这无论对于网络还是服务端性能的影响都非常大。 2. 客户端串行执行 客户端 Watcher 回调的过程是一个串行同步的过程，这为我们保证了顺序，同时，需要开发人员注意的一点是，千万不要因为一个 Watcher 的处理逻辑影响了整个客户端的 Watcher 回调。 3. 轻量级设计 WatchedEvent 是 ZooKeeper 整个 Watcher 通知机制的最小通知单元，这个数据结构中只包含三部分的内容：通知状态、事件类型和节点路径。也就是说，Watcher 通知非常简单，只会告诉客户端发生了事件，而不会说明事件的具体内容。例如针对 NodeDataChanged 事件，ZooKeeper 的 Watcher 只会通知客户指定数据节点的数据内容发生了变更，而对于原始数据以及变更后的新数据都无法从这个事件中直接获取到，而是需要客户端主动重新去获取数据，这也是 ZooKeeper 的 Watcher 机制的一个非常重要的特性。另外，客户端向服务端注册 Watcher 的时候，并不会把客户端真实的 Watcher 对象传递到服务端，仅仅只是在客户端请求中使用 boolean 类型属性进行了标记，同时服务端也仅仅只是保存了当前连接的 ServerCnxn 对象。这样轻量级的 Watcher 机制设计，在网络开销和服务端内存开销上都是非常廉价的。 ACL–保障数据的安全Zookeeper内部存储了分布式系统运行时状态的元数据，这些元数据会直接影响基于Zookeeper进行构造的分布式系统的运行状态，如何保障系统中数据的安全，从而避免因误操作而带来的数据随意变更而导致的数据库异常十分重要，Zookeeper提供了一套完善的ACL权限控制机制来保障数据的安全。 我们可以从三个方面来理解ACL机制：权限模式（Scheme）、授权对象（ID）、权限（Permission），通常使用：schema:id:permission 来标识一个有效的ACL信息。 权限模式 Schema常用的权限模式如下： IPIP： 使用 IP 识别用户，可以精确匹配 IP，也可以匹配到 IP 段 ip:168.192.1.23 ：精确匹配到 IP ip:168.192.0.1/24：模糊匹配 IP 段，168.192.0.* Note： IP 地址是 32 位，十进制表示 4 个十进制；IP 段，表示前面多少位相同。 DigestDigest，类似 username:password，用户名和密码。便于区分不同应用来进行权限控制。Zookeeper会对其进行SHA-1加密和BASE64编码。 World没有密码，对所有用户都开放权限。可以看作特殊的 Digest 模式。 Super只有超级用户，才有权限，也可看作特殊的 Digest 模式。 授权对象 ID授权对象是指，权限赋予的用户或者一个实体，例如：IP 地址或者机器。 授权模式 schema 与 授权对象 ID 之间关系： 权限 PermissionZooKeeper 中数据节点的权限分为 5 类： Create（C）：创建子节点 Delete（D）：删除子节点 Read（R）：读取当前节点，以及子节点列表 Write（W）：更新当前节点 Admin（A）：当前节点的 ACL 管理 附录UGO 权限控制UGO，（User，Group，Others），是（用户，组，权限）的简称。 Linux\Unix 文件系统，采用 UGO 的权限控制，针对文件/文件夹的创建者，创建者所在组以及其他用户，分别分配不同的权限。 UGO 的权限控制，是基于用户的，跟系统的用户体系严格绑定。 ACL 权限控制ACL，Access Control List，访问权限列表。 从 data 角度出发，赋予权限。 Note： 目前绝大部分 Unix 系统都支持了 ACL，Linux 内核从 2.6+ 也开始支持 ACL。 RBAC 权限控制RBAC，(Role-Based Access Control )基于角色的访问控制。 一个用户拥有若干角色，每一个角色拥有若干权限。 构成 用户-角色-权限 的授权模型。 在这种模型中，用户与角色之间，角色与权限之间，一般者是多对多的关系。 参考资料 从Paxos到Zookeeper分布式一致性原理与实践 Apache ZooKeeper Watcher 机制源码解释 ZooKeeper 技术内幕：ACL 权限访问控制 源码分析之Watcher机制（三）之ZooKeeper]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper选举机制及源码解析]]></title>
    <url>%2F2019%2F08%2F15%2Fzookeeper-leader-election%2F</url>
    <content type="text"><![CDATA[Zookeeper选举概述时机Leader选举是保证分布式数据一致性的关键所在。当Zookeeper集群中的一台服务器出现以下两种情况之一时，需要进入Leader选举: (1) 服务器初始化启动。 (2) 服务器运行期间无法和Leader保持连接。 在Zookeeper运行期间，Leader与非Leader服务器各司其职，即便当有非Leader服务器宕机或新加入，此时也不会影响Leader，但是一旦Leader服务器挂了，那么整个集群将暂停对外服务，进入新一轮Leader选举，其过程和启动时期的Leader选举过程基本一致。假设正在运行的有Server1、Server2、Server3三台服务器，当前Leader是Server2，若某一时刻Leader挂了，此时便开始Leader选举。 过程若进行Leader选举，则至少需要两台机器，这里选取3台机器组成的服务器集群为例。在集群初始化阶段，当有一台服务器Server1启动时，其单独无法进行和完成Leader选举，当第二台服务器Server2启动时，此时两台机器可以相互通信，每台机器都试图找到Leader，于是进入Leader选举过程。Leader 选举过程，本质就是广播优先级消息的过程，选出数据最新的服务节点，选出优先级最高的服务节点，基本步骤： 每个Server发出一个投票。Server1和Server2都会将自己作为Leader服务器来进行投票，广播自己的优先级标识 (sid，zxid)。例如服务器启动时，此时Server1的投票为(1, 0)，Server2的投票为(2, 0)，然后各自将这个投票发给集群中其他机器。 接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。 处理投票。服务器节点收到其他广播消息后，跟自己的优先级对比，自己优先级低，则变更当前节点投票的优先级(sid，zxid) ，并广播变更后的结果，优先级PK规则: 优先比较 zxid （事务 ID），其次比较sid（服务器ID） sid (服务器 ID) 是节点配置文件中设定的 对于Server1而言，它的投票是(1, 0)，接收Server2的投票为(2, 0)，首先会比较两者的ZXID，均为0，再比较myid，此时Server2的myid最大，于是更新自己的投票为(2, 0)，然后重新投票，对于Server2而言，其无须更新自己的投票，只是再次向集群中所有机器发出上一次投票信息即可。 统计投票。当任意一个服务器节点收到的投票数，超过了法定数量(quorum)亦即集群规模大小的半数以上，则升级为 Leader，并广播结果。对于Server1、Server2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出了Leader。 改变服务器状态。一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。 Leader选举算法分析在3.4.0后的Zookeeper的版本只保留了TCP版本的FastLeaderElection选举算法。当一台机器进入Leader选举时，当前集群可能会处于以下两种状态 集群中已经存在Leader。 集群中不存在Leader。 对于集群中已经存在Leader而言，此种情况一般都是某台机器启动得较晚，在其启动之前，集群已经在正常工作，对这种情况，该机器试图去选举Leader时，会被告知当前服务器的Leader信息，对于该机器而言，仅仅需要和Leader机器建立起连接，并进行状态同步即可。而在集群中不存在Leader情况下则会相对复杂，其步骤如下。 第一次投票无论哪种导致进行Leader选举，集群的所有机器都处于试图选举出一个Leader的状态，即LOOKING状态，LOOKING机器会向所有其他机器发送消息，该消息称为投票。投票中包含了(sid，zxid) 形式来标识一次投票信息。假定Zookeeper由5台机器组成，sid分别为1、2、3、4、5，zxid分别为9、9、9、8、8，并且此时sid为2的机器是Leader机器，某一时刻，1、2所在机器出现故障，因此集群开始进行Leader选举。在第一次投票时，每台机器都会将自己作为投票对象，于是sid为3、4、5的机器投票情况分别为(3, 9)，(4, 8)， (5, 8)。 变更投票每台机器发出投票后，也会收到其他机器的投票，每台机器会根据一定规则来处理收到的其他机器的投票，并以此来决定是否需要变更自己的投票，这个规则也是整个Leader选举算法的核心所在，其中术语描述如下: vote_sid：接收到的投票中所推举Leader服务器的sid。 vote_zxid：接收到的投票中所推举Leader服务器的zxid。 self_sid：当前服务器自己的sid。 self_zxid：当前服务器自己的zxid。 每次对收到的投票的处理，都是对(vote_sid, vote_zxid)和(self_sid, self_zxid)对比的过程。 规则一：如果vote_zxid大于self_zxid，就认可当前收到的投票，并再次将该投票发送出去。 规则二：如果vote_zxid小于self_zxid，那么坚持自己的投票，不做任何变更。 规则三：如果vote_zxid等于self_zxid，那么就对比两者的SID，如果vote_sid大于self_sid，那么就认可当前收到的投票，并再次将该投票发送出去。 规则四：如果vote_zxid等于self_zxid，并且vote_sid小于self_sid，那么坚持自己的投票，不做任何变更。 结合上面规则，给出下面的集群变更过程。 确定Leader经过第二轮投票后，集群中的每台机器都会再次接收到其他机器的投票，然后开始统计投票，如果一台机器收到了超过半数的相同投票，那么这个投票对应的SID机器即为Leader。此时Server3将成为Leader。 由上面规则可知，通常那台服务器上的数据越新（zxid会越大），其成为Leader的可能性越大，也就越能够保证数据的恢复。如果zxid相同，则sid越大机会越大。 Leader选举实现细节服务器状态服务器具有四种状态，分别是LOOKING、FOLLOWING、LEADING、OBSERVING。 LOOKING：寻找Leader状态。当服务器处于该状态时，它会认为当前集群中没有Leader，因此需要进入Leader选举状态。 FOLLOWING：跟随者状态。表明当前服务器角色是Follower。 LEADING：领导者状态。表明当前服务器角色是Leader。 OBSERVING：观察者状态。表明当前服务器角色是Observer。 投票数据结构每个投票中包含了两个最基本的信息，所推举服务器的sid和zxid，投票（Vote）在Zookeeper中包含字段如下 属性 说明 id 被推举 Leader 的 sid zxid 被推举 Leader 的zxid electionEpoch 投票的轮次，逻辑时钟，用来判断多个投票是否在同一轮选举周期中，该值在服务端是一个自增序列，每次进入新一轮的投票后，都会对该值进行加1操作 peerEpoch 被推举 Leader 的 epoch state 当前服务器的状态 一次 Leader 选举过程，属于同一个 electionEpoch，结束时，会选出新的 Leader；服务器节点，在比较 (sid，zxid) 之前，会先比较选举轮次 electionEpoch，只有同一轮次的 Leader 投票信息才是有效的： 外部投票轮次 &gt; 内部投票轮次，更新内部投票，并且触发当前节点投票信息的重新广播 外部投票轮次 &lt; 内部投票轮次，直接忽略当前的外部投票 外部投票轮次 = 内部投票轮次，进一步比较 (sid，zxid) 疑问：Leader 负责执行所有的事务操作，一次事务操作， Leader 如何将事务操作同步到 Follower 和 Observer ？同步、异步？ 如何保证同步过程中，事务一定执行成功？事务失败的影响？ Leader 上执行的事务状态，通过 Zab 状态更新的广播协议，更新到 Follower 和 Observer。 QuorumCnxManager：网络I/O每台服务器在启动的过程中，会启动一个QuorumPeerManager，负责各台服务器之间的底层Leader选举过程中的网络通信。 消息队列QuorumCnxManager内部维护了一系列的队列，用来保存接收到的、待发送的消息以及消息的发送器，除接收队列以外，其他队列都按照SID分组形成队列集合，如一个集群中除了自身还有3台机器，那么就会为这3台机器分别创建一个发送队列，互不干扰。 recvQueue：消息接收队列，用于存放那些从其他服务器接收到的消息。 queueSendMap：消息发送队列，用于保存那些待发送的消息，按照sid进行分组。 senderWorkerMap：发送器集合，每个SenderWorker消息发送器，都对应一台远程Zookeeper服务器，负责消息的发送，也按照SID进行分组。 lastMessageSent：最近发送过的消息，为每个sid保留最近发送过的一个消息。 建立连接为了能够相互投票，Zookeeper集群中的所有机器都需要两两建立起网络连接。QuorumCnxManager在启动时会创建一个ServerSocket来监听Leader选举的通信端口(默认为3888)。开启监听后，Zookeeper能够不断地接收到来自其他服务器的创建连接请求，在接收到其他服务器的TCP连接请求时，会进行处理。为了避免两台机器之间重复地创建TCP连接，Zookeeper只允许sid大的服务器主动和其他机器建立连接，否则断开连接。在接收到创建连接请求后，服务器通过对比自己和远程服务器的sid值来判断是否接收连接请求，如果当前服务器发现自己的sid更大，那么会断开当前连接，然后自己主动和远程服务器建立连接。一旦连接建立，就会根据远程服务器的SID来创建相应的消息发送器SendWorker和消息接收器RecvWorker，并启动。 消息接收与发送消息接收：由消息接收器RecvWorker负责，由于Zookeeper为每个远程服务器都分配一个单独的RecvWorker，因此，每个RecvWorker只需要不断地从这个TCP连接中读取消息，并将其保存到recvQueue队列中。 消息发送：由于Zookeeper为每个远程服务器都分配一个单独的SendWorker，因此，每个SendWorker只需要不断地从对应的消息发送队列中获取出一个消息发送即可，同时将这个消息放入lastMessageSent中。在SendWorker中，一旦Zookeeper发现针对当前服务器的消息发送队列为空，那么此时需要从lastMessageSent中取出一个最近发送过的消息来进行再次发送，这是为了解决接收方在消息接收前或者接收到消息后服务器挂了，导致消息尚未被正确处理。同时，Zookeeper能够保证接收方在处理消息时，会对重复消息进行正确的处理。 FastLeaderElection：选举算法核心选票管理 sendqueue：选票发送队列，用于保存待发送的选票。 recvqueue：选票接收队列，用于保存接收到的外部投票。 WorkerReceiver：选票接收器。其会不断地从QuorumCnxManager中获取其他服务器发来的选举消息，并将其转换成一个选票，然后保存到recvqueue中，在选票接收过程中，如果发现该外部选票的选举轮次小于当前服务器的，那么忽略该外部投票，同时立即发送自己的内部投票。 WorkerSender：选票发送器，不断地从sendqueue中获取待发送的选票，并将其传递到底层QuorumCnxManager中。 算法核心 上图展示了FastLeaderElection模块是如何与底层网络I/O进行交互的。Leader选举的基本流程如下: 自增选举轮次。Zookeeper规定所有有效的投票都必须在同一轮次中，在开始新一轮投票时，会首先对logicalclock进行自增操作。 初始化选票。在开始进行新一轮投票之前，每个服务器都会初始化自身的选票，并且在初始化阶段，每台服务器都会将自己推举为Leader。 发送初始化选票。完成选票的初始化后，服务器就会发起第一次投票。Zookeeper会将刚刚初始化好的选票放入sendqueue中，由发送器WorkerSender负责发送出去。 接收外部投票。每台服务器会不断地从recvqueue队列中获取外部选票。如果服务器发现无法获取到任何外部投票，那么就会立即确认自己是否和集群中其他服务器保持着有效的连接，如果没有连接，则马上建立连接，如果已经建立了连接，则再次发送自己当前的内部投票。 判断选举轮次。在发送完初始化选票之后，接着开始处理外部投票。在处理外部投票时，会根据选举轮次来进行不同的处理。 外部投票的选举轮次大于内部投票。若服务器自身的选举轮次落后于该外部投票对应服务器的选举轮次，那么就会立即更新自己的选举轮次(logicalclock)，并且清空所有已经收到的投票，然后使用初始化的投票来进行PK以确定是否变更内部投票。最终再将内部投票发送出去。 外部投票的选举轮次小于内部投票。若服务器接收的外选票的选举轮次落后于自身的选举轮次，那么Zookeeper就会直接忽略该外部投票，不做任何处理，并返回步骤4。 外部投票的选举轮次等于内部投票。此时可以开始进行选票PK。 选票PK。在进行选票PK时，符合任意一个条件就需要变更投票。 若外部投票中推举的Leader服务器的选举轮次大于内部投票，那么需要变更投票。 若选举轮次一致，那么就对比两者的zxid，若外部投票的zxid大，那么需要变更投票。 若两者的ZXID一致，那么就对比两者的sid，若外部投票的sid大，那么就需要变更投票。 变更投票。经过PK后，若确定了外部投票优于内部投票，那么就变更投票，即使用外部投票的选票信息来覆盖内部投票，变更完成后，再次将这个变更后的内部投票发送出去。 选票归档。无论是否变更了投票，都会将刚刚收到的那份外部投票放入选票集合recvset中进行归档。recvset用于记录当前服务器在本轮次的Leader选举中收到的所有外部投票（按照服务队的SID区别，如{(1, vote1), (2, vote2)…}）。 统计投票。完成选票归档后，就可以开始统计投票，统计投票是为了统计集群中是否已经有过半的服务器认可了当前的内部投票，如果确定已经有过半服务器认可了该投票，则终止投票。否则返回步骤4。 更新服务器状态。若已经确定可以终止投票，那么就开始更新服务器状态，服务器首选判断当前被过半服务器认可的投票所对应的Leader服务器是否是自己，若是自己，则将自己的服务器状态更新为LEADING，若不是，则根据具体情况来确定自己是FOLLOWING或是OBSERVING。 以上10个步骤就是FastLeaderElection的核心，其中步骤4-9会经过几轮循环，直到有Leader选举产生。 FastLeaderElection源码分析类的继承关系 1public class FastLeaderElection implements Election &#123;&#125; 说明：FastLeaderElection实现了Election接口，其需要实现接口中定义的lookForLeader方法和shutdown方法，其是标准的Fast Paxos算法的实现，各服务器之间基于TCP协议进行选举。 类的内部类FastLeaderElection有三个较为重要的内部类，分别为Notification、ToSend、Messenger。 Notification类Notification表示收到的选举投票信息（其他服务器发来的选举投票信息），其包含了被选举者的id、zxid、选举周期等信息，其buildMsg方法将选举信息封装至ByteBuffer中再进行发送。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Notifications are messages that let other peers know that * a given peer has changed its vote, either because it has * joined leader election or because it learned of another * peer with higher zxid or same zxid and higher server id */static public class Notification &#123; /* * Format version, introduced in 3.4.6 */ public final static int CURRENTVERSION = 0x2; int version; /* * Proposed leader */ long leader; /* * zxid of the proposed leader */ long zxid; /* * Epoch */ long electionEpoch; /* * current state of sender */ QuorumPeer.ServerState state; /* * Address of sender */ long sid; QuorumVerifier qv; /*Notification * epoch of the proposed leader */ long peerEpoch;&#125; ToSend类ToSend表示发送给其他服务器的选举投票信息，也包含了被选举者的id、zxid、选举周期等信息。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Messages that a peer wants to send to other peers. * These messages can be both Notifications and Acks * of reception of notification. */static public class ToSend &#123; static enum mType &#123;crequest, challenge, notification, ack&#125; ToSend(mType type, long leader, long zxid, long electionEpoch, ServerState state, long sid, long peerEpoch, byte[] configData) &#123; this.leader = leader; this.zxid = zxid; this.electionEpoch = electionEpoch; this.state = state; this.sid = sid; this.peerEpoch = peerEpoch; this.configData = configData; &#125; /* * Proposed leader in the case of notification */ long leader; /* * id contains the tag for acks, and zxid for notifications */ long zxid; /* * Epoch */ long electionEpoch; /* * Current state; */ QuorumPeer.ServerState state; /* * Address of recipient */ long sid; /* * Used to send a QuorumVerifier (configuration info) */ byte[] configData = dummyData; /* * Leader epoch */ long peerEpoch;&#125; Messenger类Messenger包含了WorkerReceiver和WorkerSender两个内部类: 123456protected class Messenger &#123; // 选票发送器 WorkerSender ws; // 选票接收器 WorkerReceiver wr; &#125; 构造函数: 123456789101112131415Messenger(QuorumCnxManager manager) &#123; // 创建WorkerSender this.ws = new WorkerSender(manager); this.wsThread = new Thread(this.ws, "WorkerSender[myid=" + self.getId() + "]"); this.wsThread.setDaemon(true); // 创建WorkerReceiver this.wr = new WorkerReceiver(manager); this.wrThread = new Thread(this.wr, "WorkerReceiver[myid=" + self.getId() + "]"); this.wrThread.setDaemon(true);&#125; WorkerReceiver123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226/** * Receives messages from instance of QuorumCnxManager on * method run(), and processes such messages. */class WorkerReceiver extends ZooKeeperThread &#123; volatile boolean stop; // 服务器之间的连接 QuorumCnxManager manager; WorkerReceiver(QuorumCnxManager manager) &#123; super("WorkerReceiver"); this.stop = false; this.manager = manager; &#125; public void run() &#123; Message response; while (!stop) &#123; // Sleeps on receive try &#123; // 从recvQueue中取出一个选举投票消息（从其他服务器发送过来） response = manager.pollRecvQueue(3000, TimeUnit.MILLISECONDS); // 无投票，跳过 if(response == null) continue; // 大小校验 ... response.buffer.clear(); // Instantiate Notification and set its attributes // 创建接收通知 Notification n = new Notification(); int rstate = response.buffer.getInt(); long rleader = response.buffer.getLong(); long rzxid = response.buffer.getLong(); long relectionEpoch = response.buffer.getLong(); long rpeerepoch; int version = 0x0; if (!backCompatibility28) &#123; rpeerepoch = response.buffer.getLong(); if (!backCompatibility40) &#123; /* * Version added in 3.4.6 */ version = response.buffer.getInt(); &#125; else &#123; LOG.info("Backward compatibility mode (36 bits), server id: &#123;&#125;", response.sid); &#125; &#125; else &#123; LOG.info("Backward compatibility mode (28 bits), server id: &#123;&#125;", response.sid); rpeerepoch = ZxidUtils.getEpochFromZxid(rzxid); &#125; QuorumVerifier rqv = null; // check if we have a version that includes config. If so extract config info from message. if (version &gt; 0x1) &#123; int configLength = response.buffer.getInt(); byte b[] = new byte[configLength]; response.buffer.get(b); synchronized(self) &#123; try &#123; rqv = self.configFromString(new String(b)); QuorumVerifier curQV = self.getQuorumVerifier(); if (rqv.getVersion() &gt; curQV.getVersion()) &#123; LOG.info("&#123;&#125; Received version: &#123;&#125; my version: &#123;&#125;", self.getId(), Long.toHexString(rqv.getVersion()), Long.toHexString(self.getQuorumVerifier().getVersion())); if (self.getPeerState() == ServerState.LOOKING) &#123; LOG.debug("Invoking processReconfig(), state: &#123;&#125;", self.getServerState()); self.processReconfig(rqv, null, null, false); if (!rqv.equals(curQV)) &#123; LOG.info("restarting leader election"); self.shuttingDownLE = true; self.getElectionAlg().shutdown(); break; &#125; &#125; else &#123; LOG.debug("Skip processReconfig(), state: &#123;&#125;", self.getServerState()); &#125; &#125; &#125; catch (IOException e) &#123; LOG.error("Something went wrong while processing config received from &#123;&#125;", response.sid); &#125; catch (ConfigException e) &#123; LOG.error("Something went wrong while processing config received from &#123;&#125;", response.sid); &#125; &#125; &#125; else &#123; LOG.info("Backward compatibility mode (before reconfig), server id: &#123;&#125;", response.sid); &#125; /* * If it is from a non-voting server (such as an observer or * a non-voting follower), respond right away. */ if(!validVoter(response.sid)) &#123; Vote current = self.getCurrentVote(); QuorumVerifier qv = self.getQuorumVerifier(); ToSend notmsg = new ToSend(ToSend.mType.notification, current.getId(), current.getZxid(), logicalclock.get(), self.getPeerState(), response.sid, current.getPeerEpoch(), qv.toString().getBytes()); sendqueue.offer(notmsg); &#125; else &#123; // Receive new message if (LOG.isDebugEnabled()) &#123; LOG.debug("Receive new notification message. My id = " + self.getId()); &#125; // State of peer that sent this message // 推选者的状态 QuorumPeer.ServerState ackstate = QuorumPeer.ServerState.LOOKING; switch (rstate) &#123; case 0: ackstate = QuorumPeer.ServerState.LOOKING; break; case 1: ackstate = QuorumPeer.ServerState.FOLLOWING; break; case 2: ackstate = QuorumPeer.ServerState.LEADING; break; case 3: ackstate = QuorumPeer.ServerState.OBSERVING; break; default: continue; &#125; n.leader = rleader; n.zxid = rzxid; n.electionEpoch = relectionEpoch; n.state = ackstate; n.sid = response.sid; n.peerEpoch = rpeerepoch; n.version = version; n.qv = rqv; /* * Print notification info */ if(LOG.isInfoEnabled())&#123; printNotification(n); &#125; /* * If this server is looking, then send proposed leader */ if(self.getPeerState() == QuorumPeer.ServerState.LOOKING)&#123; recvqueue.offer(n); /* * Send a notification back if the peer that sent this * message is also looking and its logical clock is * lagging behind. */ if((ackstate == QuorumPeer.ServerState.LOOKING) &amp;&amp; (n.electionEpoch &lt; logicalclock.get()))&#123; // 获取自己的投票 Vote v = getVote(); QuorumVerifier qv = self.getQuorumVerifier(); // 构造ToSend消息 ToSend notmsg = new ToSend(ToSend.mType.notification, v.getId(), v.getZxid(), logicalclock.get(), self.getPeerState(), response.sid, v.getPeerEpoch(), qv.toString().getBytes()); // 放入sendqueue队列，等待发送 sendqueue.offer(notmsg); &#125; &#125; else &#123; /* * If this server is not looking, but the one that sent the ack * is looking, then send back what it believes to be the leader. */ Vote current = self.getCurrentVote(); if(ackstate == QuorumPeer.ServerState.LOOKING)&#123; if(LOG.isDebugEnabled())&#123; LOG.debug("Sending new notification. My id =&#123;&#125; recipient=&#123;&#125; zxid=0x&#123;&#125; leader=&#123;&#125; config version = &#123;&#125;", self.getId(), response.sid, Long.toHexString(current.getZxid()), current.getId(), Long.toHexString(self.getQuorumVerifier().getVersion())); &#125; QuorumVerifier qv = self.getQuorumVerifier(); ToSend notmsg = new ToSend( ToSend.mType.notification, current.getId(), current.getZxid(), current.getElectionEpoch(), self.getPeerState(), response.sid, current.getPeerEpoch(), qv.toString().getBytes()); sendqueue.offer(notmsg); &#125; &#125; &#125; &#125; catch (InterruptedException e) &#123; LOG.warn("Interrupted Exception while waiting for new message" + e.toString()); &#125; &#125; LOG.info("WorkerReceiver is down"); &#125;&#125; WorkerReceiver实现了Runnable接口，是选票接收器。其会不断地从QuorumCnxManager中获取其他服务器发来的选举消息，并将其转换成一个选票，然后保存到recvqueue中，在选票接收过程中，如果发现该外部选票的选举轮次小于当前服务器的，那么忽略该外部投票，同时立即发送自己的内部投票。其是将QuorumCnxManager的Message转化为FastLeaderElection的Notification。 其中，WorkerReceiver的主要逻辑在run方法中，其首先会从QuorumCnxManager中的recvQueue队列中取出其他服务器发来的选举消息，消息封装在Message数据结构中。然后判断消息中的服务器id是否包含在可以投票的服务器集合中，若不是，则会将本服务器的内部投票发送给该服务器，其流程如下: 123456789101112131415161718192021/* * If it is from a non-voting server (such as an observer or * a non-voting follower), respond right away. */if(!validVoter(response.sid)) &#123; // 获取自己的投票 Vote current = self.getCurrentVote(); QuorumVerifier qv = self.getQuorumVerifier(); // 构造ToSend消息 ToSend notmsg = new ToSend(ToSend.mType.notification, current.getId(), current.getZxid(), logicalclock.get(), self.getPeerState(), response.sid, current.getPeerEpoch(), qv.toString().getBytes()); // 放入sendqueue队列，等待发送 sendqueue.offer(notmsg);&#125; 若包含该服务器，则根据消息（Message）解析出投票服务器的投票信息并将其封装为Notification，然后判断当前服务器是否为LOOKING，若为LOOKING，则直接将Notification放入FastLeaderElection的recvqueue（区别于recvQueue）中。然后判断投票服务器是否为LOOKING状态，并且其选举周期小于当前服务器的逻辑时钟，则将本（当前）服务器的内部投票发送给该服务器，否则，直接忽略掉该投票。其流程如下: 12345678910111213141516171819202122232425262728293031/* * If this server is looking, then send proposed leader */if(self.getPeerState() == QuorumPeer.ServerState.LOOKING)&#123; // 本服务器为LOOKING状态 // 将消息放入recvqueue中 recvqueue.offer(n); /* * Send a notification back if the peer that sent this * message is also looking and its logical clock is * lagging behind. */ if((ackstate == QuorumPeer.ServerState.LOOKING) // 推选者服务器为LOOKING状态 &amp;&amp; (n.electionEpoch &lt; logicalclock.get()))&#123; // 选举周期小于逻辑时钟 // 创建新的投票 Vote v = getVote(); QuorumVerifier qv = self.getQuorumVerifier(); // 构造新的发送消息（本服务器自己的投票） ToSend notmsg = new ToSend(ToSend.mType.notification, v.getId(), v.getZxid(), logicalclock.get(), self.getPeerState(), response.sid, v.getPeerEpoch(), qv.toString().getBytes()); // 将发送消息放置于队列，等待发送 sendqueue.offer(notmsg); &#125;&#125; 若本服务器的状态不为LOOKING，则会根据投票服务器中解析的version信息来构造ToSend消息，放入sendqueue，等待发送，起流程如下: 123456789101112131415161718192021222324252627282930313233&#125; else &#123; // 本服务器状态不为LOOKING /* * If this server is not looking, but the one that sent the ack * is looking, then send back what it believes to be the leader. */ // 获取当前投票 Vote current = self.getCurrentVote(); if(ackstate == QuorumPeer.ServerState.LOOKING)&#123; // 为LOOKING状态 if(LOG.isDebugEnabled())&#123; LOG.debug("Sending new notification. My id =&#123;&#125; recipient=&#123;&#125; zxid=0x&#123;&#125; leader=&#123;&#125; config version = &#123;&#125;", self.getId(), response.sid, Long.toHexString(current.getZxid()), current.getId(), Long.toHexString(self.getQuorumVerifier().getVersion())); &#125; QuorumVerifier qv = self.getQuorumVerifier(); // 构造ToSend消息 ToSend notmsg = new ToSend( ToSend.mType.notification, current.getId(), current.getZxid(), current.getElectionEpoch(), self.getPeerState(), response.sid, current.getPeerEpoch(), qv.toString().getBytes()); // 将发送消息放置于队列，等待发送 sendqueue.offer(notmsg); &#125; &#125;&#125; WorkerSender12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * This worker simply dequeues a message to send and * and queues it on the manager's queue. */class WorkerSender extends ZooKeeperThread &#123; volatile boolean stop; QuorumCnxManager manager; WorkerSender(QuorumCnxManager manager)&#123; super("WorkerSender"); this.stop = false; this.manager = manager; &#125; public void run() &#123; while (!stop) &#123; try &#123; ToSend m = sendqueue.poll(3000, TimeUnit.MILLISECONDS); if(m == null) continue; process(m); &#125; catch (InterruptedException e) &#123; break; &#125; &#125; LOG.info("WorkerSender is down"); &#125; /** * Called by run() once there is a new message to send. * * @param m message to send */ void process(ToSend m) &#123; ByteBuffer requestBuffer = buildMsg(m.state.ordinal(), m.leader, m.zxid, m.electionEpoch, m.peerEpoch, m.configData); manager.toSend(m.sid, requestBuffer); &#125;&#125; WorkerSender也实现了Runnable接口，为选票发送器，其会不断地从sendqueue中获取待发送的选票，并将其传递到底层QuorumCnxManager中，其过程是将FastLeaderElection的ToSend转化为QuorumCnxManager的Message。 类的构造函数 12345public FastLeaderElection(QuorumPeer self, QuorumCnxManager manager)&#123; this.stop = false; this.manager = manager; starter(self, manager);&#125; 构造函数中初始化了stop字段和manager字段，并且调用了starter函数: 123456789private void starter(QuorumPeer self, QuorumCnxManager manager) &#123; this.self = self; proposedLeader = -1; proposedZxid = -1; sendqueue = new LinkedBlockingQueue&lt;ToSend&gt;(); recvqueue = new LinkedBlockingQueue&lt;Notification&gt;(); this.messenger = new Messenger(manager);&#125; 核心函数分析sendNotifications函数 1234567891011121314151617181920212223private void sendNotifications() &#123; for (long sid : self.getCurrentAndNextConfigVoters()) &#123; // 遍历投票参与者集合 QuorumVerifier qv = self.getQuorumVerifier(); // 构造发送消息 ToSend notmsg = new ToSend(ToSend.mType.notification, proposedLeader, proposedZxid, logicalclock.get(), QuorumPeer.ServerState.LOOKING, sid, proposedEpoch, qv.toString().getBytes()); if(LOG.isDebugEnabled())&#123; LOG.debug("Sending Notification: " + proposedLeader + " (n.leader), 0x" + Long.toHexString(proposedZxid) + " (n.zxid), 0x" + Long.toHexString(logicalclock.get()) + " (n.round), " + sid + " (recipient), " + self.getId() + " (myid), 0x" + Long.toHexString(proposedEpoch) + " (n.peerEpoch)"); &#125; // 将发送消息放置于队列 sendqueue.offer(notmsg); &#125;&#125; 其会遍历所有的参与者投票集合，然后将自己的选票信息发送至上述所有的投票者集合，其并非同步发送，而是将ToSend消息放置于sendqueue中，之后由WorkerSender进行发送。 totalOrderPredicate函数1234567891011121314151617181920212223242526/** * Check if a pair (server id, zxid) succeeds our * current vote. * * @param id Server identifier * @param zxid Last zxid observed by the issuer of this vote */protected boolean totalOrderPredicate(long newId, long newZxid, long newEpoch, long curId, long curZxid, long curEpoch) &#123; LOG.debug("id: " + newId + ", proposed id: " + curId + ", zxid: 0x" + Long.toHexString(newZxid) + ", proposed zxid: 0x" + Long.toHexString(curZxid)); if(self.getQuorumVerifier().getWeight(newId) == 0)&#123; return false; &#125; /* * We return true if one of the following three cases hold: * 1- New epoch is higher * 2- New epoch is the same as current epoch, but new zxid is higher * 3- New epoch is the same as current epoch, new zxid is the same * as current zxid, but server id is higher. */ return ((newEpoch &gt; curEpoch) || ((newEpoch == curEpoch) &amp;&amp; ((newZxid &gt; curZxid) || ((newZxid == curZxid) &amp;&amp; (newId &gt; curId)))));&#125; 该函数将接收的投票与自身投票进行PK，查看是否消息中包含的服务器id是否更优，其按照epoch、zxid、id的优先级进行PK。 termPredicate函数12345678910111213141516171819202122232425262728293031/** * Termination predicate. Given a set of votes, determines if have * sufficient to declare the end of the election round. * * @param votes * Set of votes * @param vote * Identifier of the vote received last */protected boolean termPredicate(Map&lt;Long, Vote&gt; votes, Vote vote) &#123; SyncedLearnerTracker voteSet = new SyncedLearnerTracker(); voteSet.addQuorumVerifier(self.getQuorumVerifier()); if (self.getLastSeenQuorumVerifier() != null &amp;&amp; self.getLastSeenQuorumVerifier().getVersion() &gt; self .getQuorumVerifier().getVersion()) &#123; voteSet.addQuorumVerifier(self.getLastSeenQuorumVerifier()); &#125; /* * First make the views consistent. Sometimes peers will have different * zxids for a server depending on timing. */ for (Map.Entry&lt;Long, Vote&gt; entry : votes.entrySet()) &#123; // 遍历已经接收的投票集合 if (vote.equals(entry.getValue())) &#123; // 将等于当前投票的项放入set voteSet.addAck(entry.getKey()); &#125; &#125; //统计set，查看投某个id的票数是否超过一半 return voteSet.hasAllQuorums();&#125; 该函数用于判断Leader选举是否结束，即是否有一半以上的服务器选出了相同的Leader，其过程是将收到的选票与当前选票进行对比，选票相同的放入同一个集合，之后判断选票相同的集合是否超过了半数。 checkLeader函数 12345678910111213141516171819202122232425262728293031323334/** * In the case there is a leader elected, and a quorum supporting * this leader, we have to check if the leader has voted and acked * that it is leading. We need this check to avoid that peers keep * electing over and over a peer that has crashed and it is no * longer leading. * * @param votes set of votes * @param leader leader id * @param electionEpoch epoch id */protected boolean checkLeader( Map&lt;Long, Vote&gt; votes, long leader, long electionEpoch)&#123; boolean predicate = true; /* * If everyone else thinks I'm the leader, I must be the leader. * The other two checks are just for the case in which I'm not the * leader. If I'm not the leader and I haven't received a message * from leader stating that it is leading, then predicate is false. */ if(leader != self.getId())&#123; // 自己不为leader if(votes.get(leader) == null) predicate = false; // 还未选出leader else if(votes.get(leader).getState() != ServerState.LEADING) predicate = false; // 选出的leader还未给出ack信号，其他服务器还不知道leader &#125; else if(logicalclock.get() != electionEpoch) &#123; // 逻辑时钟不等于选举周期 predicate = false; &#125; return predicate;&#125; 该函数检查是否已经完成了Leader的选举，此时Leader的状态应该是LEADING状态。 lookForLeader函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202/** * Starts a new round of leader election. Whenever our QuorumPeer * changes its state to LOOKING, this method is invoked, and it * sends notifications to all other peers. */public Vote lookForLeader() throws InterruptedException &#123; // 注册jmx ... try &#123; HashMap&lt;Long, Vote&gt; recvset = new HashMap&lt;Long, Vote&gt;(); HashMap&lt;Long, Vote&gt; outofelection = new HashMap&lt;Long, Vote&gt;(); int notTimeout = finalizeWait; synchronized(this)&#123; // 更新逻辑时钟，每进行一轮选举，都需要更新逻辑时钟 logicalclock.incrementAndGet(); // 更新选票 updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch()); &#125; LOG.info("New election. My id = " + self.getId() + ", proposed zxid=0x" + Long.toHexString(proposedZxid)); // 向其他服务器发送自己的选票 sendNotifications(); /* * Loop in which we exchange notifications until we find a leader */ while ((self.getPeerState() == ServerState.LOOKING) &amp;&amp; (!stop))&#123; // 本服务器状态为LOOKING并且还未选出leader // 从recvqueue接收队列中取出投票 Notification n = recvqueue.poll(notTimeout, TimeUnit.MILLISECONDS); /* * Sends more notifications if haven't received enough. * Otherwise processes new notification. */ if(n == null)&#123; if(manager.haveDelivered())&#123; sendNotifications(); &#125; else &#123; manager.connectAll(); &#125; /* * Exponential backoff */ int tmpTimeOut = notTimeout*2; notTimeout = (tmpTimeOut &lt; maxNotificationInterval? tmpTimeOut : maxNotificationInterval); LOG.info("Notification time out: " + notTimeout); &#125; else if (validVoter(n.sid) &amp;&amp; validVoter(n.leader)) &#123; // 投票者集合中包含接收到消息中的服务器id /* * Only proceed if the vote comes from a replica in the current or next * voting view for a replica in the current or next voting view. */ switch (n.state) &#123; case LOOKING: // If notification &gt; current, replace and send messages out if (n.electionEpoch &gt; logicalclock.get()) &#123; // 其选举周期大于逻辑时钟 // 重新赋值逻辑时钟 logicalclock.set(n.electionEpoch); recvset.clear(); if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, getInitId(), getInitLastLoggedZxid(), getPeerEpoch())) &#123; // 选出较优的服务器 // 更新选票 updateProposal(n.leader, n.zxid, n.peerEpoch); &#125; else &#123; // 无法选出较优的服务器 updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch()); &#125; sendNotifications(); &#125; else if (n.electionEpoch &lt; logicalclock.get()) &#123; // 选举周期小于逻辑时钟，不做处理 if(LOG.isDebugEnabled())&#123; LOG.debug("Notification election epoch is smaller than logicalclock. n.electionEpoch = 0x" + Long.toHexString(n.electionEpoch) + ", logicalclock=0x" + Long.toHexString(logicalclock.get())); &#125; break; &#125; else if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) &#123; // 等于，并且能选出较优的服务器 // 更新选票 updateProposal(n.leader, n.zxid, n.peerEpoch); // 发送消息 sendNotifications(); &#125; if(LOG.isDebugEnabled())&#123; LOG.debug("Adding vote: from=" + n.sid + ", proposed leader=" + n.leader + ", proposed zxid=0x" + Long.toHexString(n.zxid) + ", proposed election epoch=0x" + Long.toHexString(n.electionEpoch)); &#125; // don't care about the version if it's in LOOKING state // recvset用于记录当前服务器在本轮次的Leader选举中收到的所有外部投票 recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch)); if (termPredicate(recvset, new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch))) &#123; // 若能选出leader // Verify if there is any change in the proposed leader while((n = recvqueue.poll(finalizeWait, TimeUnit.MILLISECONDS)) != null)&#123; if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch))&#123; recvqueue.put(n); break; &#125; &#125; /* * This predicate is true once we don't read any new * relevant message from the reception queue */ if (n == null) &#123; self.setPeerState((proposedLeader == self.getId()) ? ServerState.LEADING: learningState()); Vote endVote = new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch); leaveInstance(endVote); return endVote; &#125; &#125; break; case OBSERVING: LOG.debug("Notification from observer: " + n.sid); break; case FOLLOWING: case LEADING: /* * Consider all notifications from the same epoch * together. */ if(n.electionEpoch == logicalclock.get())&#123; // 与逻辑时钟相等 // 将该服务器和选票信息放入recvset中 recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch)); if(termPredicate(recvset, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state)) &amp;&amp; checkLeader(outofelection, n.leader, n.electionEpoch)) &#123; // 判断是否完成了leader选举 // 设置本服务器的状态 self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); // 创建投票信息 Vote endVote = new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch); leaveInstance(endVote); return endVote; &#125; &#125; /* * Before joining an established ensemble, verify that * a majority are following the same leader. */ outofelection.put(n.sid, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state)); if (termPredicate(outofelection, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state)) &amp;&amp; checkLeader(outofelection, n.leader, n.electionEpoch)) &#123; synchronized(this)&#123; logicalclock.set(n.electionEpoch); self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); &#125; Vote endVote = new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch); leaveInstance(endVote); return endVote; &#125; break; default: LOG.warn("Notification state unrecoginized: " + n.state + " (n.state), " + n.sid + " (n.sid)"); break; &#125; &#125; else &#123; if (!validVoter(n.leader)) &#123; LOG.warn("Ignoring notification for non-cluster member sid &#123;&#125; from sid &#123;&#125;", n.leader, n.sid); &#125; if (!validVoter(n.sid)) &#123; LOG.warn("Ignoring notification for sid &#123;&#125; from non-quorum member sid &#123;&#125;", n.leader, n.sid); &#125; &#125; &#125; return null; &#125; finally &#123; // 卸载注册的jmx ... &#125;&#125; 该函数用于开始新一轮的Leader选举，其首先会将逻辑时钟自增，然后更新本服务器的选票信息（初始化选票），之后将选票信息放入sendqueue等待发送给其他服务器，其流程如下: 12345678synchronized(this)&#123; logicalclock.incrementAndGet(); updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch());&#125;LOG.info("New election. My id = " + self.getId() + ", proposed zxid=0x" + Long.toHexString(proposedZxid));sendNotifications(); 之后每台服务器会不断地从recvqueue队列中获取外部选票。如果服务器发现无法获取到任何外部投票，就立即确认自己是否和集群中其他服务器保持着有效的连接，如果没有连接，则马上建立连接，如果已经建立了连接，则再次发送自己当前的内部投票，其流程如下: 12345678910111213141516171819202122232425// 从recvqueue接收队列中取出投票Notification n = recvqueue.poll(notTimeout, TimeUnit.MILLISECONDS);/* * Sends more notifications if haven't received enough. * Otherwise processes new notification. */if(n == null)&#123; // 无法获取选票 if(manager.haveDelivered())&#123; // manager已经发送了所有选票消息（表示有连接） // 向所有其他服务器发送消息 sendNotifications(); &#125; else &#123; // 还未发送所有消息（表示无连接） // 连接其他每个服务器 manager.connectAll(); &#125; /* * Exponential backoff */ int tmpTimeOut = notTimeout*2; notTimeout = (tmpTimeOut &lt; maxNotificationInterval? tmpTimeOut : maxNotificationInterval); LOG.info("Notification time out: " + notTimeout);&#125; 在发送完初始化选票之后，接着开始处理外部投票。在处理外部投票时，会根据选举轮次来进行不同的处理。 外部投票的选举轮次大于内部投票。若服务器自身的选举轮次落后于该外部投票对应服务器的选举轮次，那么就会立即更新自己的选举轮次(logicalclock)，并且清空所有已经收到的投票，然后使用初始化的投票来进行PK以确定是否变更内部投票。最终再将内部投票发送出去。 外部投票的选举轮次小于内部投票。若服务器接收的外选票的选举轮次落后于自身的选举轮次，那么Zookeeper就会直接忽略该外部投票，不做任何处理。 外部投票的选举轮次等于内部投票。此时可以开始进行选票PK，如果消息中的选票更优，则需要更新本服务器内部选票，再发送给其他服务器。 之后再对选票进行归档操作，无论是否变更了投票，都会将刚刚收到的那份外部投票放入选票集合recvset中进行归档，其中recvset用于记录当前服务器在本轮次的Leader选举中收到的所有外部投票，然后开始统计投票，统计投票是为了统计集群中是否已经有过半的服务器认可了当前的内部投票，如果确定已经有过半服务器认可了该投票，然后再进行最后一次确认，判断是否又有更优的选票产生，若无，则终止投票，然后最终的选票，其流程如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677// If notification &gt; current, replace and send messages outif (n.electionEpoch &gt; logicalclock.get()) &#123; // 其选举周期大于逻辑时钟 // 重新赋值逻辑时钟 logicalclock.set(n.electionEpoch); // 清空所有接收到的所有选票 recvset.clear(); if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, getInitId(), getInitLastLoggedZxid(), getPeerEpoch())) &#123; // 进行PK，选出较优的服务 // 更新选票 updateProposal(n.leader, n.zxid, n.peerEpoch); &#125; else &#123; // 无法选出较优的服务器 // 更新选票 updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch()); &#125; // 发送本服务器的内部选票消息 sendNotifications();&#125; else if (n.electionEpoch &lt; logicalclock.get()) &#123; // 选举周期小于逻辑时钟，不做处理，直接忽略 if(LOG.isDebugEnabled())&#123; LOG.debug("Notification election epoch is smaller than logicalclock. n.electionEpoch = 0x" + Long.toHexString(n.electionEpoch) + ", logicalclock=0x" + Long.toHexString(logicalclock.get())); &#125; break;&#125; else if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) &#123; // PK，选出较优的服务器 // 更新选票 updateProposal(n.leader, n.zxid, n.peerEpoch); // 发送消息 sendNotifications();&#125;if(LOG.isDebugEnabled())&#123; LOG.debug("Adding vote: from=" + n.sid + ", proposed leader=" + n.leader + ", proposed zxid=0x" + Long.toHexString(n.zxid) + ", proposed election epoch=0x" + Long.toHexString(n.electionEpoch));&#125;// don't care about the version if it's in LOOKING state// recvset用于记录当前服务器在本轮次的Leader选举中收到的所有外部投票recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch));if (termPredicate(recvset, new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch))) &#123; // 若能选出leader // Verify if there is any change in the proposed leader while((n = recvqueue.poll(finalizeWait, TimeUnit.MILLISECONDS)) != null)&#123; // 遍历已经接收的投票集合 if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch))&#123; // 选票有变更，比之前提议的Leader有更好的选票加入 // 将更优的选票放在recvset中 recvqueue.put(n); break; &#125; &#125; /* * This predicate is true once we don't read any new * relevant message from the reception queue */ if (n == null) &#123; // 表示之前提议的Leader已经是最优的 // 设置服务器状态 self.setPeerState((proposedLeader == self.getId()) ? ServerState.LEADING: learningState()); // 最终的选票 Vote endVote = new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch); // 清空recvqueue队列的选票 leaveInstance(endVote); // 返回选票 return endVote; &#125;&#125; 若选票中的服务器状态为FOLLOWING或者LEADING时，其大致步骤会判断选举周期是否等于逻辑时钟，归档选票，是否已经完成了Leader选举，设置服务器状态，修改逻辑时钟等于选举周期，返回最终选票，其流程如下: 123456789101112131415161718192021222324252627282930313233343536373839404142if(n.electionEpoch == logicalclock.get())&#123; // 与逻辑时钟相等 // 将该服务器和选票信息放入recvset中 recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch)); if(termPredicate(recvset, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state)) &amp;&amp; checkLeader(outofelection, n.leader, n.electionEpoch)) &#123; // 已经完成了leader选举 // 设置本服务器的状态 self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); // 最终的选票 Vote endVote = new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch); // 清空recvqueue队列的选票 leaveInstance(endVote); return endVote; &#125;&#125;/* * Before joining an established ensemble, verify that * a majority are following the same leader. */outofelection.put(n.sid, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state));if (termPredicate(outofelection, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state)) &amp;&amp; checkLeader(outofelection, n.leader, n.electionEpoch)) &#123; // 已经完成了leader选举 synchronized(this)&#123; // 设置逻辑时钟 logicalclock.set(n.electionEpoch); // 设置状态 self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); &#125; // 最终选票 Vote endVote = new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch); // 清空recvqueue队列的选票 leaveInstance(endVote); // 返回选票 return endVote;&#125; 参考资料 Zookeeper目录: https://www.cnblogs.com/leesf456/p/6239578.html Zookeeper源码分析目录: https://www.cnblogs.com/leesf456/p/6518040.html]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>Leader选举</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行万里路:葛剑雄旅行]]></title>
    <url>%2F2019%2F08%2F10%2Fgejianxiong-xingwanlilu%2F</url>
    <content type="text"><![CDATA[缘起之前在微博上了解过有一个很著名的文化学者: 葛剑雄，那时其title为复旦大学图书馆馆长。后来在杭州图书馆偶尔看到他的一本书叫《行万里路: 葛剑雄旅行》，带回来读了下，没想到他的授业恩师居然是赫赫有名的谭其骧教授(最早通过《中国国家地图集历史地图集》知道他，这部作品是迄今最权威的中国历史政区地图集，谭先生也是中国历史地理学科主要奠基人和开拓者)，所以拜读的兴趣就更浓厚。书中有不少精彩的文章，下面是一些书摘。 开放的城市离不开移民移民迁移关乎城市兴衰 都市离不开移民，无论是在其形成之初，还是在其发展过程中。一旦移民断绝，甚至出现人口大量外迁，富有该都市特色的文化亦随之而停滞，而衰落，以至最终消亡，古今都市概莫能外。 如西汉长安，是在秦朝咸阳城外的废墟上新建的，故址也只是一个乡，几乎已没有原住民。但在建成之日即迁入大批功臣、贵族、关东六国后裔及豪强，以后又通过迁入陵县(依托皇帝陵墓而设的县)的办法，在长安附近形成了一个城市群，总人口超过一百万。西汉后期，长安一带已是“名都对郭，邑居相承，英俊之域，绂冕所兴，冠盖如云”的繁华都市，形成“五方杂错，风俗不纯”的文化特色。但在公元初新莽政权覆灭引发的战乱中，长安人口损失惨重。东汉建都洛阳后，不仅吸引了大批移民，连长安和关中的精英也纷纷迁往。尽管关中父老仍在梦想有朝一日首都迁回长安，但实际上再未恢复昔日的繁盛。 在北魏孝文帝拓跋宏决定将首都从平城(今山西大同一带)迁往洛阳之时，东汉以来的洛阳早已不复存在。所以在迁都之初只能以金墉城为临时驻地，与西汉迁都长安时的形势相似。但孝文帝实行了最彻底的迁都和汉化政策，所以平城的人口特别是其中的上中层，几乎全部被迁往洛阳。由于此前北魏已在其辖境范围内进行过多次强制性的移民，平城集中了中国北方的上层人士，因而这次新的移民就意味着他们都集中到了洛阳。如在中原战乱中迁往河西走廊的移民后裔，和当地长期形成的文化精英，由西域(泛指今新疆、中亚、印度等地)内迁的僧人、商人、学者等，都先被迁至平城，又被迁于洛阳。此后，洛阳作为北方政权的首都成为中外移民的集中点，也成为多元文化的交汇点。《洛阳伽蓝记》描述了当时的繁盛：“自葱岭已西，至于大秦，百国千城，莫不欢附，商胡贩客，日奔塞下，所谓尽天地之区，已乐中国风土，因而宅者，不可胜数。是以附化之民，万有余家。门巷修整，阊阖填列，青槐荫陌，绿树垂庭，天下难得之货，咸悉在焉。”如此丰富的物质文明也滋养了极其多彩的精神文明，云冈石窟、大批寺庙和辉煌的佛教艺术应运而生。永安二年(529年)，南朝梁国的陈庆之在洛阳短期停留。尽管洛阳的极盛时期已经过去，还是让陈庆之瞠目结舌：“自晋、宋以来，号洛阳为荒土，此中(南方)谓长江以北，尽是夷狄。昨至洛阳，始知衣冠士族并在中原，礼仪富盛，人物殷阜，目所不识，口不能传。所谓‘帝京翼翼，四方之则’，始知登泰山卑培嵝，涉江海者小湘、沅。北人安可不重?” 近代上海是一个典型的移民城市。1843年上海开埠时，整个上海县只有50余万人口，英租界和法租界所在地是上海县城外的乡村，大部分还是农田和坟墓，人口稀少。但到1900年，上海的人口已经突破100万，到1949年更高达500多万。上海开埠时总共才有26位外国人，但以后迅速增加，经常保持着数万人的规模，1943年外国侨民高达15万，1949年上海解放时还有28000多人。上海从一个中等水平的江南县城一跃成为中国和亚洲最大、最发达的都市，移民无疑具有决定性的作用。但自1949年至20世纪70年代末，上海的高素质人口大量迁出，如迁往成为首都的北京，参军参干，支援外地建设，求学，随国民党迁往台湾，迁往港澳和国外，60年代后大规模的大小三线建设和知识青年上山下乡，迁出人口数以百万计。迁入的人口不但数量有限，而且以干部、退伍军人、体力劳动者为主，少数大专毕业生往往学非所用，作用无法充分发挥。再无新的外国侨民迁入，原有侨民大多迁出，未迁者也陆续消失，最后一位外国侨民至80年代初死亡。由移民带入的内资、外资全部断绝。上海都市文化的长期萧条正是这些因素的必然结果。而同期的香港却因大批高素质内地移民的迁入而获益，更因其特殊地位而成为东西方、海内外文化接触和交流的场所，在相当程度上已与上海主客易位。 地图上的中国和历史上的中国疆域：谭其骧编纂历史地图集的历程 但在重大原则问题上，谭先生对中国疆域的处理是经过深思熟虑，始终坚持的。长期以来，出于政治目的，史学界对今天中国境内的疆域一直强调“自古以来”，似乎中国从夏、商、周三代以来一直是这么大，似乎不找到一点“自古以来”的证据，一个地方归属于中国就失去了合法性。其中最敏感的地方就是台湾，由于谭先生坚持实事求是原则，以史料史实为根据，因此，“文革”期间这成为一条重要的“反革命”罪状，他受到了严厉的批判斗争。在修订过程中，他对台湾的处理方案多次被主管部门否决，《图集》的正式出版因此推迟了好几年，直到中央领导亲自过问并签阅批准，才涉险过关。但是，谭先生坚持认为： 台湾在明朝以前，既没有设过羁縻府州，也没有设过羁縻卫所，岛上的部落首领没有向大陆王朝进过贡，称过臣，中原王朝更没有在台湾岛上设官置守。过去我们历史学界也受了“左”的影响，把“台湾自古以来是中国的一部分”这句话曲解了。台湾自古以来是中国的一部分，这是一点没有错的，但是你不能把这句话解释为台湾自古以来是中原王朝的一部分，这是完全违反历史事实，明以前历代中原王朝都管不到台湾。有人要把台湾纳入中国从三国时算起，理由是三国时候孙权曾经派军队到过台湾，但历史事实是“军士万人征夷州（即台湾），军行经岁，士众疾疫死者十有八九”，只俘虏了几千人回来，“得不偿失”。我们根据这条史料，就说台湾从三国时候起就是大陆王朝的领土，不是笑话吗？派了一支军队去，俘虏了几千人回来，这块土地就是孙吴的了？这跟清代沙俄殖民者派一支探险队在黑龙江某个地方劫掠一番，就宣称这个地方已归沙俄所有，有什么区别？ 有人也感到这样实在说不过去，于是又提出了所谓台澎一体论，这也是绝对讲不通的。我们知道，南宋时澎湖在福建泉州同安县辖境之内，元朝在岛上设立了巡检司，这是大陆王朝在澎湖岛上设立政权之始，这是靠得住的。有些同志主张“台澎一体”论，说是既然在澎湖设立了巡检司，可见元朝已管到了台湾，这怎么说得通？在那么小的澎湖列岛上设了巡检司，就会管到那么大的台湾？宋元明清时，一个县可以设立几个巡检司，这等于现在的公安分局或者是派出所。设在澎湖岛上的巡检司，它就能管辖整个台湾了？有什么证据呢？相反，我们有好多证据证明是管不到的。（台湾）为什么自古以来是中国的？因为历史演变的结果，到了清朝台湾是清帝国疆域的一部分。所以台湾岛上的土著民族——高山族是我们中华民族的一个组成部分，是我们中国的一个少数民族。对台湾我们应该这样理解，在明朝以前，台湾岛是由我们中华民族的成员之一高山族居住着的，他们自己管理自己，中原王朝管不到。到了明朝后期。才有大陆上的汉人跑到台湾岛的西海岸建立了汉人的政权。……一直到1683年（康熙二十二年），清朝平定台湾，台湾才开始同大陆属于一个政权。 但这种机械的、教条的观念根深蒂固，无处不在，以至在解释任何一个地方“自古以来”就属于中国时，总是采取“实用”甚至“歪曲”的态度，只讲一部分被认为是有利的事实，却完全不提相反的事实，使绝大多数人误以为自古以来都是如此。 例如新疆，只说公元前60年汉宣帝设立西域都护府，却不提及王莽时已经撤销，东汉时“三通三绝”，以后多数年代名存实亡，或者仅是部分恢复；只说唐朝打败突厥，控制整个西域地区，却不提及安史之乱后唐朝再未重返西域；只说蒙古征服西辽，却不提及元朝从未完全统治西域地区。事实上，中原王朝对西域的统治直到乾隆二十四年（1759年）才重新实现。对于清朝来说，西域的确是新纳的疆域，因此，才有“新疆”的命名。谭先生还以云南为例，虽然汉、晋时代是由中原王朝统治，但是，在南朝后期就脱离了中原王朝。隋唐时期，云南是中原王朝的羁縻地区，不是直辖地区。8世纪中叶以后，南诏依附吐蕃反唐，根本就脱离了唐朝。南诏以后成为大理。总之，从6世纪脱离中原王朝，经过了差不多700年，到13世纪才由元朝征服大理，云南地区又被中原王朝统治。 不过，谭先生特别强调：“我们认为18世纪中叶以后，1840年以前的中国范围是我们几千年来历史发展所自然形成的中国，这就是我们历史上的中国。至于现在的中国疆域，已经不是历史上自然形成的那个范围了，而是这一百多年来资本主义列强、帝国主义侵略宰割了我们的部分领土的结果，所以不能代表我们历史上的中国的疆域了”，“为什么说清朝的版图是历史发展自然形成的呢？而不是说清帝国扩张侵略的结果？因为历史事实的确是这样。……清朝以前，我们中原地区跟各个边疆地区关系长期以来就很密切了，不但经济、文化方面很密切，并且在政治上曾经几度和中原地区在一个政权统治之下。” 虽然作为谭先生的学生与助手，深切理解他无法突破政治底线的苦衷。他的上述说法在理论上存在着局限性，在实际上也存在着无法调和的矛盾：一方面，从秦朝最多300多万平方公里的疆域发展到清朝极盛时期1300多万平方公里的疆域，并不能一概称之为“自然形成”。我们不能因为中国最终形成了一个疆域辽阔的国家，就将历史上那些侵略扩张行为视为促进王朝统一、社会进步的必要手段。秦始皇征服岭南，汉武帝用兵闽粤，唐朝灭高句丽、高昌、薛延陀，蒙古人建立元朝，清朝灭明、平定准噶尔，客观上促成国家统一，为中国疆域最终形成奠定了有利条件。但是，这种侵略扩张行为本身，未必就有“自然”的正义性可言；另一方面，在尚未形成现代国际法和国际关系、不同国家民族平等观念之前，世界上能够生存和发展下来的国家，特别是葡、西、荷、英、法、德、日、美、加、澳等近代大国强国，无一不是侵略扩张的产物，中国岂能例外？1840年以前，中国疆域之所以保持稳定，一个重要的有利因素是地理环境的封闭性，以致在工业化以前的外部世界尚缺乏这种突破地理障碍的能力。尽管如此，唐朝军队在中亚受挫于阿拉伯军队，伊斯兰教在突厥语游牧民族的武力支持下侵入新疆地区取代佛教，葡萄牙人长期占据澳门，葡萄牙人、西班牙人、荷兰人相继占据台湾和澎湖，沙俄哥萨克闯进黑龙江流域烧杀抢掠，这也不能不说是国家竞争“自然”的结果。在中俄雅克萨之战并导致《尼布楚条约》签订后，清朝应该了解俄国人的真实意图，而且有足够的时间和能力移民实边，却继续实施对东北的封禁，以致俄国人进入黑龙江以北和乌苏里江以东如入无人之境，至今一些俄国学家还声称俄国人是这片”新土地的开发者”，而非中国领土的掠夺者。但在清朝对东北开禁，鼓励移民，设置府州县，建东三省后，俄国与日本尽管仍然积心积虑要占据东北，却未能得逞。这岂不也是自然的结果吗？ 顺化散记]]></content>
      <categories>
        <category>历史&amp;地理</category>
        <category>历史地理</category>
      </categories>
      <tags>
        <tag>历史地理</tag>
        <tag>葛剑雄</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bash Shell常用快捷键]]></title>
    <url>%2F2019%2F08%2F06%2Fbash-shortcuts%2F</url>
    <content type="text"><![CDATA[移动光标 ctrl+b: 前移一个字符(backward) ctrl+f: 后移一个字符(forward) alt+b: 前移一个单词 alt+f: 后移一个单词 ctrl+a: 移到行首（a是首字母） ctrl+e: 移到行尾（end） ctrl+xx: 行首到当前光标替换 编辑命令 alt+.: 粘帖最后一次命令最后的参数（通常用于mkdir long-long-dir后, cd配合着alt+.） alt+d: 删除当前光标到临近右边单词开始(delete) ctrl+w: 删除当前光标到临近左边单词结束(word) ctrl+h: 删除光标前一个字符（相当于backspace） ctrl+d: 删除光标后一个字符（相当于delete） ctrl+u: 删除光标左边所有 ctrl+k: 删除光标右边所有 ctrl+l: 清屏 ctrl+shift+c: 复制（相当于鼠标左键拖拽） ctrl+shift+v: 粘贴（相当于鼠标中键） 其它 ctrl+n: 下一条命令 ctrl+p: 上一条命令 alt+n: 下一条命令（例如输入ls, 然后按’alt+n’, 就会找到历史记录下的ls命令） alt+p: 上一条命令（跟alt+n相似） shift+PageUp: 向上翻页 shift+PageDown: 向下翻页 ctrl+r: 进入历史查找命令记录， 输入关键字。 多次按返回下一个匹配项 zsh d: 列出以前的打开的命令 j: jump到以前某个目录，模糊匹配]]></content>
      <categories>
        <category>linux命令</category>
        <category>Bash Shell快捷键</category>
      </categories>
      <tags>
        <tag>Bash Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim命令]]></title>
    <url>%2F2019%2F08%2F05%2Flinux-vim%2F</url>
    <content type="text"><![CDATA[一般模式光标移动 h 或 向左箭头键(←) 光标向左移动一个字符 j 或 向下箭头键(↓) 光标向下移动一个字符 k 或 向上箭头键(↑) 光标向上移动一个字符 l 或 向右箭头键(→) 光标向右移动一个字符 20j 向下移动20行(以上四个命令可以配合数字使用)，在Vim中，很多命令都可以配合数字使用，比如删除10个字符10x w 前移一个单词，光标停在下一个单词开头 2w 重复执行w操作2次 W 移动下一个单词开头，但忽略一些标点 e 前移一个单词，光标停在下一个单词末尾 E 移动到下一个单词末尾，如果词尾有标点，则移动到标点 b 后移一个单词，光标停在上一个单词开头 B 移动到上一个单词开头，忽略一些标点 ( 前移1句 ) 后移1句 3) 光标移动到向下3句 { 前移1段 } 后移1段 f (find）命令也可以用于移动，fx将找到光标后第一个为x的字符，3fd将找到第三个为d的字符 F 同f，反向查找 [Ctrl] + [f] 屏幕『向下』移动一页，相当于 [Page Down]按键 (常用) [Ctrl] + [b] 屏幕『向上』移动一页，相当于 [Page Up] 按键 (常用) [Ctrl] + [d] 屏幕『向下』移动半页 [Ctrl] + [u] 屏幕『向上』移动半页 + 光标移动到非空格符的下一行 - 光标移动到非空格符的上一行 n&lt;space&gt; 那个 n 表示『数字』，例如 20 。按下数字后再按空格键，光标会向右移动这一行的 n 个字符。例如 20&lt;space&gt; 则光标会向后面移动 20 个字符距离。 0 或功能键[Home] 这是数字『 0 』：移动到这一行的最前面字符处 (常用) $ 或功能键[End] 移动到这一行的最后面字符处(常用) H 光标移动到这个屏幕的最上方那一行的第一个字符 M 光标移动到这个屏幕的中央那一行的第一个字符 L 光标移动到这个屏幕的最下方那一行的第一个字符 G 移动到这个档案的最后一行(常用) nG n 为数字。移动到这个档案的第 n 行。例如 20G 则会移动到这个档案的第 20 行(可配合 :set nu) gg 移动到这个档案的第一行，相当于 1G 啊！ (常用) n&lt;Enter&gt; n 为数字。光标向下移动 n 行(常用) 复制粘贴 x, X 在一行字当中，x 为向后删除一个字符 (相当于 [del] 按键)， X 为向前删除一个字符(相当于 [backspace] 亦即是退格键) (常用) nx n 为数字，连续向后删除 n 个字符。举例来说，我要连续删除 10 个字符， 『10x』。 dd 删除游标所在的那一整行(常用) ndd n 为数字。删除光标所在的向下 n 行，例如 20dd 则是删除 20 行 (常用) d1G 删除光标所在到第一行的所有数据 dG 删除光标所在到最后一行的所有数据 d$ 删除游标所在处，到该行的最后一个字符 d0 那个是数字的 0 ，删除游标所在处，到该行的最前面一个字符 yy 复制游标所在的那一行(常用) nyy n 为数字。复制光标所在的向下 n 行，例如 20yy 则是复制 20 行(常用) y1G 复制游标所在行到第一行的所有数据 yG 复制游标所在行到最后一行的所有数据 y0 复制光标所在的那个字符到该行行首的所有数据 y$ 复制光标所在的那个字符到该行行尾的所有数据 p, P p 为将已复制的数据在光标下一行贴上，P 则为贴在游标上一行！ 举例来说，我目前光标在第 20 行，且已经复制了 10 行数据。则按下 p 后， 那 10 行数据会贴在原本的 20 行之后，亦即由 21 行开始贴。但如果是按下 P 呢？ 那么原本的第 20 行会被推到变成 30 行。 (常用) J 将光标所在行与下一行的数据结合成同一行 c 重复删除多个数据，例如向下删除 10 行，[ 10cj ] u 复原前一个动作。(常用) [Ctrl]+r 重做上一个动作。(常用) 这个 u 与 [Ctrl]+r 是很常用的指令！一个是复原，另一个则是重做一次～ . 不要怀疑！这就是小数点！意思是重复前一个动作的意思。 如果你想要重复删除、重复贴上等等动作，按下小数点『.』就好了！ (常用) 搜索替换 /word 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用) ?word 向光标之上寻找一个字符串名称为 word 的字符串。 n 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！ N 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。 使用 /word 配合 n 及 N 是非常有帮助的！可以让你重复的找到一些你搜寻的关键词！ :n1,n2s/word1/word2/g n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则： 『:100,200s/vbird/VBIRD/g』。(常用) :1,$s/word1/word2/g 或 :%s/word1/word2/g 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用) :1,$s/word1/word2/gc 或 :%s/word1/word2/gc 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用) 一般模式切换到编辑模式 进入输入或取代的编辑模式 i, I 进入输入模式(Insert mode)： i 为『从目前光标所在处输入』， I 为『在目前所在行的第一个非空格符处开始输入』。 (常用) a, A 进入输入模式(Insert mode)： a 为『从目前光标所在的下一个字符处开始输入』， A 为『从光标所在行的最后一个字符处开始输入』。(常用) o, O 进入输入模式(Insert mode)： 这是英文字母 o 的大小写。o 为『在目前光标所在的下一行处输入新的一行』； O 为在目前光标所在处的上一行输入新的一行！(常用) r, R 进入取代模式(Replace mode)： r 只会取代光标所在的那一个字符一次；R会一直取代光标所在的文字，直到按下 ESC 为止；(常用) 上面这些按键中，在 vi 画面的左下角处会出现『–INSERT–』或『–REPLACE–』的字样。 由名称就知道该动作了吧！！特别注意的是，我们上面也提过了，你想要在档案里面输入字符时， 一定要在左下角处看到 INSERT 或 REPLACE 才能输入喔！ [Esc] 退出编辑模式，回到一般模式中(常用) 一般模式切换到指令行模式 指令行的储存、离开等指令 :w 将编辑的数据写入硬盘档案中(常用) :w! 若文件属性为『只读』时，强制写入该档案。不过，到底能不能写入， 还是跟你对该档案的档案权限有关啊！ :q 离开 vi (常用) :q! 若曾修改过档案，又不想储存，使用 ! 为强制离开不储存档案。 注意一下啊，那个惊叹号 (!) 在 vi 当中，常常具有『强制』的意思～ :wq 储存后离开，若为 :wq! 则为强制储存后离开 (常用) ZZ 这是大写的 Z 喔！若档案没有更动，则不储存离开，若档案已经被更动过，则储存后离开！ :w [filename] 将编辑的数据储存成另一个档案（类似另存新档） :r [filename] 在编辑的数据中，读入另一个档案的数据。亦即将 『filename』 这个档案内容加到游标所在行后面 :n1,n2 w [filename] 将 n1 到 n2 的内容储存成 filename 这个档案。 :! command 暂时离开 vi 到指令行模式下执行 command 的显示结果！例如 『:! ls /home』即可在 vi 当中察看 /home 底下以 ls 输出的档案信息！ vim 环境的变更 :set nu 显示行号，设定之后，会在每一行的前缀显示该行的行号 :set nonu 与 set nu 相反，为取消行号！ 区域选择 &lt;action&gt;a&lt;object&gt; 或 &lt;action&gt;i&lt;object&gt;在visual 模式下，这些命令很强大，其命令格式为 1&lt;action&gt;a&lt;object&gt;` 和 `&lt;action&gt;i&lt;object&gt; action可以是任何的命令，如 d (删除), y (拷贝), v (可以视模式选择)。 object 可能是： w 一个单词， W 一个以空格为分隔的单词， s 一个句字， p 一个段落。也可以是一个特别的字符：&quot;、 &#39;、 )、 }、 ]。 假设你有一个字符串 (map (+) (&quot;foo&quot;)).而光标键在第一个 o的位置。 vi&quot; → 会选择 foo. va&quot; → 会选择 &quot;foo&quot;. vi) → 会选择 &quot;foo&quot;. va) → 会选择(&quot;foo&quot;). v2i) → 会选择 map (+) (&quot;foo&quot;) v2a) → 会选择 (map (+) (&quot;foo&quot;)) 块操作: &lt;C-v&gt;块操作，典型的操作： 0 &lt;C-v&gt; &lt;C-d&gt; I-- [ESC] ^ → 到行头 &lt;C-v&gt; → 开始块操作 &lt;C-d&gt; → 向下移动 (你也可以使用hjkl来移动光标，或是使用%，或是别的) I-- [ESC] → I是插入，插入“--”，按ESC键来为每一行生效。 自动提示： &lt;C-n&gt; 和 &lt;C-p&gt;在 Insert 模式下，你可以输入一个词的开头，然后按 &lt;C-p&gt;或是&lt;C-n&gt;，自动补齐功能就出现了…… 参考资料 简明 VIM 练级攻略: https://coolshell.cn/articles/5426.html Vim使用笔记: https://www.cnblogs.com/jiqingwu/archive/2012/06/14/vim_notes.html Vim命令合集: https://www.jianshu.com/p/117253829581 ###]]></content>
      <categories>
        <category>linux命令</category>
        <category>vim命令</category>
      </categories>
      <tags>
        <tag>vim命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grep命令]]></title>
    <url>%2F2019%2F08%2F05%2Fgrep-command%2F</url>
    <content type="text"><![CDATA[基本用法grep命令123456-c 统计包含匹配字符串的行数。-i 忽略字符大小写的差别。-v 反转查找。-o 只输出文件中匹配到的部分。-E 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。-r 递归查找。 GNU Grep用法 示例1grep "test[53]" jfedu.txt 以字符test开头，接5或者3的行]]></content>
      <categories>
        <category>linux命令</category>
        <category>grep命令</category>
      </categories>
      <tags>
        <tag>grep命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux基本命令]]></title>
    <url>%2F2019%2F08%2F05%2Flinux-command%2F</url>
    <content type="text"><![CDATA[sort: 排序sort 命令的用法很简单。最基本的用法有两种： 12cat data.txt | sortsort data.txt 选项： 12-r 倒序-f 忽略字母大小写 -k 指定用于排序的栏目范围，-t 指定栏目的分隔符。 以下实例，用:分割行，排序值从第3栏开始： 1234567$ sort -k 3 -t : sort.txtaaa:30:1.6bbb:10:2.5ccc:50:3.3ddd:20:4.2eee:60:5.1eee:40:5.4 uniq: 重复uniq 的作用可以描述为：针对相邻的重复行，进行相应的动作。 这句话中，有两个地方需要注意。首先，针对的是相邻的重复行。因此，uniq 对于不相邻的重复行是不起作用的。其次，进行相应的动作。这意味着，uniq 可以做的事情很多，不止一样。 不带任何参数的时候 uniq 的动作是：对相邻的重复行进行去除。例如： 1cat &lt;filename&gt; | sort | uniq 我们已经见过了 sort 的作用，那么上面命令的作用就很显然了：将 &lt;filename&gt; 按照 ASCII 升序排序；然后去除重复出现的行；最后将这个没有重复行的内容输出到标准输出。 给 uniq 加上参数，就能解锁更多姿势。 12345cat &lt;filename&gt; | sort | uniq -d # 只显示重复的行，每行只显示一次cat &lt;filename&gt; | sort | uniq -D # 只显示重复的行cat &lt;filename&gt; | sort | uniq -i # 忽略大小写cat &lt;filename&gt; | sort | uniq -u # 只显示只出现一次的行cat &lt;filename&gt; | sort | uniq -c # 统计每行重复的次数 xargs: 将标准输入转为命令行参数Unix有些命令可以接受”标准输入”（stdin）作为参数。 12&gt; $ cat /etc/passwd | grep root&gt; 上面的代码使用了管道命令（|）。管道命令的作用，是将左侧命令（cat /etc/passwd）的标准输出转换为标准输入，提供给右侧命令（grep root）作为参数。 但是，大多数命令都不接受标准输入作为参数，只能直接在命令行输入参数，这导致无法用管道命令传递参数。举例来说，echo命令就不接受管道传参。 12&gt; $ echo "hello world" | echo&gt; 上面的代码不会有输出。因为管道右侧的echo不接受管道传来的标准输入作为参数。 xargs命令的作用，是将标准输入转为命令行参数。 123&gt; $ echo "hello world" | xargs echo&gt; hello world&gt; 上面的代码将管道左侧的标准输入，转为命令行参数hello world，传给第二个echo命令。 示例 查找所有后辍为txt的文件列表，然后进行grep: 1$ find . -name "*.txt" | xargs grep "abc" 由于xargs默认将空格作为分隔符，所以不太适合处理文件名，因为文件名可能包含空格。 find命令有一个特别的参数-print0，指定输出的文件列表以null分隔。然后，xargs命令的-0参数表示用null当作分隔符。 1find . -type f -name "*.log" -print0 | xargs -0 rm -f 统计一个源代码目录中所有 php 文件的行数： 1find . -type f -name "*.php" -print0 | xargs -0 wc -l 查找所有的 jpg 文件，并且压缩它们： 1find . -type f -name "*.jpg" -print | xargs tar -czvf images.tar.gz]]></content>
      <categories>
        <category>linux命令</category>
        <category>linux基本命令</category>
      </categories>
      <tags>
        <tag>linux基本命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk用法]]></title>
    <url>%2F2019%2F08%2F05%2Fawk-syntax%2F</url>
    <content type="text"><![CDATA[基本用法1awk [options] 'pattern &#123;action&#125;' file... awk的工作过程是这样的：按行读取输入(标准输入或文件)，对于符合模式pattern的行，执行action。当pattern省略时表示匹配任何字符串；当action省略时表示执行&#39;{print}&#39;；它们不可以同时省略。 模式可以是以下任意一个： /正则表达式/：使用通配符的扩展集。 关系表达式：使用运算符进行操作，可以是字符串或数字的比较测试。 模式匹配表达式：用运算符~(匹配) 和 ~!(不匹配)。 BEGIN语句块、pattern语句块、END语句块 每一行输入，对awk来说都是一条记录(record)，awk使用$0来引用当前记录： 12[root@centos7 ~]# head -1 /etc/passwd | awk '&#123;print $0&#125;'root:x:0:0:root:/root:/bin/bash 例子中将命令head -1 /etc/passwd作为awk的输入，awk省略了pattern，action为print $0，意为打印当前记录。对于每条记录，awk使用分隔符将其分割成列，第一列用$1表示，第二列用$2表示…最后一列用$NF表示 选项-F表示指定分隔符如输出文件/etc/passwd第一行第一列(用户名)和最后一列(登录shell)： 12[root@centos7 ~]# head -1 /etc/passwd | awk -F: '&#123;print $1,$NF&#125;'root /bin/bash 当没有指定分隔符时，使用一到多个blank(空白字符，由空格键或TAB键产生)作为分隔符。输出的分隔符默认为空格。如输出命令ls -l *的结果中，文件大小和文件名： 1234567[root@centos7 temp]# ls -l * | awk '&#123;print $5,$NF&#125;'13 b.txt58 c.txt12 d.txt0 e.txt0 f.txt24 test.sh 还可以对任意列进行过滤： 12[root@centos7 temp]# ls -l *|awk '$5&gt;20 &amp;&amp; $NF ~ /txt$/'-rw-r--r-- 1 nobody nobody 58 11月 16 16:34 c.txt 其中$5&gt;20表示第五列的值大于20；&amp;&amp;表示逻辑与；$NF ~ /txt$/中，~表示匹配，符号//内部是正则表达式。这里省略了action，整条awk语句表示打印文件大小大于20字节并且文件名以txt结尾的行。 awk用NR表示行号 123[root@centos7 temp]# awk '/^root/ || NR==2' /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologin 例子中||表示逻辑或，语句表示：输出文件/etc/passwd中以root开头的行或者第二行。 在一些情况下，使用awk过滤甚至比使用grep更灵活如获得ifconfig的输出中网卡名及其对应的mtu值 12345[root@idc-v-71253 ~]# ifconfig|awk '/^\S/&#123;print $1"\t"$NF&#125;'ens32: 1500ens33: 1500lo: 65536#这里的正则表示不以空白字符开头的行，输出内容中使用\t进行了格式化。 以上所说的NR、NF等都是awk的内建变量，下面列出部分常用内置变量 12345678910$0 当前记录（这个变量中存放着整个行的内容）$1~$n 当前记录的第n个字段，字段间由FS分隔FS 输入字段分隔符 默认是空格或TabNF 当前记录中的字段个数，就是有多少列NR 行号，从1开始，如果有多个文件话，这个值也不断累加。FNR 输入文件行号RS 输入的记录分隔符， 默认为换行符OFS 输出字段分隔符， 默认也是空格ORS 输出的记录分隔符，默认为换行符FILENAME 当前输入文件的名字 awk中还可以使用自定义变量，如将网卡名赋值给变量a，然后输出网卡名及其对应的RX bytes的值(注意不同模式匹配及其action的写法)： 1234[root@idc-v-71253 ~]# ifconfig|awk '/^\S/&#123;a=$1&#125;/RX p/&#123;print a,$5&#125;'ens32: 999477100ens33: 1663197120lo: 0 awk中有两个特殊的pattern：BEGIN和END；它们不会对输入文本进行匹配，BEGIN对应的action部分组合成一个代码块，在任何输入开始之前执行；END对应的action部分组合成一个代码块，在所有输入处理完成之后执行。 12345678#注意类似于C语言的赋值及print函数用法[root@centos7 temp]# ls -l *|awk 'BEGIN&#123;print "size name\n---------"&#125;$5&gt;20&#123;x+=$5;print $5,$NF&#125;END&#123;print "---------\ntotal",x&#125;'size name---------58 c.txt24 test.sh---------total 82 再来看看统计每个用户的进程的占了多少内存（注：sum的RSS那一列） 123456$ ps aux | awk &apos;NR!=1&#123;a[$1]+=$6;&#125; END &#123; for(i in a) print i &quot;, &quot; a[i]&quot;KB&quot;;&#125;&apos;dbus, 540KBmysql, 99928KBwww, 3264924KBroot, 63644KBhchen, 6020KB awk还支持数组，数组的索引都被视为字符串(即关联数组)，可以使用for循环遍历数组元素如输出文件/etc/passwd中各种登录shell及其总数量 1234567#注意数组赋值及for循环遍历数组的写法[root@centos7 temp]# awk -F ':' '&#123;a[$NF]++&#125;END&#123;for(i in a) print i,a[i]&#125;' /etc/passwd/bin/sync 1/bin/bash 2/sbin/nologin 19/sbin/halt 1/sbin/shutdown 1 当然也有if分支语句 123#注意大括号是如何界定action块的[root@centos7 temp]# netstat -antp|awk '&#123;if($6=="LISTEN")&#123;x++&#125;else&#123;y++&#125;&#125;END&#123;print x,y&#125;'6 3 pattern之间可以用逗号分隔，表示从匹配第一个模式开始直到匹配第二个模式 12345[root@centos7 ~]# awk '/^root/,/^adm/' /etc/passwd root:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologin 还支持三目操作符pattern1 ? pattern2 : pattern3，表示判断pattern1是否匹配，true则匹配pattern2，false则匹配pattern3，pattern也可以是类似C语言的表达式。如判断文件/etc/passwd中UID大于500的登录shell是否为/bin/bash，是则输出整行，否则输出UID为0的行： 12345#注意为避免混淆对目录分隔符进行了转义[root@centos7 ~]# awk -F: '$3&gt;500?/\/bin\/bash$/:$3==0 &#123;print $0&#125;' /etc/passwd root:x:0:0:root:/root:/bin/bashlearner:x:1000:1000::/home/learner:/bin/bash#三目运算符也可以嵌套，例子略 选项-f file表示从file中读取awk指令 1234567891011121314#打印斐波那契数列前十项[root@centos7 temp]# cat test.awk BEGIN&#123; $1=1 $2=1 OFS="," for(i=3;i&lt;=10;i++) &#123; $i=$(i-2)+$(i-1) &#125; print&#125;[root@centos7 temp]# awk -f test.awk 1,1,2,3,5,8,13,21,34,55 选项-F指定列分隔符 1234#多个字符作为分隔符时[root@centos7 temp]# echo 1.2,3:4 5|awk -F '[., :]' '&#123;print $2,$NF&#125;'2 5#这里-F后单引号中的内容也是正则表达式 选项-v var=val设定变量 1234567#这里printf函数用法类似C语言同名函数[root@centos7 ~]# awk -v n=5 'BEGIN&#123;for(i=0;i&lt;n;i++) printf "%02d\n",i&#125;' 0001020304 print等函数还支持使用重定向符&gt;和&gt;&gt;将输出保存至文件 1234567#如按第一列(IP)分类拆分文件access.log，并保存至ip.txt文件中[root@centos7 temp]# awk '&#123;print &gt; $1".txt"&#125;' access.log [root@centos7 temp]# ls -l 172.20.71.*-rw-r--r-- 1 root root 5297 11月 22 21:33 172.20.71.38.txt-rw-r--r-- 1 root root 1236 11月 22 21:33 172.20.71.39.txt-rw-r--r-- 1 root root 4533 11月 22 21:33 172.20.71.84.txt-rw-r--r-- 1 root root 2328 11月 22 21:33 172.20.71.85.txt 内建函数length()获得字符串长度 12[root@centos7 temp]# awk -F: '&#123;if(length($1)&gt;=16)print&#125;' /etc/passwd systemd-bus-proxy:x:999:997:systemd Bus Proxy:/:/sbin/nologin split()将字符串按分隔符分隔，并保存至数组 12345678[root@centos7 temp]# head -1 /etc/passwd|awk '&#123;split($0,arr,/:/);for(i=1;i&lt;=length(arr);i++) print arr[i]&#125;'rootx00root/root/bin/bash getline从输入(可以是管道、另一个文件或当前文件的下一行)中获得记录，赋值给变量或重置某些环境变量 1234567891011121314151617181920212223#从shell命令date中通过管道获得当前的小时数[root@centos7 temp]# awk 'BEGIN&#123;"date"|getline;split($5,arr,/:/);print arr[1]&#125;'09#从文件中获取，此时会覆盖当前的$0。(注意逐行处理b.txt的同时也在逐行从c.txt中获得记录并覆盖$0，当getline先遇到eof时&lt;即c.txt文件行数较少&gt;将输出空行)[root@centos7 temp]# awk '&#123;getline &lt;"c.txt";print $4&#125;' b.txt "https://segmentfault.com/blog/learnning"[root@centos7 temp]# #赋值给变量[root@centos7 temp]# awk '&#123;getline blog &lt;"c.txt";print $0"\n"blog&#125;' b.txt aasdasdadsadBLOG ADDRESS IS "https://segmentfault.com/blog/learnning"[root@centos7 temp]# #读取下一行(也会覆盖当前$0)[root@centos7 temp]# cat fileanny100bob150cindy120[root@centos7 temp]# awk '&#123;getline;total+=$0&#125;END&#123;print total&#125;' file370#此时表示只对偶数行进行处理 next作用和getline类似，也是读取下一行并覆盖$0，区别是next执行后，其后的命令不再执行，而是读取下一行从头再执行。 123456789101112131415161718192021#跳过以a-s开头的行，统计行数，打印最终结果[root@centos7 temp]# awk '/^[a-s]/&#123;next&#125;&#123;count++&#125;END&#123;print count&#125;' /etc/passwd2[root@centos7 temp]# #又如合并相同列的两个文件[root@centos7 temp]# cat f.txt 学号 分值00001 8000002 7500003 90[root@centos7 temp]# cat e.txt 姓名 学号张三 00001李四 00002王五 00003[root@centos7 temp]# awk 'NR==FNR&#123;a[$1]=$2;next&#125;&#123;print $0,a[$2]&#125;' f.txt e.txt 姓名 学号 分值张三 00001 80李四 00002 75王五 00003 90#这里当读第一个文件时NR==FNR成立，执行a[$1]=$2，然后next忽略后面的。读取第二个文件时，NR==FNR不成立，执行后面的打印命令 sub(regex,substr,string)替换字符串string(省略时为$0)中首个出现匹配正则regex的子串substr 12[root@centos7 temp]# echo 178278 world|awk 'sub(/[0-9]+/,"hello")'hello world gsub(regex,substr,string)与sub()类似，但不止替换第一个，而是全局替换 123456[root@centos7 temp]# head -n5 /etc/passwd|awk '&#123;gsub(/[0-9]+/,"----");print $0&#125;' root:x:----:----:root:/root:/bin/bashbin:x:----:----:bin:/bin:/sbin/nologindaemon:x:----:----:daemon:/sbin:/sbin/nologinadm:x:----:----:adm:/var/adm:/sbin/nologinlp:x:----:----:lp:/var/spool/lpd:/sbin/nologin substr(str,n,m)切割字符串str，从第n个字符开始，切割m个。如果m省略，则到结尾 12[root@centos7 temp]# echo "hello,世界！"|awk '&#123;print substr($0,8,1)&#125;'界 tolower(str)和toupper(str)表示大小写转换 12[root@centos7 temp]# echo "hello,世界！"|awk '&#123;A=toupper($0);print A&#125;'HELLO,世界！ system(cmd)执行shell命令cmd，返回执行结果，执行成功为0，失败为非0 123#此处if语句判断和C语言一致，0为false，非0为true[root@centos7 temp]# awk 'BEGIN&#123;if(!system("date&gt;/dev/null"))print "success"&#125;'success match(str,regex)返回字符串str中匹配正则regex的位置 12[root@centos7 temp]# awk 'BEGIN&#123;A=match("abc.f.11.12.1.98",/[0-9]&#123;1,3&#125;\./);print A&#125;'7 示例 条件统计 1awk '$2&gt;=6 &amp;&amp; $2&lt;20 &#123; tot++ &#125; END &#123; print +tot&#125;' 花式用法 12345#按连接数查看客户端IPnetstat -ntu | awk '&#123;print $5&#125;' | cut -d: -f1 | sort | uniq -c | sort -nr #打印99乘法表seq 9 | sed 'H;g' | awk -v RS='' '&#123;for(i=1;i&lt;=NF;i++)printf("%dx%d=%d%s", i, NR, i*NR, i==NR?"\n":"\t")&#125;']]></content>
      <categories>
        <category>linux命令</category>
        <category>awk命令</category>
      </categories>
      <tags>
        <tag>awk命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2019%2F08%2F05%2Fregexp-syntax%2F</url>
    <content type="text"><![CDATA[元字符 字符 描述 \ 将下一个字符标记为一个特殊字符、或一个原义字符、或一个 向后引用、或一个八进制转义符。例如，’n’ 匹配字符 “n”。’\n’ 匹配一个换行符。序列 ‘\‘ 匹配 “&quot; 而 “(“ 则匹配 “(“。 ^ 匹配输入字符串的开始位置。如果设置了 RegExp 对象的 Multiline 属性，^ 也匹配 ‘\n’ 或 ‘\r’ 之后的位置。 $ 匹配输入字符串的结束位置。如果设置了RegExp 对象的 Multiline 属性，$ 也匹配 ‘\n’ 或 ‘\r’ 之前的位置。 * 匹配前面的子表达式零次或多次。例如，zo* 能匹配 “z” 以及 “zoo”。* 等价于{0,}。 + 匹配前面的子表达式一次或多次。例如，’zo+’ 能匹配 “zo” 以及 “zoo”，但不能匹配 “z”。+ 等价于 {1,}。 ? 匹配前面的子表达式零次或一次。例如，”do(es)?” 可以匹配 “do” 或 “does” 。? 等价于 {0,1}。 {n} n 是一个非负整数。匹配确定的 n 次。例如，’o{2}’ 不能匹配 “Bob” 中的 ‘o’，但是能匹配 “food” 中的两个 o。 {n,} n 是一个非负整数。至少匹配n 次。例如，’o{2,}’ 不能匹配 “Bob” 中的 ‘o’，但能匹配 “foooood” 中的所有 o。’o{1,}’ 等价于 ‘o+’。’o{0,}’ 则等价于 ‘o*’。 {n,m} m 和 n 均为非负整数，其中n &lt;= m。最少匹配 n 次且最多匹配 m 次。例如，”o{1,3}” 将匹配 “fooooood” 中的前三个 o。’o{0,1}’ 等价于 ‘o?’。请注意在逗号和两个数之间不能有空格。 ? 当该字符紧跟在任何一个其他限制符 (*, +, ?, {n}, {n,}, {n,m}) 后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串 “oooo”，’o+?’ 将匹配单个 “o”，而 ‘o+’ 将匹配所有 ‘o’。 . 匹配除换行符（\n、\r）之外的任何单个字符。要匹配包括 ‘\n’ 在内的任何字符，请使用像”(.|\n)“的模式。 (pattern) 匹配 pattern 并获取这一匹配。所获取的匹配可以从产生的 Matches 集合得到。要匹配圆括号字符，请使用 ‘(‘ 或 ‘)‘。 (?:pattern) 匹配 pattern 但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用 “或” 字符 (|) 来组合一个模式的各个部分是很有用。例如， ‘industr(?:y|ies) 就是一个比 ‘industry|industries’ 更简略的表达式。 (?=pattern) 正向肯定预查（look ahead positive assert），在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，”Windows(?=95|98|NT|2000)”能匹配”Windows2000”中的”Windows”，但不能匹配”Windows3.1”中的”Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。 (?!pattern) 正向否定预查(negative assert)，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如”Windows(?!95|98|NT|2000)”能匹配”Windows3.1”中的”Windows”，但不能匹配”Windows2000”中的”Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。 (?&lt;=pattern) 反向(look behind)肯定预查，与正向肯定预查类似，只是方向相反。例如，”`(?&lt;=95 (?&lt;!pattern) 反向否定预查，与正向否定预查类似，只是方向相反。例如”`(?&lt;!95 x|y 匹配 x 或 y。例如，’z|food’ 能匹配 “z” 或 “food”。’(z|f)ood’ 则匹配 “zood” 或 “food”。 [xyz] 字符集合。匹配所包含的任意一个字符。例如， ‘[abc]’ 可以匹配 “plain” 中的 ‘a’。 [^xyz] 负值字符集合。匹配未包含的任意字符。例如， ‘[^abc]’ 可以匹配 “plain” 中的’p’、’l’、’i’、’n’。 [a-z] 字符范围。匹配指定范围内的任意字符。例如，’[a-z]’ 可以匹配 ‘a’ 到 ‘z’ 范围内的任意小写字母字符。 [^a-z] 负值字符范围。匹配任何不在指定范围内的任意字符。例如，’[^a-z]’ 可以匹配任何不在 ‘a’ 到 ‘z’ 范围内的任意字符。 \b 匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’。 \B 匹配非单词边界。’er\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’。 \cx 匹配由 x 指明的控制字符。例如， \cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 ‘c’ 字符。 \d 匹配一个数字字符。等价于 [0-9]。 \D 匹配一个非数字字符。等价于 [^0-9]。 \f 匹配一个换页符。等价于 \x0c 和 \cL。 \n 匹配一个换行符。等价于 \x0a 和 \cJ。 \r 匹配一个回车符。等价于 \x0d 和 \cM。 \s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。 \S 匹配任何非空白字符。等价于 [^ \f\n\r\t\v]。 \t 匹配一个制表符。等价于 \x09 和 \cI。 \v 匹配一个垂直制表符。等价于 \x0b 和 \cK。 \w 匹配字母、数字、下划线。等价于’[A-Za-z0-9_]’。 \W 匹配非字母、数字、下划线。等价于 ‘[^A-Za-z0-9_]’。 \xn 匹配 n，其中 n 为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，’\x41’ 匹配 “A”。’\x041’ 则等价于 ‘\x04’ &amp; “1”。正则表达式中可以使用 ASCII 编码。 \num 匹配 num，其中 num 是一个正整数。对所获取的匹配的引用。例如，’(.)\1’ 匹配两个连续的相同字符。 \n 标识一个八进制转义值或一个向后引用。如果 \n 之前至少 n 个获取的子表达式，则 n 为向后引用。否则，如果 n 为八进制数字 (0-7)，则 n 为一个八进制转义值。 \nm 标识一个八进制转义值或一个向后引用。如果 \nm 之前至少有 nm 个获得子表达式，则 nm 为向后引用。如果 \nm 之前至少有 n 个获取，则 n 为一个后跟文字 m 的向后引用。如果前面的条件都不满足，若 n 和 m 均为八进制数字 (0-7)，则 \nm 将匹配八进制转义值 nm。 \nml 如果 n 为八进制数字 (0-3)，且 m 和 l 均为八进制数字 (0-7)，则匹配八进制转义值 nml。 \un 匹配 n，其中 n 是一个用四个十六进制数字表示的 Unicode 字符。例如， \u00A9 匹配版权符号 (?)。 常用正则表达式收集 最全的常用正则表达式大全——包括校验数字、字符、一些特殊的需求等 常用正则表达式 - 收集一些在平时项目开发中经常用到的正则表达式]]></content>
      <categories>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paxos精髓及示例]]></title>
    <url>%2F2019%2F07%2F31%2Fpaxos-essence-and-demo%2F</url>
    <content type="text"><![CDATA[Paxos精髓paxos协议用来解决的问题可以用一句话来简化: proposer将发起提案（value）给所有accpetor，超过半数accpetor获得批准后，proposer将提案写入accpetor内，最终所有accpetor获得一致性的确定性取值，且后续不允许再修改。 协议分为两大阶段，每个阶段又分为a/b两小步骤： 准备阶段（占坑阶段）Phase 1a: PrepareProposer选择一个提议编号n，向所有的Acceptor广播Prepare（n）请求。 Phase 1b: PromiseAcceptor接收到Prepare（n）请求，若提议编号n比之前接收的Prepare请求都要大，则承诺将不会接收提议编号比n小的提议，并且带上之前Accept的提议中编号小于n的最大的提议，否则不予理会。 接受阶段（提交阶段）Phase 2a: Accept​ 1. 如果未超过半数accpetor响应，直接转为提议失败； ​ 2. 如果超过多数Acceptor的承诺，又分为不同情况： ​ 2.1 如果所有Acceptor都未接收过值（都为null），那么向所有的Acceptor发起自己的值和提议编号n，记住，一定是所有Acceptor都没接受过值； ​ 2.2 如果有部分Acceptor接收过值，那么从所有接受过的值中选择对应的提议编号最大的作为提议的值，提议编号仍然为n。但此时Proposer就不能提议自己的值，只能信任Acceptor通过的值，维护一但获得确定性取值就不能更改原则； Phase 2b: AcceptedAcceptor接收到提议后，如果该提议版本号不等于自身保存记录的版本号（第一阶段记录的），不接受该请求，相等则写入本地。 整个paxos协议过程看似复杂难懂，但只要把握和理解这两点就基本理解了paxos的精髓： 理解第一阶段accpetor的处理流程：如果本地已经写入了，不再接受和同意后面的所有请求，并返回本地写入的值；如果本地未写入，则本地记录该请求的版本号，并不再接受其他版本号的请求，简单来说只信任最后一次提交的版本号的请求，使其他版本号写入失效； 理解第二阶段proposer的处理流程：未超过半数accpetor响应，提议失败；超过半数的accpetor值都为空才提交自身要写入的值，否则选择非空值里版本号最大的值提交，最大的区别在于是提交的值是自身的还是使用以前提交的。 Basic Paxos信息流协议过程举例看这个最简单的例子：1个processor，3个Acceptor，无learner。 目标：proposer向3个aceptort 将name变量写为v1。 第一阶段A：proposer发起prepare（name，n1）,n1是递增提议版本号，发送给3个Acceptor，说，我现在要写name这个变量，我的版本号是n1第一阶段B：Acceptor收到proposer的消息，比对自己内部保存的内容，发现之前name变量（null，null）没有被写入且未收到过提议，都返回给proposer，并在内部记录name这个变量，已经有proposer申请提议了，提议版本号是n1;第二阶段A：proposer收到3个Acceptor的响应，响应内容都是：name变量现在还没有写入，你可以来写。proposer确认获得超过半数以上Acceptor同意，发起第二阶段写入操作：accept（v1,n1），告诉Acceptor我现在要把name变量协议v1,我的版本号是刚刚获得通过的n1;第二阶段B：accpetor收到accept（v1,n1），比对自身的版本号是一致的，保存成功，并响应accepted（v1,n1）；结果阶段：proposer收到3个accepted响应都成功，超过半数响应成功，到此name变量被确定为v1。 以上是正常的paxos协议提议确定流程，是不是很简单，很容易理解呢？ 确定你理解了上面的例子再往后看。 这是最简单也最容易理解的例子，但真实情况远比这个复杂，还有以下问题： 如果其中的某个Acceptor没响应怎么处理？如果只写成功了一个accpetor又怎么处理，写成功两个呢？如果多个proposer并发写会导致accpetor写成不同值吗？learner角色是做什么用？为什么是超过半数同意？ paxos特殊情况下的处理第一种情况：Proposer提议正常，未超过accpetor失败情况 问题：还是上面的例子，如果第二阶段B，只有2个accpetor响应接收提议成功，另外1个没有响应怎么处理呢？ 处理：proposer发现只有2个成功，已经超过半数，那么还是认为提议成功，并把消息传递给learner，由learner角色将确定的提议通知给所有accpetor，最终使最后未响应的accpetor也同步更新，通过learner角色使所有Acceptor达到最终一致性。 第二种情况：Proposer提议正常，但超过accpetor失败情况 问题：假设有2个accpetor失败，又该如何处理呢？ 处理：由于未达到超过半数同意条件，proposer要么直接提示失败，要么递增版本号重新发起提议，如果重新发起提议对于第一次写入成功的accpetor不会修改，另外两个accpetor会重新接受提议，达到最终成功。 情况再复杂一点：还是一样有3个accpetor，但有两个proposer。 情况一：proposer1和proposer2串行执行 proposer1和最开始情况一样，把name设置为v1，并接受提议。 proposer1提议结束后，proposer2发起提议流程： 第一阶段A：proposer1发起prepare（name，n2） 第一阶段B：Acceptor收到proposer的消息，发现内部name已经写入确定了，返回（name,v1,n1） 第二阶段A：proposer收到3个Acceptor的响应，发现超过半数都是v1，说明name已经确定为v1，接受这个值，不再发起提议操作。 情况二：proposer1和proposer2交错执行 proposer1提议accpetor1成功，但写入accpetor2和accpetor3时，发现版本号已经小于accpetor内部记录的版本号（保存了proposer2的版本号），直接返回失败。 proposer2写入accpetor2和accpetor3成功，写入accpetor1失败，但最终还是超过半数写入v2成功，name变量最终确定为v2； proposer1递增版本号再重试发现超过半数为v2，接受name变量为v2，也不再写入v1。name最终确定还是为v2 情况三：proposer1和proposer2第一次都只写成功1个Acceptor怎么办 都只写成功一个，未超过半数，那么Proposer会递增版本号重新发起提议，这里需要分多种情况： 3个Acceptor都响应提议，发现Acceptor1{v1,n1} ,Acceptor2{v2,n2},Acceptor{null,null}，Processor选择最大的{v2,n2}发起第二阶段，成功后name值为v2; 2个Acceptor都响应提议， 如果是Acceptor1{v1,n1} ,Acceptor2{v2,n2}，那么选择最大的{v2,n2}发起第二阶段，成功后name值为v2; 如果是Acceptor1{v1,n1} ,Acceptor3{null,null}，那么选择最大的{v1,n1}发起第二阶段，成功后name值为v1; 如果是Acceptor2{v2,n2} ,Acceptor3{null,null}，那么选择最大的{v2,n2}发起第二阶段，成功后name值为v2; 只有1个Acceptor响应提议，未达到半数，放弃或者递增版本号重新发起提议可以看到，都未达到半数时，最终值是不确定的！ 参考资料 paxos算法wiki: https://en.wikipedia.org/wiki/Paxos_(computer_science)#Basic_Paxos 首先，推荐的是知行学社的《分布式系统与Paxos算法视频课程》： 视频讲的非常好，很适合入门，循序渐进慢慢推导，我自己看了不下5遍，视频讲解理解更深，推荐大家都看看。 推荐刘杰的《分布式系统原理介绍》 ，里面有关于paxos的详细介绍，例子非常多，也有包括paxos协议的证明过程，大而全，质量相当高的一份学习资料！ 推荐的一份高质量ppt《可靠分布式系统基础 Paxos 的直观解释》: https://drmingdrmer.github.io/tech/distributed/2015/11/11/paxos-slide.html； 技术类的东西怎么能只停留在看上面，肯定要看代码啊，推荐微信开源的phxpaxos：https://github.com/tencent-wechat/phxpaxos，结合代码对协议理解更深。 《Paxos Made live》。这篇论文是 Google 发表的，讨论了 paxos 的工程实践，也就是 chubby 这个众所周知的分布式服务的实现，可以结合《The Chubby lock service for loosely-coupled distributed systems》 一起看。实际应用中的难点，比如 master 租约实现、group membership 变化、Snapshot 加快复制和恢复以及实际应用中遇到的故障、测试等问题，特别是最后的测试部分。非常值得一读。《The Chubby lock service for loosely-coupled distributed systems》 更多介绍了 Chubby 服务本身的设计决策，为什么是分布式锁服务，为什么是粗粒度的锁，为什么是目录文件模式，事件通知、多机房部署以及应用碰到的使用问题等等。 Paxos 我还着重推荐阅读微信后端团队写的系列博客，包括他们开源的 phxpaxos 实现，基本上将所有问题都讨论到了，并且通俗易懂。 一致性方面另一块就是 Raft 算法，按照 Google Chubby 论文里的说法， 1`Indeed, all working protocols for asynchronous consensus we have so far encountered have Paxos at their core.` 但是 Raft 真的好理解多了，我读的是《In Search of an Understandable Consensus Algorithm》，论文写到这么详细的步骤，你不想理解都难。毕竟 Raft 号称就是一个 Understandable Consensus Algorithm。无论从任何角度，都推荐阅读这一篇论文。 首先能理解 paxos 的一些难点，其次是了解 Raft 的实现，加深对 Etcd 等系统的理解。这篇论文还有一个 250 多页的加强版《CONSENSUS: BRIDGING THEORY AND PRACTICE》，教你一行一行写出一个 Raft 实现，我还没有学习，有兴趣可以自行了解。Raft 通过明确引入 leader（其实 multi paxos 引申出来也有，但是没有这么明确的表述）来负责 client 交互和日志复制，将整个算法过程非常清晰地表达出来。Raft 的算法正确性的核心在于保证 Leader Completeness ，选举算法选出来的 leader 一定是包含了所有 committed entries 的，这是因为所有 committed entries 一定会在多数派至少一个成员里存在，所以设计的选举算法一定能选出来这么一个成员作为 leader。多数派 accept 应该说是一致性算法正确性的最重要的保证。 最后，我还读了《Building Consistent Transactions with Inconsistent Replication》，包括作者的演讲，作者也开放了源码。Google Spanner 基本是将 paxos 算法应用到了极致，但是毕竟不是所有公司都是这么财大气粗搞得起 TrueTime API，架得起全球机房，控制或者承受得了事务延时。这篇论文提出了另一个思路，论文介绍的算法分为两个层次： IR 和基于其他上的 TAPIR。前者就是 Inconsistent Replication，它将操作分为两类： inconsistent： 可以任意顺序执行，成功执行的操作将持久化，支持 failover。 consensus：同样可以任意顺序执行并持久化 failover，但是会返回一个唯一的一致(consensus)结果。 Java简单实现Basic Paxos算法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249/** * Alipay.com Inc. * Copyright (c) 2004-2019 All Rights Reserved. */package paxos;import com.google.common.base.Charsets;import org.apache.commons.lang3.StringUtils;import com.google.common.hash.HashFunction;import com.google.common.hash.Hashing;import java.util.ArrayList;import java.util.Arrays;import java.util.Collection;import java.util.Collections;import java.util.List;import java.util.Random;/** * @author xbyan * @version $Id: PaxosStudy.java, v 0.1 2019-07-01 5:47 PM xbyan Exp $$ */public class PaxosStudy &#123; private static final HashFunction HASH_FUNCTION = Hashing.murmur3_32(); private static final Random RAMDOM = new Random(); private static final String[] PROPOSALS = &#123; "ProjectA", "ProjectB", "ProjectC" &#125;; public static void main(String[] args) throws InterruptedException &#123; //新建server组 List&lt;Acceptor&gt; acceptors = new ArrayList&lt;&gt;(); Arrays.asList("A", "B", "C", "D", "E").forEach(name -&gt; acceptors.add(new Acceptor(name))); //开始投票 Proposer.vote(new Proposal(1L, null), acceptors); &#125; private static void printInfo(String subject, String operation, String result) throws InterruptedException &#123; System.out.println(subject + ":" + operation + "&lt;" + result + "&gt;"); Thread.sleep(1000); &#125; /** * 对于提案的约束，第三方约束要求 * 如果maxVote不存在，那么没有限制，下一次表决可以使用任意提案 * 否则，下一次表决要沿用maxVote提案 * * @param currentVoteNumber * @param proposals * @return */ private static Proposal nextProposal(long currentVoteNumber, List&lt;Proposal&gt; proposals) &#123; long voteNumber = currentVoteNumber + 1; //刚开始投票 if (proposals.isEmpty()) &#123; Proposal proposal = new Proposal(voteNumber, PROPOSALS[RAMDOM.nextInt(PROPOSALS.length)]); System.out.println("NEXT PROPOSER(刚开始投票), currentVoteNumber" + currentVoteNumber + ",proposal=" + proposal); return proposal; &#125; //为之前的票排序 Collections.sort(proposals); Proposal maxVote = proposals.get(proposals.size() - 1); //列表里最大值 long maxVoteNumber = maxVote.getVoteNumber(); String content = maxVote.getContent(); if (maxVoteNumber &gt;= currentVoteNumber) &#123; throw new IllegalStateException("illegal state maxVoteNumber"); &#125; if (content != null) &#123; Proposal proposal = new Proposal(voteNumber, content); System.out.println("NEXT PROPOSER(content不为空), voteNumber=" + voteNumber + ",proposal=" + proposal + ",maxVote=" + maxVote); return proposal; &#125; else &#123; Proposal proposal = new Proposal(voteNumber, PROPOSALS[RAMDOM.nextInt(PROPOSALS.length)]); System.out.println("NEXT PROPOSER(content为空), voteNumber=" + voteNumber + ",proposal=" + proposal + ",maxVote=" + maxVote); return proposal; &#125; &#125; private static class Proposer &#123; /** * @param proposal * @param acceptors */ public static void vote(Proposal proposal, Collection&lt;Acceptor&gt; acceptors) throws InterruptedException &#123; //法定人数 n/2+1 int quorum = Math.floorDiv(acceptors.size(), 2) + 1; System.out.println("quorum " + quorum); int count = 0; while (true) &#123; //开始投票 printInfo("VOTE_ROUND", "START", ++count + "：开始向acceptor发送信息"); List&lt;Proposal&gt; proposals = new ArrayList&lt;&gt;(); for (Acceptor acceptor : acceptors) &#123; Promise promise = acceptor.onPrepare(proposal); if (promise != null &amp;&amp; promise.isAck()) &#123; //Acceptor批准后就可以将发起者加入发起者列表里 System.out .println("--------------------------------------------------ACCEPTOR批准,将发起者加入队列里,proposal=" + promise.getProposal()); proposals.add(promise.getProposal()); &#125; &#125; if (proposals.size() &lt; quorum) &#123; //小于法定人数 printInfo("PROPOSER[" + proposal + "]", "VOTE", "NOT PREPARED：接收到消息的acceptor小于半数"); System.out.println("onPrepare阶段编号增加 重新发起投票"); proposal = nextProposal(proposal.getVoteNumber(), proposals); continue; &#125; System.out.println("接收到消息的acceptor大于半数，开始提案内容"); int acceptCount = 0; for (Acceptor acceptor1 : acceptors) &#123; if (acceptor1.onAccept(proposal)) acceptCount++; &#125; if (acceptCount &lt; quorum) &#123; printInfo("PROPOSER[" + proposal + "]", "VOTE", "NOT ACCEPTED,接受提案的少于半数"); System.out.println("onAccept阶段编号增加 重新发起投票"); proposal = nextProposal(proposal.getVoteNumber(), proposals); continue; &#125; break; &#125; printInfo("PROPOSER[" + proposal + "]", "VOTE", "SUCCESS，接受提案成功"); &#125; &#125; private static class Acceptor &#123; //上次表决结果 private Proposal last = new Proposal(); private String name; public Acceptor(String name) &#123; this.name = name; &#125; public Promise onPrepare(Proposal proposal) throws InterruptedException &#123; //假设这个过程有50%的几率失败 if (Math.random() - 0.5 &gt; 0) &#123; printInfo("ACCEPTER_" + name, "PREPARE", "NO RESPONSE 向" + name + " 发送失败"); return null; &#125; if (proposal == null) throw new IllegalArgumentException("null proposal"); //大于 if (proposal.getVoteNumber() &gt; last.getVoteNumber()) &#123; Promise response = new Promise(true, last); last = proposal; printInfo("ACCEPTER_" + name, "PREPARE", "OK " + name + " 记下编号，返回信息,last=" + last); return response; &#125; else &#123; printInfo("ACCEPTER_" + name, "PREPARE", "REJECTED " + name + " 已保存有选定的编号，拒绝"); return new Promise(false, null); &#125; &#125; public boolean onAccept(Proposal proposal) throws InterruptedException &#123; //假设这个过程有50%的几率失败 if (Math.random() - 0.5 &gt; 0) &#123; printInfo("ACCEPTER_" + name, "ACCEPT", "NO RESPONSE " + name + " 发送提案失败"); return false; &#125; boolean accepted = last.equals(proposal); printInfo("ACCEPTER_" + name, "ACCEPT", name + (accepted ? "OK：通过" : "NO: 不通过") + "last=" + last + ",proposal=" + proposal); return accepted; &#125; &#125; private static class Promise &#123; private final boolean ack; private final Proposal proposal; private Promise(boolean ack, Proposal proposal) &#123; this.ack = ack; this.proposal = proposal; &#125; public boolean isAck() &#123; return ack; &#125; public Proposal getProposal() &#123; return proposal; &#125; &#125; private static class Proposal implements Comparable&lt;Proposal&gt; &#123; private final long voteNumber; private final String content; public Proposal(long voteNumber, String content) &#123; this.voteNumber = voteNumber; this.content = content; &#125; public Proposal() &#123; this(0, null); &#125; public long getVoteNumber() &#123; return voteNumber; &#125; public String getContent() &#123; return content; &#125; public int compareTo(Proposal o) &#123; return Long.compare(voteNumber, o.voteNumber); &#125; @Override public boolean equals(Object obj) &#123; if (obj == null) &#123; return false; &#125; if (!(obj instanceof Proposal)) return false; Proposal proposal = (Proposal) obj; return voteNumber == proposal.voteNumber &amp;&amp; StringUtils.equals(content, proposal.content); &#125; @Override public int hashCode() &#123; return HASH_FUNCTION.newHasher().putLong(voteNumber).putString(content, Charsets.UTF_8) .hash().asInt(); &#125; @Override public String toString() &#123; return new StringBuilder().append(voteNumber).append(':').append(content).toString(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>paxos算法</category>
      </categories>
      <tags>
        <tag>paxos</tag>
        <tag>分布式一致性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paxos-by-example]]></title>
    <url>%2F2019%2F07%2F29%2Fpaxos-by-example%2F</url>
    <content type="text"><![CDATA[Paxos算法在分布式领域具有非常重要的地位。但是Paxos算法有两个比较明显的缺点：1.难以理解 2.工程实现更难。 网上有很多讲解Paxos算法的文章，但是质量参差不齐。看了很多关于Paxos的资料后发现，学习Paxos最好的资料是论文Paxos Made Simple，其次是中、英文版维基百科对Paxos的介绍。本文试图带大家一步步揭开Paxos神秘的面纱。 Paxos是什么 Paxos算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一。 Google Chubby的作者Mike Burrows说过这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品。 虽然Mike Burrows说得有点夸张，但是至少说明了Paxos算法的地位。然而，Paxos算法也因为晦涩难懂而臭名昭著。本文的目的就是带领大家深入浅出理解Paxos算法，不仅理解它的执行流程，还要理解算法的推导过程，作者是怎么一步步想到最终的方案的。只有理解了推导过程，才能深刻掌握该算法的精髓。而且理解推导过程对于我们的思维也是非常有帮助的，可能会给我们带来一些解决问题的思路，对我们有所启发。 最近在看paxos算法, 看了不少博客, 都感觉总结得不够到位,甚至有理解偏差的地方,倒是在博客的引用文章里面找到了一篇英文文献,看完后豁然开朗茅塞顿开. 于是把那篇英文原文翻译了一下, 加深一下理解 英文文献虽是2012年的了, 但是短小精悍,值得一看,原文地址是: angus.nyc/2012/paxos-by-example/ 文章中不全是直译, 有些地方改成了我自己的理解, 所以有可能有理解错的地方, 还望指出. 关于作者: Lamport提到Paxos算法，我们不得不首先来介绍下Paxos算法的作者Leslie Lamport(莱斯利·兰伯特, http://www.lamport.org )及其对计算机科学尤其是分布式计算领域的杰出贡献。作为2013年的新科图灵奖得主，Lamport是计算机科学领域一位拥有杰出成就的传奇人物，其先后多次荣获ACM和IEEE以及其他各类计算机重大奖项。Lamport对时间时钟、面包店算法、拜占庭将军问题以及Paxos算法的创造性研究，极大地推动了计算机科学尤其是分布式计算的发展，全世界无数工程师得益于他的理论，其中Paxos算法的提出，正是Lamport多年的研究成果。 说起Paxos理论的发表，还有一段非常有趣的历史故事。Lamport早在1990年就已经将其对Paxos算法的研究论文《The PartTime Parliament》提交给ACM TOCS Jnl.的评审委员会了，但是由于Lamport“创造性”地使用了故事的方式来进行算法的描述，导致当时委员会的工作人员没有一个能够正确地理解他对算法的描述，时任主编要求Lamport使用严谨的数据证明方式来描述该算法，否则他们将不考虑接受这篇论文。遗憾的是，Lamport并没有接收他们的建议，当然也就拒绝了对论文的修改，并撤销了对这篇论文的提交。在后来的一个会议上，Lamport还对此事耿耿于怀：“为什么这些搞理论的人一点幽默感也没有呢？” 幸运的是，还是有人能够理解Lamport那公认的令人晦涩的算法描述的。1996年，来自微软的Butler Lampson在WDAG96上提出了重新审视这篇分布式论文的建议，在次年的WDAG97上，麻省理工学院的Nancy Lynch也公布了其根据Lamport的原文重新修改后的《Revisiting the Paxos Algorithm》，“帮助”Lamport用数学的形式化术语定义并证明了Paxos算法。于是在1998年的ACM TOCS上，这篇延迟了9年的论文终于被接受了，也标志着Paxos算法正式被计算机科学接受并开始影响更多的工程师解决分布式一致性问题。 后来在2001年，Lamport本人也做出了让步，这次他放弃了故事的描述方式，而是使用了通俗易懂的语言重新讲述了原文，并发表了《Paxos Made Simple》——当然，Lamport甚为固执地认为他自己的表述语言没有歧义，并且也足够让人明白Paxos算法，因此不需要数学来协助描述，于是整篇文章还是没有任何数学符号。好在这篇文章已经能够被更多的人理解，相信绝大多数的Paxos爱好者也都是从这篇文章开始慢慢进入了Paxos的神秘世界。 由于Lamport个人自负固执的性格，使得Paxos理论的诞生可谓一波三折。关于Paxos理论的诞生过程，后来也成为了计算机科学领域被广泛流传的学术趣事。 图0: Lamport本尊 译文开始本文通过一个实例描述了一个叫Paxos的分布式一致性算法 分布式一致性算法通常是用于让多个计算机节点就某个单值的修改达成一致, 例如事务的提交commit或回滚rollback, 典型的思想就是二阶段提交或三阶段提交. 算法并不关心这个单值是什么,只关心最后只有唯一一个值被所有的节点选中, 对该值的修改达成一致 在一个分布式系统中这非常的难, 因为不同机器之前的消息可能会丢失, 或是被无限延迟, 甚至机器本身也会挂掉 Paxos算法可以保证最终只会有一个值会被选中, 但是不能保证如果集群中的大多数节点挂掉后,还能选中一个值 概述Paxos的节点可以是proposer, acceptor,learner中的任何一种角色.proposer会提议一个它想被采纳的值, 一般是通过发送一个包含该提议值的提案给集群中的每个acceptor. 然后acceptor会独立的决定是否接受这个提案–它可能会接收到多个proposer的不同提案, 决定好后会把它的决定发给learner. learner的作用是判断一个值是否已经被接受.在Paxos算法中, 只有一个值被大多数的acceptor选中, 才算被Paxos算法选中.在实际项目中, 一个节点可能担任多种角色, 但是在这个例子里面, 每一种角色都是一个独立的节点. 图1 Paxos的基本架构. 几个proposer向acceptor提交提案. 当一个acceptor接受了一个值后, 它把它的决定发给learner Paxos算法例子在标准的Paxos算法中, 一个proposer会发送两种类型的消息给acceptors: prepare request 和accept request. 在算法的第一阶段, proposer会发送一个prepare request给每一个acceptor, 这个prepare request包含一个提议值value和一个提案号number.每一个proposer的提案号number必须是正数, 单调递增的,独一无二的自然数[1]. 在下面图2的例子中, 有两个proposer, 都在发送prepare request. proposer A的请求([n=2,v=8])比proposer B的请求([n=4,v=5])先到达acceptor X和acceptor Y, 而proposer B的请求先到达acceptor Z 图2: Paxos. proposer A和B各发送了一个prepare request 给每一个accetor. 在这个例子中, proposer A的请求先到达acceptor X和Y, 而proposer B的请求先到达了acceptor Z. 如果一个acceptor接收到了一个prepare request而又没接收过其他的提案, 这个acceptor会回应一个prepare response, 并保证不会接受其他比当前提案号number更小的提案. 下面图3展示了每个acceptor是如何响应它们接收到的prepare request的 图3: 每个acceptor都对它接收到的第一个prepare request 进行prepare response. 最终, acceptor Z接收到了proposer A的请求[2], acceptor X和Y收到了proposer B的请求. 如果一个acceptor之前已经接收了一个提案号更高的prepare request的话, 那么后面收到的prepare request就会被忽略, 这个例子中就是acceptor Z会忽略掉proposer A的请求(Z已经接收了B的提案n=4). 如果acceptor之前没有接收过更高提案号的提案, 它会保证忽略其他提案号比这个更低的请求(注意是包括prepare request和accept request),然后在prepare response中把它已经选中的提案号最高的提议(包括这个提议的值)发送给proposer.在这个例子就是acceptor X和Y给proposer B的响应(X和Y已经接受了一个B的[n=2,v=8]的提案,当再收到[n=4, v=5]的提案时, 它保证会忽略任何n&lt;4的prepare request和accept request, 然后把[n=2, v=8]的prepare response回去给B) 图4: acceptor Z 忽略了proposer A的请求, 因为它已经接收到了更高的提议号(4 &gt; 2). acceptor X和Y给proposer B响应了它见过最高的提议号的提案, 并承诺会忽略提案号更小的提案 一旦proposer 收到了大多数acceptor的prepare response, 它就可以开始给所有的acceptor发送accept request了. 因为proposer A接收到prepare response中表明在它之前没有更早的提案. 所以它的accept request中的提案号和提议值都是跟prepare request中的一样.然而,这个accept request被所有的acceptor都拒绝了,因为它们都承诺了不会接收提议号比4更新的提案(因为都见过proposer B的提案了) Proposer B的accept request情况就跟proposer A的有所不同了. 首先它的提案号还是它之前的提案号(n=4), 但是提议值却不是它之前的提议值了(它之前提议的是v=5), 而是它在prepare response中接收到的提议值(v=8)[3] 图5: proposer B也发送了一个accept request给每一个acceptor.acceptor request中的提案号是它之前的提案号(4), 而提议值是它在prepare response中收到的值(8, 来自proposer A的提案[n=2, v=8]) 如果acceptor收到了一个提案号不小于它见过的accept request, 它会接受这个accept request并且发送一个通知给到learner节点. 当learner节点发现大多数acceptor都已经接受了一个值的时候,Paxos算法就认为这个值已经被选中. 当一个值被paxos选中后,后续的流程中将不能改变这个值.例如,当另一个proposer C发送一个提案号更高的prepare request(例如, [n=6, v=7])过来时,所有的acceptor都会响应它之前已经选中的提议(n=4,v=8).这会要求proposer C在后续的acceptor request中也只能发送[n=6, v=8]的提议, 即只能确认一遍已经选中的这个值. 再后面, 如果有少量的acceptor还没选中一个值的话, 这个流程会保证最终所有的节点都会一致地选中同一个值. 译文结束 提案号的唯一性的保证不是Paxos算法的内容 ↩︎ 这个也可能不会发送, 但是这个算法中也是有可能的 ↩︎ 注意这个是它从prepare response中收到的最高的提案号. 在这个例子中, proposer B的提案号(n=4)是比proposer A的提案号(n=2)大. 但是在它的prepare request的响应中, 它只收到了proposer A的提议([n=2, v=8]). 如果它在prepare response中没有收到其他的提案的话, 它就会发送自己的提案([n=4,v=5]) ↩︎ 参考资料 NEAT ALGORITHMS - PAXOS(小动画可以辅助读懂): http://harry.me/blog/2014/12/27/neat-algorithms-paxos/ paxos-by-example(通过一个例子理解paxos算法): https://juejin.im/post/5d159590f265da1b86089a89 Paxos Made Live: an amazing paper from Google describing the challenges of their Paxos implementation in Chubby: http://static.googleusercontent.com/media/research.google.com/en//archive/paxos_made_live.pdf A Quora thread explaining Paxos in a few different ways: https://www.quora.com/Distributed-Systems/What-is-a-simple-explanation-of-the-Paxos-algorithm Raft - An Understandable Consensus Algorithm. Raft is another conensus algorithm designed for humans to understand, which if you can tell from the above wall of text might be a problem with Paxos. https://ramcloud.stanford.edu/raft.pdf 左耳听风 - 推荐阅读：分布式数据调度相关论文: http://10-02.com/180118-32%20_%20%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87.html]]></content>
      <categories>
        <category>分布式一致性</category>
        <category>paxos算法</category>
      </categories>
      <tags>
        <tag>paxos</tag>
        <tag>分布式一致性</tag>
      </tags>
  </entry>
</search>
